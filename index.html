
    <!DOCTYPE html>
    <html lang="vi">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Practice Test #3 - AWS Certified SysOps Administrator Associate</title>
        <style>
            
    body {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        line-height: 1.6;
        margin: 0;
        padding-top: 60px; /* Space for score-stats-container */
        background-color: #020617;
        color: #c7d1dd;
        font-size: 16px;
    }
    main {
        max-width: 850px;
        margin: 0 auto;
        padding: 20px;
    }
    h1 {
        text-align: center;
        color: #e0e7ff;
        margin-bottom: 20px;
        font-size: 2em;
    }
    .quiz-description {
        background-color: #0f172a;
        padding: 15px;
        border-radius: 8px;
        margin-bottom: 30px;
        border: 1px solid #1e293b;
        color: #a0aec0;
    }
    .question-container {
        background-color: #0f172a;
        border: 1px solid #1e293b;
        border-radius: 8px;
        padding: 20px;
        margin-bottom: 25px;
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    }
    .question-prompt {
        font-weight: 600;
        margin-bottom: 15px;
        color: #e0e7ff;
        font-size: 1.1em;
    }
    .options-container {
        display: flex;
        flex-direction: column;
        gap: 10px;
        margin-bottom: 15px;
    }
    .option-label {
        display: flex;
        align-items: center;
        gap: 10px;
        padding: 10px 15px;
        border: 1px solid #334155;
        border-radius: 6px;
        cursor: pointer;
        transition: background-color 0.2s, border-color 0.2s;
    }
    .option-label:hover {
        background-color: #1e293b;
    }
    input[type="radio"],
    input[type="checkbox"] {
        appearance: none; /* Hide default radio/checkbox */
        width: 20px;
        height: 20px;
        border: 2px solid #64748b;
        border-radius: 50%; /* For radio */
        background-color: transparent;
        display: grid;
        place-content: center;
        flex-shrink: 0;
        cursor: pointer;
    }
    input[type="checkbox"] {
        border-radius: 4px; /* For checkbox */
    }
    input[type="radio"]::before,
    input[type="checkbox"]::before {
        content: '';
        width: 10px;
        height: 10px;
        border-radius: 50%; /* For radio */
        transform: scale(0);
        transition: transform 0.2s ease-in-out;
        background-color: #3b82f6; /* Active color */
    }
    input[type="checkbox"]::before {
        border-radius: 2px; /* For checkbox */
    }
    input[type="radio"]:checked::before,
    input[type="checkbox"]:checked::before {
        transform: scale(1);
    }
    input[type="radio"]:checked,
    input[type="checkbox"]:checked {
        border-color: #3b82f6;
    }

    .submit-btn, .explanation-btn {
        background-color: #22c55e; /* Success green */
        color: white;
        border: none;
        padding: 10px 20px;
        border-radius: 6px;
        cursor: pointer;
        font-size: 1em;
        transition: background-color 0.2s;
        margin-top: 15px;
        width: fit-content;
    }
    .submit-btn:hover {
        background-color: #16a34a;
    }
    .explanation-btn {
        background-color: #3b82f6; /* Info blue */
        margin-left: 10px;
    }
    .explanation-btn:hover {
        background-color: #2563eb;
    }

    .correct-feedback {
        border: 2px solid #22c55e; /* Green for correct */
        background-color: #dcfce7;
        color: #15803d;
        padding: 10px;
        border-radius: 6px;
        margin-top: 10px;
    }
    .incorrect-feedback {
        border: 2px solid #ef4444; /* Red for incorrect */
        background-color: #fee2e2;
        color: #b91c1c;
        padding: 10px;
        border-radius: 6px;
        margin-top: 10px;
    }
    .option-label.correct {
        border-color: #22c55e;
        background-color: #2f453a;
    }
    .option-label.incorrect {
        border-color: #ef4444;
        background-color: #3f2f31;
    }

    /* Modal styles */
    .modal {
        display: none; /* Hidden by default */
        position: fixed; /* Stay in place */
        z-index: 1; /* Sit on top */
        left: 0;
        top: 0;
        width: 100%; /* Full width */
        height: 100%; /* Full height */
        overflow: auto; /* Enable scroll if needed */
        background-color: rgba(0,0,0,0.7); /* Black w/ opacity */
    }
    .modal-content {
        background-color: #0f172a;
        margin: 15% auto; /* 15% from the top and centered */
        padding: 20px;
        border: 1px solid #888;
        width: 80%; /* Could be more or less, depending on screen size */
        border-radius: 10px;
        position: relative;
        color: #e0e7ff;
    }
    .close-button {
        color: #aaa;
        float: right;
        font-size: 28px;
        font-weight: bold;
    }
    .close-button:hover,
    .close-button:focus {
        color: #fff;
        text-decoration: none;
        cursor: pointer;
    }
    .modal-body {
        margin-top: 20px;
    }
    .section-header {
        font-size: 1.4em;
        font-weight: bold;
        margin-top: 30px;
        margin-bottom: 20px;
        padding-bottom: 10px;
        border-bottom: 2px solid #64748b;
        color: #e0e7ff;
    }
    #score-stats-container {
        position: fixed;
        z-index: 10;
        top: 0;
        height: auto; /* Allow height to adjust */
        width: 100%;
        background-color: #020617;
        padding: 8px 16px; /* Tăng padding trên dưới để có thêm không gian */
        color: #e0e7ff;
        font-weight: 600;
        display: flex; /* Dùng flexbox */
        flex-wrap: wrap; /* Cho phép các mục xuống dòng nếu không đủ chỗ */
        align-items: center;
        justify-content: space-around; /* Phân bổ không gian đều */
        box-shadow: 0 2px 5px rgba(0,0,0,0.3);
        gap: 15px; /* Khoảng cách giữa các mục */
    }
    #score-stats-container div {
        flex-shrink: 0; /* Ngăn các mục co lại */
        white-space: nowrap; /* Giữ các nhãn trên một dòng */
    }
    .question-prompt img {
        max-width: 100%; /* Đảm bảo hình ảnh không tràn ra ngoài */
        height: auto;
        display: block; /* Để kiểm soát margin dễ dàng hơn */
        margin-top: 10px; /* Khoảng cách giữa văn bản và hình ảnh */
        border-radius: 5px;
    }
    
        </style>
    </head>
    <body>
        <section id="score-stats-container">
            <div class="score-item">Tổng số câu hỏi: <span id="total-questions-display">0</span></div>
            <div class="score-item">Trả lời đúng: <span id="correct-answers-display">0</span></div>
            <div class="score-item">Trả lời sai: <span id="wrong-answers-display">0</span></div>
        </section>
        <main>
            <h1>Practice Test #3 - AWS Certified SysOps Administrator Associate</h1>
            <div class="quiz-description">
                <h3>Giới thiệu về bài kiểm tra này:</h3>
                <p>About this practice exam: - questions order and response orders are randomized - you can only review the answer after finishing the exam due to how Udemy works - it consists of 65 questions, the duration is 130 minutes, the passing score is 720 ====== In case of an issue with a question: - ask a question in the Q&A - please take a screenshot of the question (because they're randomized) and attach it - we will get back to you as soon as possible and fix the issue Good luck, and happy learning!</p>
                <p><strong>Điểm đậu:</strong> 72%</p>
            </div>
            <section id="quiz-container"></section> 

            <div id="explanationModal" class="modal">
                <div class="modal-content">
                    <span class="close-button">&times;</span>
                    <h2>Giải thích</h2>
                    <div class="modal-body" id="modalExplanationContent">
                        </div>
                </div>
            </div>
        </main>
        <script>
            
    const allQuizData = [{"question": "<p>A startup uses Amazon S3 buckets for storing their customer data. The company has defined different retention periods for different objects present in their Amazon S3 buckets, based on the compliance requirements. But, the retention rules do not seem to work as expected.</p>\n<p>Which of the following points are important to remember when configuring retention periods for objects in Amazon S3 buckets (Select two)?</p>\n", "answers": ["When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version", "You cannot place a retention period on an object version through a bucket default setting", "When you use bucket default settings, you specify a Retain Until Date for the object version", "Different versions of a single object can have different retention modes and periods", "The bucket default settings will override any explicit retention mode or period you request on an object version"], "correct_answer": ["When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version", "Different versions of a single object can have different retention modes and periods"], "explanation": "<p>Correct options:</p>\n<p><strong>When you apply a retention period to an object version explicitly, you specify a <code>Retain Until Date</code> for the object version</strong> - You can place a retention period on an object version either explicitly or through a bucket default setting. When you apply a retention period to an object version explicitly, you specify a <code>Retain Until Date</code> for the object version. Amazon S3 stores the Retain Until Date setting in the object version's metadata and protects the object version until the retention period expires.</p>\n<p><strong>Different versions of a single object can have different retention modes and periods</strong> - Like all other Object Lock settings, retention periods apply to individual object versions. Different versions of a single object can have different retention modes and periods.</p>\n<p>For example, suppose that you have an object that is 15 days into a 30-day retention period, and you PUT an object into Amazon S3 with the same name and a 60-day retention period. In this case, your PUT succeeds, and Amazon S3 creates a new version of the object with a 60-day retention period. The older version maintains its original retention period and becomes deletable in 15 days.</p>\n<p>Incorrect options:</p>\n<p><strong>You cannot place a retention period on an object version through a bucket default setting</strong> - You can place a retention period on an object version either explicitly or through a bucket default setting.</p>\n<p><strong>When you use bucket default settings, you specify a <code>Retain Until Date</code> for the object version</strong> - When you use bucket default settings, you don't specify a Retain Until Date. Instead, you specify a duration, in either days or years, for which every object version placed in the bucket should be protected.</p>\n<p><strong>The bucket default settings will override any explicit retention mode or period you request on an object version</strong> - If your request to place an object version in a bucket contains an explicit retention mode and period, those settings override any bucket default settings for that object version.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "checkbox"}, {"question": "<p>A media company runs its business on Amazon EC2 instances backed by Amazon S3 storage. The company is apprehensive about the consistent increase in costs incurred from S3 buckets. The company wants to make some decisions regarding data retention, storage and deletion based on S3 usage and cost reports. As a SysOps Administrator, you have been hired to develop a solution to track the costs incurred by each S3 bucket in the AWS account.</p>\n<p>How will you configure this requirement?</p>\n", "answers": ["Configure AWS Budgets to see the cost against each S3 bucket in the AWS account", "Use AWS Simple Monthly Calculator to check the cost against each S3 bucket in your AWS account", "Use AWS Trusted Advisor's rich set of best practice checks to configure cost utilization for individual S3 buckets. Trusted advisor also provides recommendations based on the findings derived from analyzing your AWS cloud architecture", "Add a common tag to each bucket. Activate the tag as a cost allocation tag. Use the AWS Cost Explorer to create a cost report for the tag"], "correct_answer": "Add a common tag to each bucket. Activate the tag as a cost allocation tag. Use the AWS Cost Explorer to create a cost report for the tag", "explanation": "<p>Correct option:</p>\n<p><strong>Add a common tag to each bucket. Activate the tag as a cost allocation tag. Use the AWS Cost Explorer to create a cost report for the tag</strong></p>\n<p>Before you begin, your AWS Identity and Access Management (IAM) policy must have permission to:\nAccess the Billing and Cost Management console, Perform the actions s3:GetBucketTagging and s3:PutBucketTagging.</p>\n<p>Start by adding a common tag to each bucket. Activate the tag as a cost allocation tag. Use the AWS Cost Explorer to create a cost report for the tag. After you create the cost report, you can use it to review the cost of each bucket that has the cost allocation tag that you created.</p>\n<p>You can set up a daily or hourly AWS Cost and Usage report to get more Amazon S3 billing details. However, these reports won't show you who made requests to your buckets. To get more information on certain Amazon S3 billing items, you must enable logging ahead of time. Then, you'll have logs that contain Amazon S3 request details.</p>\n<p>Incorrect options:</p>\n<p><strong>Configure AWS Budgets to see the cost against each S3 bucket in the AWS account</strong> - AWS Budgets gives you the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount. You can also use AWS Budgets to set reservation utilization or coverage targets and receive alerts when your metrics drop below the threshold you define. It cannot showcase cost on each S3 bucket.</p>\n<p><strong>Use AWS Simple Monthly Calculator to check the cost against each S3 bucket in your AWS account</strong> - The AWS Simple Monthly Calculator is an easy-to-use online tool that enables you to estimate the monthly cost of AWS services for your use case based on your expected usage. This useful tool helps estimate the cost of resources, but current use case is not about estimations but being able to understand which bucket is incurring the maximum cost.</p>\n<p><strong>Use AWS Trusted Advisor's rich set of best practice checks to configure cost utilization for individual S3 buckets. Trusted advisor also provides recommendations based on the findings derived from analyzing your AWS cloud architecture</strong> - AWS Trusted Advisor offers a rich set of best practice checks and recommendations across five categories. For Amazon S3 buckets, Trusted Advisor offers checks the following checks- 1) Checks buckets in Amazon S3 that have open access permissions 2)Checks the logging configuration of Amazon S3 buckets- whether it is enabled and for what duration, 3) Checks for Amazon S3 buckets that do not have versioning enabled. Trusted advisor cannot however generate reports for costs incurred on S3 buckets.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-find-bucket-cost/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-find-bucket-cost/</a></p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>\n<p><a href=\"https://aws.amazon.com/getting-started/hands-on/control-your-costs-free-tier-budgets/\">https://aws.amazon.com/getting-started/hands-on/control-your-costs-free-tier-budgets/</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>A company is moving their on-premises technology infrastructure to AWS Cloud. Compliance rules and regulatory guidelines mandate the company to use their own software that needs socket level configurations. As the company is new to AWS Cloud, they have reached out to you for guidance on this requirement.</p>\n<p>As an AWS Certified SysOps Administrator, which option will you suggest for the given requirement?</p>\n", "answers": ["Opt for On-Demand instances that are highly available and require no prior planning", "Opt for Reserved Instances that allow you to plan and help install the necessary software", "Opt for Amazon EC2 Dedicated Host", "Opt for Amazon EC2 Dedicated Instance"], "correct_answer": "Opt for Amazon EC2 Dedicated Host", "explanation": "<p>Correct option:</p>\n<p><strong>Opt for Amazon EC2 Dedicated Host</strong></p>\n<p>An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing per-socket, per-core, or per-VM software licenses, including Windows Server, Microsoft SQL Server, SUSE, and Linux Enterprise Server. Hence, is the right choice for current requirement.</p>\n<p>Differences between Dedicated Hosts and Dedicated Instances:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q4-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Opt for Amazon EC2 Dedicated Instance</strong> - Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.</p>\n<p><strong>Opt for On-Demand instances that are highly available and require no prior planning</strong></p>\n<p><strong>Opt for Reserved Instances that allow you to plan and help install the necessary software</strong></p>\n<p>You cannot install your own software that needs socket level programming on On-Demand or Reserved Instances.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>After a developer had mistakenly shutdown a test instance, the Team Lead has decided to configure termination protection on all the instances. As a systems administrator, you have been tasked to review the termination policy and check its viability for the given requirements.</p>\n<p>Which of the following choices are correct about Amazon EC2 instance's termination policy (Select two)?</p>\n", "answers": ["The DisableApiTermination attribute prevents you from terminating an instance by initiating shutdown from the instance", "The DisableApiTermination attribute does not prevent you from terminating an instance by initiating shutdown from Amazon EC2 console", "You can't enable termination protection for Spot Instances", "To prevent instances that are part of an Auto Scaling group from terminating on scale in, use instance protection", "The DisableApiTermination attribute prevents Amazon EC2 Auto Scaling from terminating an instance"], "correct_answer": ["You can't enable termination protection for Spot Instances", "To prevent instances that are part of an Auto Scaling group from terminating on scale in, use instance protection"], "explanation": "<p>Correct options:</p>\n<p><strong>You can't enable termination protection for Spot Instances</strong> - You can't enable termination protection for Spot Instances—a Spot Instance is terminated when the Spot price exceeds the amount you're willing to pay for Spot Instances. However, you can prepare your application to handle Spot Instance interruptions.</p>\n<p><strong>To prevent instances that are part of an Auto Scaling group from terminating on scale in, use instance protection</strong> - The DisableApiTermination attribute does not prevent Amazon EC2 Auto Scaling from terminating an instance. For instances in an Auto Scaling group, use the following Amazon EC2 Auto Scaling features instead of Amazon EC2 termination protection:</p>\n<ol>\n<li><p>To prevent instances that are part of an Auto Scaling group from terminating on scale in, use instance protection.</p></li>\n<li><p>To prevent Amazon EC2 Auto Scaling from terminating unhealthy instances, suspend the ReplaceUnhealthy process.</p></li>\n<li><p>To specify which instances Amazon EC2 Auto Scaling should terminate first, choose a termination policy.</p></li>\n</ol>\n<p>Incorrect options:</p>\n<p><strong>The <code>DisableApiTermination</code> attribute prevents you from terminating an instance by initiating shutdown from the instance</strong> - This is false. The <code>DisableApiTermination</code> attribute does not prevent you from terminating an instance by initiating shutdown from the instance</p>\n<p><strong>The <code>DisableApiTermination</code> attribute does not prevent you from terminating an instance by initiating shutdown from Amazon EC2 console</strong> - By default, you can terminate your instance using the Amazon EC2 console, command line interface, or API. To prevent your instance from being accidentally terminated using Amazon EC2, you can enable termination protection for the instance.</p>\n<p><strong>The <code>DisableApiTermination</code> attribute prevents Amazon EC2 Auto Scaling from terminating an instance</strong> - The <code>DisableApiTermination</code> attribute does not prevent Amazon EC2 Auto Scaling from terminating an instance.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/terminating-instances.html#Using_ChangingDisableAPITermination\">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/terminating-instances.html#Using_ChangingDisableAPITermination</a></p>\n", "section": "Domain 2: High Availability", "type": "checkbox"}, {"question": "<p>Security and Compliance is a Shared Responsibility between AWS and the customer. As part of this Shared Responsibility, the customer is also responsible for securing the resources that he has procured under his AWS account.</p>\n<p>Which of the following is the responsibility of the customer?</p>\n", "answers": ["For Amazon S3 service, managing the operating system and platform is customer responsibility", "AWS is responsible for patching and fixing flaws within the infrastructure, for patching the guest Operating Systems and applications of the customers", "AWS is responsible for training their customers and their employees as part of Customer Specific training", "For Amazon EC2 service, managing guest operating system (including updates and security patches), application software and Security Groups is the responsibility of the customer"], "correct_answer": "For Amazon EC2 service, managing guest operating system (including updates and security patches), application software and Security Groups is the responsibility of the customer", "explanation": "<p>Correct option:</p>\n<p><strong>For Amazon EC2 service, managing guest operating system (including updates and security patches), application software and Security Groups is responsibility of the customer</strong></p>\n<p>Customer responsibility will be determined by the AWS Cloud services that a customer selects. This determines the amount of configuration work the customer must perform as part of their security responsibilities. For example, a service such as Amazon Elastic Compute Cloud (Amazon EC2) is categorized as Infrastructure as a Service (IaaS) and, as such, requires the customer to perform all of the necessary security configuration and management tasks. Customers that deploy an Amazon EC2 instance are responsible for management of the guest operating system (including updates and security patches), any application software or utilities installed by the customer on the instances, and the configuration of the AWS-provided firewall (called a security group) on each instance.</p>\n<p>AWS Shared Responsibility Model:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q5-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/compliance/shared-responsibility-model/\">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p>\n<p>Incorrect options:</p>\n<p><strong>For Amazon S3 service, managing the operating system and platform is customer responsibility</strong> - For abstracted services, such as Amazon S3, AWS operates the infrastructure layer, the operating system, and platforms, and customers access the endpoints to store and retrieve data. Customers are responsible for managing their data (including encryption options), classifying their assets, and using IAM tools to apply the appropriate permissions.</p>\n<p><strong>AWS is responsible for patching and fixing flaws within the infrastructure, for patching the guest Operating Systems and applications of the customers</strong> - As part of Patch management, AWS is responsible for patching and fixing flaws within the infrastructure, but customers are responsible for patching their guest OS and applications.</p>\n<p><strong>AWS is responsible for training their customers and their employees as part of Customer Specific training</strong> - As part of Awareness &amp; Training, AWS trains AWS employees, but a customer must train their own employees.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/compliance/shared-responsibility-model/\">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A web application runs on multiple Amazon EC2 instances that are configured behind an Auto Scaling Group (ASG). The company wants the scaling action to begin as soon as CPU utilization crosses 50% for the instances.</p>\n<p>Which is the right way to configure this scaling action?</p>\n", "answers": ["Configure ASG to use predictive scaling", "Configure ASG to scale based on a schedule", "Configure ASG to scale based on demand", "Configure ASG to scale based on events"], "correct_answer": "Configure ASG to scale based on demand", "explanation": "<p>Correct option:</p>\n<p><strong>Configure ASG to scale based on demand</strong> - When you configure dynamic scaling ( Scaling on Demand), you define how to scale the capacity of your Auto Scaling group in response to changing demand.</p>\n<p>For example, let's say that you have a web application that currently runs on two instances, and you want the CPU utilization of the Auto Scaling group to stay at around 50 percent when the load on the application changes. This gives you extra capacity to handle traffic spikes without maintaining an excessive number of idle resources.</p>\n<p>You can configure your Auto Scaling group to scale dynamically to meet this need by creating a scaling policy. Amazon EC2 Auto Scaling can then scale out your group (add more instances) to deal with high demand at peak times, and scale in your group (run fewer instances) to reduce costs during periods of low utilization.</p>\n<p>Incorrect options:</p>\n<p><strong>Configure ASG to use predictive scaling</strong> - You can also use Amazon EC2 Auto Scaling in combination with AWS Auto Scaling to scale resources across multiple services. AWS Auto Scaling can help you maintain optimal availability and performance by combining predictive scaling and dynamic scaling (proactive and reactive approaches, respectively) to scale your Amazon EC2 capacity faster. Scaling based on demand is a direct way to achieve the current requirement and hence is the correct option.</p>\n<p><strong>Configure ASG to scale based on a schedule</strong> - Scaling by schedule means that scaling actions are performed automatically as a function of time and date. This is useful when you know exactly when to increase or decrease the number of instances in your group, simply because the need arises on a predictable schedule.</p>\n<p><strong>Configure ASG to scale based on events</strong> - This is a made-up option and given only as a distractor.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/scaling_plan.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/scaling_plan.html</a></p>\n", "section": "Domain 2: High Availability", "type": "radio"}, {"question": "<p>Two AWS CloudFormation stack policies are shared below.</p>\n<p>As a SysOps Administrator, can you identify the actions possible with each of the policies below?</p>\n<p>Policy-1:</p>\n<pre><code>{\n  \"Statement\" : [\n    {\n      \"Effect\" : \"Deny\",\n      \"Action\" : \"Update:*\",\n      \"Principal\": \"*\",\n      \"Resource\" : \"LogicalResourceId/MyDatabase\"\n    },\n    {\n      \"Effect\" : \"Allow\",\n      \"Action\" : \"Update:*\",\n      \"Principal\": \"*\",\n      \"Resource\" : \"*\"\n    }\n  ]\n}\n</code></pre>\n<p>Policy-2:</p>\n<pre><code>{\n  \"Statement\" : [\n    {\n      \"Effect\" : \"Allow\",\n      \"Action\" : \"Update:*\",\n      \"Principal\": \"*\",\n      \"NotResource\" : \"LogicalResourceId/MyDatabase\"\n    }\n  ]\n}\n</code></pre>\n", "answers": ["Policy-1 denies updates on MyDatabase , whereas, Policy-2 allows updates on MyDatabase", "Policy-2 denies updates on MyDatabase , whereas, Policy-1 allows updates on MyDatabase", "Both the policies allow updates on all resources", "Both the policies deny all update actions on the database with the MyDatabase logical ID. And they allow all update actions on all other stack resources"], "correct_answer": "Both the policies deny all update actions on the database with the MyDatabase logical ID. And they allow all update actions on all other stack resources", "explanation": "<p>Correct option:</p>\n<p><strong>Both the policies deny all update actions on the database with the <code>MyDatabase</code> logical ID. And they allow all update actions on all other stack resources</strong></p>\n<p>The policy-1 denies all update actions on the database with the MyDatabase logical ID. It allows all update actions on all other stack resources with an Allow statement. The Allow statement doesn't apply to the MyDatabase resource because the Deny statement always overrides allow actions.</p>\n<p>You can achieve the same result by using a default denial. When you set a stack policy, AWS CloudFormation denies any update that is not explicitly allowed. Policy-2 allows updates to all resources except for the MyDatabase, which is denied by default.</p>\n<p>If a stack policy includes overlapping statements (both allowing and denying updates on a resource), a Deny statement always overrides an Allow statement. To ensure that a resource is protected, use a Deny statement for that resource.</p>\n<p>Incorrect options:</p>\n<p><strong>Policy-1 denies updates on <code>MyDatabase</code>, whereas, Policy-2 allows updates on <code>MyDatabase</code></strong></p>\n<p><strong>Policy-2 denies updates on <code>MyDatabase</code>, whereas, Policy-1 allows updates on <code>MyDatabase</code></strong></p>\n<p><strong>Both the policies allow updates on all resources</strong></p>\n<p>These three options contradict the explanation above, so these options are incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#nested\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#nested</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>A SysOps Administrator has been tasked with copying AMIs from one Region to another. While doing this task, the following error message popped up: Linux error message \"This AMI was copied from an AMI with a kernel that is unavailable in the destination Region: {Image ID}\"</p>\n<p>Which of the following would you identify as the root-cause behind the issue?</p>\n", "answers": ["Linux hardware virtual machine (HVM) AMIs aren't supported in all AWS Regions and copying these across unsupported Regions gives rise to this error", "Linux paravirtual (PV) AMIs aren't supported in all AWS Regions and copying these across unsupported Regions gives this error", "Linux AMIs do not support copy across Regions", "The error is a general indication of AMI not being provisioned correctly"], "correct_answer": "Linux paravirtual (PV) AMIs aren't supported in all AWS Regions and copying these across unsupported Regions gives this error", "explanation": "<p>Correct option:</p>\n<p><strong>Linux paravirtual (PV) AMIs aren't supported in all AWS Regions and copying these across unsupported Regions gives raise to this error</strong></p>\n<p>Linux Amazon Machine Images use one of two types of virtualization: paravirtual (PV) or hardware virtual machine (HVM). The main differences between PV and HVM AMIs are the way in which they boot and whether they can take advantage of special hardware extensions (CPU, network, and storage) for better performance.</p>\n<p>Linux paravirtual (PV) AMIs aren't supported in all AWS Regions. If you receive this message, you can create a new HVM instance, and then attach new EBS volumes to the HVM instance. Then, copy over data from the EBS volumes attached to the old PV instance.</p>\n<p>Incorrect options:</p>\n<p><strong>Linux hardware virtual machine (HVM) AMIs aren't supported in all AWS Regions and copying these across unsupported Regions gives rise to this error</strong> - All Regions support HVM AMIs.</p>\n<p><strong>Linux AMIs do not support copy across Regions</strong> - This statement is incorrect as you can indeed copy AMIs across Regions.</p>\n<p><strong>The error is a general indication of AMI not being provisioned correctly</strong> - This has been added as a distractor.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/copy-ami-region/\">https://aws.amazon.com/premiumsupport/knowledge-center/copy-ami-region/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>As a SysOps Administrator, you manage a large team of IAM user accounts that are part of multiple AWS accounts. With the growing team size, you have decided to divide IAM users into IAM groups to be able to manage the permissions and policies better.</p>\n<p>Which of the following statements are valid about IAM Groups? (Select two)</p>\n", "answers": ["An IAM user can belong to multiple IAM groups and an IAM group can be part of another IAM group", "Groups cannot be given security credentials directly, they however can take up an IAM Role to access web services directly", "An IAM user can belong to multiple IAM groups. But, Groups cannot belong to other groups", "Groups can be granted permissions using access control policies", "Groups can be given security credentials to be able to access web services directly"], "correct_answer": ["An IAM user can belong to multiple IAM groups. But, Groups cannot belong to other groups", "Groups can be granted permissions using access control policies"], "explanation": "<p>Correct options:</p>\n<p><strong>An IAM user can belong to multiple IAM groups. But, Groups cannot belong to other groups</strong> - A Group can contain many users, and a user can belong to multiple Groups. But, Groups can't be nested; they can contain only users, not other groups.</p>\n<p><strong>Groups can be granted permissions using access control policies</strong> - Groups can be granted permissions using access control policies. This makes it easier to manage permissions for a collection of users, rather than having to manage permissions for each individual user.</p>\n<p>Incorrect options:</p>\n<p><strong>An IAM user can belong to multiple IAM groups and an IAM group can be part of another IAM group</strong> - As discussed above, groups cannot be nested.</p>\n<p><strong>Groups cannot be given security credentials directly, they however can take up an IAM Role to access web services directly</strong> - You cannot assign an IAM Role to an IAM group.</p>\n<p><strong>Groups can be given security credentials to be able to access web services directly</strong> - Groups do not have security credentials, and cannot access web services directly; they exist solely to make it easier to manage user permissions.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/iam/faqs/\">https://aws.amazon.com/iam/faqs/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "checkbox"}, {"question": "<p>A multi-national company runs its technology infrastructure on a fleet of Amazon EC2 instances, Elastic Load Balancers, Amazon RDS and Amazon S3 storage. The company already uses AWS Config to track and maintain configuration changes. The company also uses AWS Systems Manager for maintaining their AWS infrastructure.</p>\n<p>Which feature of AWS Systems Manager can help with configuration management, software audit and integration with AWS Config?</p>\n", "answers": ["Use Systems Manager Automation that simplifies common maintenance and configuration tasks", "Use AWS Systems Manager Inventory that provides visibility into your Amazon EC2 and on-premises computing environment", "Use AWS Systems Manager Patch Manager that automates the process of patching managed instances with both security related and other types of updates", "Use AWS Systems Manager Maintenance Windows to help you define a schedule to check configurations and track any needed changes"], "correct_answer": "Use AWS Systems Manager Inventory that provides visibility into your Amazon EC2 and on-premises computing environment", "explanation": "<p>Correct option:</p>\n<p><strong>Use AWS Systems Manager Inventory that provides visibility into your Amazon EC2 and on-premises computing environment</strong></p>\n<p>AWS Systems Manager Inventory provides visibility into your Amazon EC2 and on-premises computing environment. You can use Inventory to collect metadata from your managed instances. You can store this metadata in a central Amazon Simple Storage Service (Amazon S3) bucket, and then use built-in tools to query the data and quickly determine which instances are running the software and configurations required by your software policy, and which instances need to be updated. You can configure Inventory on all of your managed instances by using a one-click procedure. You can also configure and view inventory data from multiple AWS Regions and accounts.</p>\n<p>Use Systems Manager Inventory with AWS Config to audit your application configurations over time.</p>\n<p>Incorrect options:</p>\n<p><strong>Use Systems Manager Automation that simplifies common maintenance and configuration tasks</strong> - Systems Manager Automation simplifies common maintenance and deployment tasks of EC2 instances and other AWS resources. Automation enables you to - Build Automation workflows to configure and manage instances and AWS resources, Receive notifications about Automation tasks and workflows by using Amazon EventBridge. This feature is not useful for tracking configuration changes.</p>\n<p><strong>Use AWS Systems Manager Patch Manager that automates the process of patching managed instances with both security related and other types of updates</strong> - AWS Systems Manager Patch Manager automates the process of patching managed instances with both security related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. This feature is not useful for tracking configuration changes.</p>\n<p><strong>Use AWS Systems Manager Maintenance Windows to help you define a schedule to check configurations and track any needed changes</strong> - AWS Systems Manager Maintenance Windows let you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-best-practices.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-best-practices.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-inventory.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>An analytics company generates reports for various client applications, some of which have critical data. As per the company's compliance guidelines, data has to be encrypted during data exchange, for all channels of communication. An Amazon S3 bucket is configured as website endpoint and this is now being added as custom origin for CloudFront.</p>\n<p>How will you secure this channel, as per company's requirements?</p>\n", "answers": ["Configure CloudFront that mandates viewers to use HTTPS to request objects from S3. Configure S3 bucket to support HTTPS communication only. This will force CloudFront to use HTTPS for communication between CloudFront and S3", "Configure CloudFront to mandate viewers to use HTTPS to request objects from S3. However, CloudFront and S3 will use HTTP to communicate with each other", "Communication between CloudFront and Amazon S3 is always on HTTP protocol since the network used for communication is internal to AWS and is inherently secure", "CloudFront always forwards requests to S3 by using the protocol that viewers used to submit the requests. So, we only need to configure CloudFront to mandate use of HTTPS for users"], "correct_answer": "Configure CloudFront to mandate viewers to use HTTPS to request objects from S3. However, CloudFront and S3 will use HTTP to communicate with each other", "explanation": "<p>Correct option:</p>\n<p><strong>Configure CloudFront to mandate viewers to use HTTPS to request objects from S3. CloudFront and S3 will use HTTP to communicate with each other</strong></p>\n<p>If your Amazon S3 bucket is configured as a website endpoint, you can't configure CloudFront to use HTTPS to communicate with your origin because Amazon S3 doesn't support HTTPS connections in that configuration.</p>\n<p>HTTPS for Communication Between CloudFront and Your Amazon S3 Origin:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q11-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-s3-origin.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-s3-origin.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Configure CloudFront that mandates viewers to use HTTPS to request objects from S3. Configure S3 bucket to support HTTPS communication only. This will force CloudFront to use HTTPS for communication between CloudFront and S3</strong> - As discussed above, HTTPS between CloudFront and Amazon S3 is not supported when S3 bucket is configured as a website endpoint.</p>\n<p><strong>Communication between CloudFront and Amazon S3 is always on HTTP protocol since the network used for communication is internal to AWS and is inherently secure</strong> - When your origin is an Amazon S3 bucket, your options for using HTTPS for communications with CloudFront depend on how you're using the bucket. If your Amazon S3 bucket is configured as a website endpoint, you can't configure CloudFront to use HTTPS to communicate with your origin.</p>\n<p>When your origin is an Amazon S3 bucket that supports HTTPS communication, CloudFront always forwards requests to S3 by using the protocol that viewers used to submit the requests.</p>\n<p><strong>CloudFront always forwards requests to S3 by using the protocol that viewers used to submit the requests. So, we only need to configure CloudFront to mandate use of HTTPS for users</strong> - This option has been added as a distractor. As mentioned earlier, if your Amazon S3 bucket is configured as a website endpoint, you can't configure CloudFront to use HTTPS while communication with S3.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-s3-origin.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-s3-origin.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A junior developer is tasked with creating necessary configurations for AWS CloudFormation that is extensively used in a project. After declaring the necessary stack policy, the developer realized that the users still do not have access to stack resources. The stack policy created by the developer looks like so:</p>\n<pre><code>{\n  \"Statement\" : [\n    {\n      \"Effect\" : \"Allow\",\n      \"Action\" : \"Update:*\",\n      \"Principal\": \"*\",\n      \"Resource\" : \"*\"\n    },\n    {\n      \"Effect\" : \"Deny\",\n      \"Action\" : \"Update:*\",\n      \"Principal\": \"*\",\n      \"Resource\" : \"LogicalResourceId/ProductionDatabase\"\n    }\n  ]\n}\n</code></pre>\n<p>Why are the users unable to access the stack resources even after giving access permissions to all?</p>\n", "answers": ["A stack policy applies only during stack updates, it doesn't provide access controls. The developer needs to provide access through IAM policies", "The stack policy is invalid and hence the users are not granted any permissions. The developer needs to fix the syntactical errors in the policy", "Stack policies do not allow wildcard character value ( * ) for the Principal element of the policy", "Stack policies are associated to a particular IAM role or an IAM user. Hence, they only work for the users you have explicitly attached the policy to"], "correct_answer": "A stack policy applies only during stack updates, it doesn't provide access controls. The developer needs to provide access through IAM policies", "explanation": "<p>Correct option:</p>\n<p><strong>A stack policy applies only during stack updates, it doesn't provide access controls. The developer needs to provide access through IAM policies</strong> - When you create a stack, all update actions are allowed on all resources. By default, anyone with stack update permissions can update all of the resources in the stack. You can prevent stack resources from being unintentionally updated or deleted during a stack update by using a stack policy. A stack policy is a JSON document that defines the update actions that can be performed on designated resources.</p>\n<p>After you set a stack policy, all of the resources in the stack are protected by default. To allow updates on specific resources, you specify an explicit Allow statement for those resources in your stack policy. You can define only one stack policy per stack, but, you can protect multiple resources within a single policy.</p>\n<p>A stack policy applies only during stack updates. It doesn't provide access controls like an AWS Identity and Access Management (IAM) policy. Use a stack policy only as a fail-safe mechanism to prevent accidental updates to specific stack resources. To control access to AWS resources or actions, use IAM.</p>\n<p>Incorrect options:</p>\n<p><strong>The stack policy is invalid and hence the users are not granted any permissions. The developer needs to fix the syntactical errors in the policy</strong> - This statement is incorrect and given only as a distractor.</p>\n<p><strong>Stack policies do not allow wildcard character value (<code>*</code>) for the <code>Principal</code> element of the policy</strong> - The Principal element specifies the entity that the policy applies to. This element is required while creating a policy, but supports only the wildcard (*), which means that the policy applies to all principals.</p>\n<p><strong>Stack policies are associated to a particular IAM role or an IAM user. Hence, they only work for the users you have explicitly attached the policy to</strong> - A stack policy applies to all AWS CloudFormation users who attempt to update the stack. You can't associate different stack policies with different users.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>A large IT company uses several AWS accounts for the different lines of business. Quite often, the systems administrator is faced with the problem of sharing Customer Master Keys (CMKs) across multiple AWS accounts for accessing AWS resources spread across these accounts.</p>\n<p>How will you implement a solution to address this issue?</p>\n", "answers": ["The key policy for the CMK must give the external account (or users and roles in the external account) permission to use the CMK. IAM policies in the external account must delegate the key policy permissions to its users and roles", "Use AWS KMS service-linked roles to share access across AWS accounts", "AWS Owned CMK can be used across AWS accounts. Configure an AWS Owned CMK and use it across accounts that need to share the key material", "Declare a key policy for the CMK to give the external account permission to use the CMK. This key policy should be embedded with the first request of every transaction"], "correct_answer": "The key policy for the CMK must give the external account (or users and roles in the external account) permission to use the CMK. IAM policies in the external account must delegate the key policy permissions to its users and roles", "explanation": "<p>Correct option:</p>\n<p><strong>The key policy for the CMK must give the external account (or users and roles in the external account) permission to use the CMK. IAM policies in the external account must delegate the key policy permissions to its users and roles</strong></p>\n<p>You can allow IAM users or roles in one AWS account to use a customer master key (CMK) in a different AWS account. You can add these permissions when you create the CMK or change the permissions for an existing CMK.</p>\n<p>To give permission to use a CMK to users and roles in another account, you must use two different types of policies:</p>\n<ol>\n<li><p>The key policy for the CMK must give the external account (or users and roles in the external account) permission to use the CMK. The key policy is in the account that owns the CMK.</p></li>\n<li><p>IAM policies in the external account must delegate the key policy permissions to its users and roles. These policies are set in the external account and give permissions to users and roles in that account.</p></li>\n</ol>\n<p>Incorrect options:</p>\n<p><strong>AWS Owned CMK can be used across AWS accounts. Configure an AWS Owned CMK and use it across accounts that need to share the key material</strong> - AWS owned CMKs are a collection of CMKs that an AWS service owns and manages for use in multiple AWS accounts. However, you cannot view, use, track, or audit them</p>\n<p><strong>Use AWS KMS service-linked roles to share access across AWS accounts</strong> - AWS Key Management Service uses AWS Identity and Access Management (IAM) service-linked roles. A service-linked role is a unique type of IAM role that is linked directly to AWS KMS. Service-linked roles are defined by AWS KMS and include all the permissions that the service requires to call other AWS services on your behalf. You cannot use AWS KMS service-linked roles to share access across AWS accounts.</p>\n<p><strong>Declare a key policy for the CMK to give the external account permission to use the CMK. This key policy should be embedded with the first request of every transaction</strong> - Key policy can not be directly shared across accounts.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html\">https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-owned-cmk\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-owned-cmk</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>An e-commerce company is running its server infrastructure on Amazon EC2 instance store-backed instances. For better performance, the company has decided to move their applications to another Amazon EC2 instance store-backed instance with a different instance type.</p>\n<p>How will you configure a solution for this requirement?</p>\n", "answers": ["You can't resize an instance store-backed instance. Instead, you choose a new compatible instance and move your application to the new instance", "You can't resize an instance store-backed instance. Instead, configure an EBS volume to be the root device for the instance and migrate using the EBS volume", "Create an image of your instance, and then launch a new instance from this image with the instance type that you need. Take any Elastic IP address that you've associated with your original instance and associate it with the new instance for uninterrupted service to your application", "Create an image of your instance, and then launch a new instance from this image with the instance type that you need. Any public IP address associated with the instance can be moved with the instance for uninterrupted access of services"], "correct_answer": "Create an image of your instance, and then launch a new instance from this image with the instance type that you need. Take any Elastic IP address that you've associated with your original instance and associate it with the new instance for uninterrupted service to your application", "explanation": "<p>Correct option:</p>\n<p><strong>Create an image of your instance, and then launch a new instance from this image with the instance type that you need. Take any Elastic IP address that you've associated with your original instance and associate it with the new instance for uninterrupted service to your application</strong></p>\n<p>When you want to move your application from one instance store-backed instance to an instance store-backed instance with a different instance type, you must migrate it by creating an image from your instance, and then launching a new instance from this image with the instance type that you need. To ensure that your users can continue to use the applications that you're hosting on your instance uninterrupted, you must take any Elastic IP address that you've associated with your original instance and associate it with the new instance. Then you can terminate the original instance.</p>\n<p>Complete steps to migrate an instance store-backed instance:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q14-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>You can't resize an instance store-backed instance. Instead, you choose a new compatible instance and move your application to the new instance</strong> - An instance store-backed EC2 instance can be resized, as explained above.</p>\n<p><strong>You can't resize an instance store-backed instance. Instead, configure an EBS volume to be the root device for the instance and migrate using the EBS volume</strong> - This statement is incorrect.</p>\n<p><strong>Create an image of your instance, and then launch a new instance from this image with the instance type that you need. Any public IP address associated with the instance can be moved with the instance for uninterrupted access of services</strong> - Public IP addresses are released when an instance is changed. You need an Elastic IP to keep the service uninterrupted for users, since these can be moved across instances.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>A developer is tasked with cleaning up obsolete resources. When he tried to delete an AWS CloudFormation stack, the stack deletion process returned without any error or a success message. The stack was not deleted either.</p>\n<p>What is the reason for this behavior and how will you fix it?</p>\n", "answers": ["The AWS user who initiated the stack deletion does not have enough permsissions to carry out the action", "Some resources must be empty before they can be deleted. Such resources will not be deleted if they are not empty and stack deletion fails without any error", "If you attempt to delete a stack with termination protection enabled, the deletion fails and the stack - including its status - remains unchanged", "Dependent resources should be deleted first, before deleting rest of the resources in the stack. If this order is not followed, then stack deletion fails without an error"], "correct_answer": "If you attempt to delete a stack with termination protection enabled, the deletion fails and the stack - including its status - remains unchanged", "explanation": "<p>Correct option:</p>\n<p><strong>If you attempt to delete a stack with termination protection enabled, the deletion fails and the stack - including its status - remains unchanged</strong></p>\n<p>You cannot delete stacks that have termination protection enabled. If you attempt to delete a stack with termination protection enabled, the deletion fails and the stack - including its status - remains unchanged. Disable termination protection on the stack, then perform the delete operation again.</p>\n<p>This includes nested stacks whose root stacks have termination protection enabled. Disable termination protection on the root stack, then perform the delete operation again. It is strongly recommended that you do not delete nested stacks directly, but only delete them as part of deleting the root stack and all its resources.</p>\n<p>Complete steps for Stack deletion:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q15-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-delete-stack.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-delete-stack.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>The AWS user who initiated the stack deletion does not have enough permsissions to carry out the action</strong> - If user does not have enough permissions to delete the stack, an error explaining the same is displayed and stack will be in the DELETE_FAILED state.</p>\n<p><strong>Some resources must be empty before they can be deleted. Such resources will not be deleted if they are not empty and stack deletion fails without any error</strong> - Some resources must be empty before they can be deleted. For example, you must delete all objects in an Amazon S3 bucket or remove all instances in an Amazon EC2 security group before you can delete the bucket or security group. Otherwise, stack delete fails and stack will be in the DELETE_FAILED state.</p>\n<p><strong>Dependent resources should be deleted first, before deleting rest of the resources in the stack. If this order is not followed, then stack deletion fails without an error</strong> - Any error during stack deletion will result in stack being in DELETE_FAILED state.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/troubleshooting.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>An organization that started as a single AWS account, gradually moved to a multi-account set up. The organization also has multiple AWS environments in each account, that were being managed at account level. Backups are a big part of this management task. The organization is looking at moving to a centralized backup management process that consolidates and automates Cross-Region backup tasks across AWS accounts.</p>\n<p>Which of the solutions below is the right choice for this requirement?</p>\n", "answers": ["Configure AWS Systems Manager Maintenance Windows to schedule backup tasks as per company's policies. Tag the resources to help identify them by the AWS environment they run in. Amazon CloudWatch dashboards hosted by Systems Manager to get an overall view of status of all resources under the AWS account", "Use Amazon EventBridge to create a workflow for scheduled backup of all AWS resources under an account. Amazon S3 lifecycle policies, Amazon EC2 instance backups and Amazon RDS backups can be used to create the events for the EventBridge. The same workflow can be scheduled to work on production and non-production environments, based on the tags created", "Create a backup plan in AWS Backup. Assign tags to resources based on the environment ( Production, Development, Testing). Create one backup policy for production environments and one backup policy for non-production environments. Schedule the backup plan based on the organization's backup policies", "Use Amazon Data Lifecycle Manager to manage creation, deletion and managing of all the AWS resources under an account. Tag all the resources that need to be backed up and use lifecycle policies to customize the backup management to cater to the needs of the organization"], "correct_answer": "Create a backup plan in AWS Backup. Assign tags to resources based on the environment ( Production, Development, Testing). Create one backup policy for production environments and one backup policy for non-production environments. Schedule the backup plan based on the organization's backup policies", "explanation": "<p>Correct option:</p>\n<p><strong>Create a backup plan in AWS Backup. Assign tags to resources based on the environment ( Production, Development, Testing). Create one backup policy for production environments and one backup policy for non-production environments. Schedule the backup plan based on the organization's backup policies</strong></p>\n<p>AWS Backup is a fully managed and cost-effective backup service that simplifies and automates data backup across AWS services including Amazon EBS, Amazon EC2, Amazon RDS, Amazon Aurora, Amazon DynamoDB, Amazon EFS, and AWS Storage Gateway. In addition, AWS Backup leverages AWS Organizations to implement and maintain a central view of backup policy across resources in multi-account AWS environment. Customers simply tag and associate their AWS resources with backup policies managed by AWS Backup for Cross-Region data replication.</p>\n<p>The following post shows how to centrally manage backup tasks across AWS accounts in your organization by deploying backup policies with AWS Backup.</p>\n<p>Example AWS Backup Architecture:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q16-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/blogs/storage/centralized-cross-account-management-with-cross-region-copy-using-aws-backup/\">https://aws.amazon.com/blogs/storage/centralized-cross-account-management-with-cross-region-copy-using-aws-backup/</a></p>\n<p>Incorrect options:</p>\n<p><strong>Configure AWS Systems Manager Maintenance Windows to schedule backup tasks as per company's policies. Tag the resources to help identify them by the AWS environment they run in. Amazon CloudWatch dashboards hosted by Systems Manager to get an overall view of status of all resources under the AWS account</strong></p>\n<p>AWS Systems Manager Maintenance Windows let you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches. Although a useful service, it is not suited for the given requirements.</p>\n<p><strong>Use Amazon EventBridge to create a workflow for scheduled backup of all AWS resources under an account. Amazon S3 lifecycle policies, Amazon EC2 instance backups and Amazon RDS backups can be used to create the events for the EventBridge. The same workflow can be scheduled to work on production and non-production environments, based on the tags created</strong> - Amazon EventBridge is a serverless event bus that makes it easy to connect applications together using data from your own applications, integrated Software-as-a-Service (SaaS) applications, and AWS services. It is possible to build a backup solution using EventBridge, but it will not be an optimized one, since AWS offers services with better features for centrally managing backups.</p>\n<p><strong>Use Amazon Data Lifecycle Manager to manage creation, deletion and managing of all the AWS resources under an account. Tag all the resources that need to be backed up and use lifecycle policies to customize the backup management to cater to the needs of the organization</strong> - DLM provides a simple way to manage the lifecycle of EBS resources, such as volume snapshots. You should use DLM when you want to automate the creation, retention, and deletion of EBS snapshots.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/blogs/storage/centralized-cross-account-management-with-cross-region-copy-using-aws-backup/\">https://aws.amazon.com/blogs/storage/centralized-cross-account-management-with-cross-region-copy-using-aws-backup/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-maintenance.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-maintenance.html</a></p>\n<p><a href=\"https://aws.amazon.com/eventbridge/\">https://aws.amazon.com/eventbridge/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>The development team at an IT company is looking at moving its web applications to Amazon EC2 instances. The team is weighing its options for EBS volumes and instance store-backed instances for these applications with varied workloads.</p>\n<p>Which of the following would you identify as correct regarding instance store and EBS volumes? (Select three)</p>\n", "answers": ["Use separate Amazon EBS volumes for the operating system and your data, even though root volume persistence feature is available", "Data stored in instance store is preserved when you stop or terminate your instance. However, data is lost when you hibernate the instance. Configure EBS volumes or have a backup plan to avoid using critical data to this behavior", "EBS snapshots only capture data that has been written to your Amazon EBS volume, which might exclude any data that has been locally cached by your application or operating system", "By default, data on a non-root EBS volume is preserved even if the instance is shutdown or terminated", "EBS encryption does not support boot volumes", "Snapshots of EBS volumes, stored on Amazon S3, can be accessed using Amazon S3 APIs"], "correct_answer": ["Use separate Amazon EBS volumes for the operating system and your data, even though root volume persistence feature is available", "EBS snapshots only capture data that has been written to your Amazon EBS volume, which might exclude any data that has been locally cached by your application or operating system", "By default, data on a non-root EBS volume is preserved even if the instance is shutdown or terminated"], "explanation": "<p>Correct options:</p>\n<p><strong>Use separate Amazon EBS volumes for the operating system and your data, even though root volume persistence feature is available</strong></p>\n<p>As a best practice, AWS recommends use of separate Amazon EBS volumes for the operating system and your data. This ensures that the volume with your data persists even after instance termination or any issues to the operating system.</p>\n<p><strong>EBS snapshots only capture data that has been written to your Amazon EBS volume, which might exclude any data that has been locally cached by your application or operating system</strong></p>\n<p>Snapshots only capture data that has been written to your Amazon EBS volume, which might exclude any data that has been locally cached by your application or OS. To ensure consistent snapshots on volumes attached to an instance, AWS recommends detaching the volume cleanly, issuing the snapshot command, and then reattaching the volume. For Amazon EBS volumes that serve as root devices, AWS recommends shutting down the machine to take a clean snapshot.</p>\n<p><strong>By default, data on a non-root EBS volume is preserved even if the instance is shutdown or terminated</strong></p>\n<p>By default, when you attach a non-root EBS volume to an instance, its DeleteOnTermination attribute is set to false. Therefore, the default is to preserve these volumes. After the instance terminates, you can take a snapshot of the preserved volume or attach it to another instance. You must delete a volume to avoid incurring further charges.</p>\n<p>Incorrect options:</p>\n<p><strong>Data stored in instance store is preserved when you stop or terminate your instance. However, data is lost when you hibernate the instance. Configure EBS volumes or have a backup plan to avoid using critical data to this behavior</strong> - Data stored in instance store is lost when you stop, hibernate or terminate the instance.</p>\n<p><strong>EBS encryption does not support boot volumes</strong> - EBS volumes used as root devices can be encrypted without any issue.</p>\n<p><strong>Snapshots of EBS volumes, stored on Amazon S3, can be accessed using Amazon S3 APIs</strong> - This is incorrect. Snapshots are only available through the Amazon EC2 API.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-best-practices.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-best-practices.html</a></p>\n<p><a href=\"https://aws.amazon.com/ebs/faqs/\">https://aws.amazon.com/ebs/faqs/</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "checkbox"}, {"question": "<p>A retail company has complex AWS VPC architecture that is getting difficult to maintain. The company has decided to configure VPC flow logs to track the network traffic to analyze various traffic flow scenarios. The systems administration team has configured VPC flow logs for one of the VPCs, but it's not able to see any logs. After initial analysis, the team has been able to track the error. It says <code>Access error</code> and the administrator of the team wants to change the IAM Role defined in the flow log definition.</p>\n<p>What is the correct way of configuration a solution for this issue so that the VPC flow logs can be operational?</p>\n", "answers": ["The error indicates that IAM role does not have a trust relationship with the flow logs service. Change the trust relationship from flow log configuration", "The flow log is still in the process of being created. It sometimes takes almost 10 minutes to start the logs", "The error indicates IAM role is not correctly configured. After you've created a flow log, you cannot change its configuration. Instead, you need to delete the flow log and create a new one with the required configuration", "The error indicates an internal error has occurred in the flow logs service. Raise a service request with AWS"], "correct_answer": "The error indicates IAM role is not correctly configured. After you've created a flow log, you cannot change its configuration. Instead, you need to delete the flow log and create a new one with the required configuration", "explanation": "<p>Correct option:</p>\n<p><strong>The error indicates IAM role is not correctly configured. After you've created a flow log, you cannot change its configuration. Instead, you need to delete the flow log and create a new one with the required configuration</strong></p>\n<p>Access error can be caused by one of the following reasons:</p>\n<ol>\n<li><p>The IAM role for your flow log does not have sufficient permissions to publish flow log records to the CloudWatch log group</p></li>\n<li><p>The IAM role does not have a trust relationship with the flow logs service</p></li>\n<li><p>The trust relationship does not specify the flow logs service as the principal</p></li>\n</ol>\n<p>After you've created a flow log, you cannot change its configuration or the flow log record format. For example, you can't associate a different IAM role with the flow log, or add or remove fields in the flow log record. Instead, you can delete the flow log and create a new one with the required configuration.</p>\n<p>Incorrect options:</p>\n<p><strong>The error indicates that IAM role does not have a trust relationship with the flow logs service. Change the trust relationship from flow log configuration</strong> - As discussed above, VPC flow log configuration cannot be changed once created.</p>\n<p><strong>The flow log is still in the process of being created. It sometimes takes almost 10 minutes to start the logs</strong> - This scenario is possible when you have just configured the flow logs. However, the status of the flow logs will not be in <code>error</code> state.</p>\n<p><strong>The error indicates an internal error has occurred in the flow logs service. Raise a service request with AWS</strong> - This is a made-up option, given only as a distractor.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-troubleshooting.html\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-troubleshooting.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html#flow-log-records\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html#flow-log-records</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>A data analytics company runs its technology operations on AWS Cloud using different VPC configurations for each of its applications. A systems administrator wants to configure the Network Access Control List (ACL) and Security Group (SG) of VPC1 to allow access for AWS resources in VPC2.</p>\n<p>Which is the best way of configuring this requirement?</p>\n", "answers": ["Network ACLs and Security Groups share a parent-child relationship. If resources in VPC2 are given inbound and outbound permissions on Network ACLs of VPC1, the resources will get necessary permissions on the associated security groups too", "By default, Security Groups allow outbound traffic. Hence, only the inbound traffic configuration of the security groups have to be changed to allow requests from resources in VPC2 to access instances in VPC1. If the subnet is not associated with any Network ACL, you will not need any configuration changes", "Based on the inbound and outbound traffic configurations on Network ACL of VPC1, you can create a similar deny rules on Security Groups of the instances in VPC1 to deny all traffic, other than the one originating from resources in VPC2", "The Security Groups of instances on VPC1 should be configured to allow inbound traffic from resources in VPC2. By default, Network ACLs allow all inbound and outbound traffic. So, a default Network ACLs on VPC1 will not need any configuration changes"], "correct_answer": "The Security Groups of instances on VPC1 should be configured to allow inbound traffic from resources in VPC2. By default, Network ACLs allow all inbound and outbound traffic. So, a default Network ACLs on VPC1 will not need any configuration changes", "explanation": "<p>Correct option:</p>\n<p><strong>The Security Groups of instances on VPC1 should be configured to allow inbound traffic from resources in VPC2. By default, Network ACLs allow all inbound and outbound traffic. So, a default Network ACLs on VPC1 will not need any configuration changes</strong> - A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC can be assigned to a different set of security groups.</p>\n<p>A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.</p>\n<p>Security groups are stateful — if you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules. Responses to allowed inbound traffic are allowed to flow out, regardless of outbound rules.</p>\n<p>Network ACLs are stateless, which means that responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa).</p>\n<p>Incorrect options:</p>\n<p><strong>Network ACLs and Security Groups share a parent-child relationship. If resources in VPC2 are given inbound and outbound permissions on Network ACLs of VPC1, the resources will get necessary permissions on the associated security groups too</strong> - This is an incorrect statement. Security Groups act at instance level and Network ACLs are at subnet level. They are different levels of security provided by AWS and do not form any hierarchy.</p>\n<p><strong>By default, Security Groups allow outbound traffic. Hence, only the inbound traffic configuration of the security groups have to be changed to allow requests from resources in VPC2 to access instances in VPC1. If the subnet is not associated with any Network ACL, you will not need any configuration changes</strong> - Each subnet in your VPC must be associated with a network ACL. If you don't explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL. Hence, a subnet will always have a network ACL associated with it.</p>\n<p><strong>Based on the inbound and outbound traffic configurations on Network ACL of VPC1, you can create a similar deny rules on Security Groups of the instances in VPC1 to deny all traffic, other than the one originating from resources in VPC2</strong> - Security Groups and Network ACLs are mutually exclusive and do not share permissions. Also, Security Groups can only be used to specify allow rules, and not deny rules.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>A banking service uses Amazon EC2 instances and Amazon RDS databases to run its core business functionalities. The Chief Technology Officer (CTO) of the company has requested granular OS level metrics from the database service for benchmarking.</p>\n<p>As a SysOps Administrator, how will you provide this information?</p>\n", "answers": ["Enable Enhanced Monitoring for your RDS DB instance", "Subscribe to Amazon RDS events to be notified when changes occur with a DB instance and its connected resources", "Subscribe to CloudWatch metrics that track CPU utilization of the instances the RDS is hosted on", "Enable Performance Insights to expand on the existing Amazon RDS monitoring features to illustrate your database's performance"], "correct_answer": "Enable Enhanced Monitoring for your RDS DB instance", "explanation": "<p>Correct option:</p>\n<p><strong>Enable Enhanced Monitoring for your RDS DB instance</strong> - Amazon RDS provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB instance using the console. Also, you can consume the Enhanced Monitoring JSON output from Amazon CloudWatch Logs in a monitoring system of your choice.</p>\n<p>By default, Enhanced Monitoring metrics are stored for 30 days in the CloudWatch Logs, which are different from typical CloudWatch metrics. Enhanced Monitoring for RDS provides the following OS metrics:\n1.Free Memory\n2.Active Memory\n3.Swap Free\n4.Processes Running\n5.File System Used</p>\n<p>You can use these metrics to understand the environment's performance, and these metrics are ingested by Amazon CloudWatch Logs as log entries. You can use CloudWatch to create alarms based on metrics. These alarms run actions, and you can publish these metrics from within your infrastructure, device, or application into CloudWatch as a custom metric. By using Enhanced Monitoring and CloudWatch together, you can automate tasks by creating a custom metric for the CloudWatch Logs RDS ingested date from the Enhanced Monitoring metrics. Enhanced Monitoring metrics are useful when you want to see how different processes or threads on a DB instance use the CPU.</p>\n<p>Incorrect options:</p>\n<p><strong>Subscribe to Amazon RDS events to be notified when changes occur with a DB instance and its connected resources</strong> - Subscribe to Amazon RDS events to be notified when changes occur with a DB instance, DB snapshot, DB parameter group, or DB security group. Amazon RDS uses the Amazon Simple Notification Service (Amazon SNS) to provide notification when an Amazon RDS event occurs. This option is not relevant for the given use-case.</p>\n<p><strong>Subscribe to CloudWatch metrics that track CPU utilization of the instances the RDS is hosted on</strong> - CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, and Enhanced Monitoring gathers its metrics from an agent on the instance. As a result, you might find differences between the measurements, because the hypervisor layer performs a small amount of work. The differences can be greater if your DB instances use smaller instance classes, because then there are likely more virtual machines (VMs) that are managed by the hypervisor layer on a single physical instance. Enhanced Monitoring metrics are useful when you want to see how different processes or threads on a DB instance use the CPU.</p>\n<p><strong>Enable Performance Insights to expand on the existing Amazon RDS monitoring features to illustrate your database's performance</strong> - Performance Insights collects metric data from the database engine to monitor the actual load on a database. Performance Insights will not help in gathering granular OS level metrics.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/custom-cloudwatch-metrics-rds/\">https://aws.amazon.com/premiumsupport/knowledge-center/custom-cloudwatch-metrics-rds/</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>An e-commerce company has used Aurora Serverless MySQL compatible DB clusters for deploying a new application to understand its capacity needs. Based on the scaling actions of Aurora, the company will decide on the database requirements for deploying the new application. In this context, the company wants to audit the database activity, collect and publish the logs generated by Aurora Serverless to Amazon CloudWatch.</p>\n<p>What configuration steps are needed for this requirement?</p>\n", "answers": ["You can view the logs directly from the Amazon Relational Database Service (Amazon RDS) console", "Aurora Serverless cluster is integrated with Amazon CloudWatch and logs are sent automatically", "For MySQL-compatible DB clusters, you can enable the slow query log, general log, or audit logs to get a view of the database activity", "Aurora Serverless connects to a proxy fleet of DB instances and hence you cannot see the log files. You can connect with your AWS support contact to get help on this requirement"], "correct_answer": "For MySQL-compatible DB clusters, you can enable the slow query log, general log, or audit logs to get a view of the database activity", "explanation": "<p>Correct option:</p>\n<p><strong>For MySQL-compatible DB clusters, you can enable the slow query log, general log, or audit logs to get a view of the database activity</strong></p>\n<p>By design, Aurora Serverless connects to a proxy fleet of DB instances that scales automatically. Because there isn't a direct DB instance to access and host the log files, you can't view the logs directly from the Amazon Relational Database Service (Amazon RDS) console. However, you can view and download logs that are sent to the CloudWatch console.</p>\n<p>To enable logs, first modify the cluster parameter groups for an Aurora serverless cluster. For MySQL-compatible DB clusters, you can enable the slow query log, general log, or audit logs.</p>\n<p>Complete steps for enabling Aurora serverless logs for MySQL:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q21-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/aurora-serverless-logs-enable-view/\">https://aws.amazon.com/premiumsupport/knowledge-center/aurora-serverless-logs-enable-view/</a></p>\n<p>Incorrect options:</p>\n<p><strong>You can view the logs directly from the Amazon Relational Database Service (Amazon RDS) console</strong> - Because there isn't a direct DB instance to access and host the log files, you can't view the logs directly from the Amazon Relational Database Service (Amazon RDS) console.</p>\n<p><strong>Aurora Serverless cluster is integrated with Amazon CloudWatch and logs are sent automatically</strong> - Aurora Serverless will automatically publish the logs to CloudWatch, if Aurora cluster is configured as mentioned above. They are not enabled by default.</p>\n<p><strong>Aurora Serverless connects to a proxy fleet of DB instances and hence you cannot see the log files. You can connect with your AWS support contact to get help on this requirement</strong> - As discussed above, there isn't a direct DB instance to access and host the log files. However, Aurora can be configured as above, for MYSQL to push logs to CloudWatch from where they can be accessed and analyzed.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/aurora-serverless-logs-enable-view/\">https://aws.amazon.com/premiumsupport/knowledge-center/aurora-serverless-logs-enable-view/</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>A retail company has a web application that is deployed on 10 EC2 instances running behind an Application Load Balancer. You have configured your web application to capture the IP address of the client making requests. When viewing the data captured you notice that every IP address being captured is the same, which also happens to be the IP address of the Application Load Balancer.</p>\n<p>As a SysOps Administrator, what should you do to identify the true IP address of the client?</p>\n", "answers": ["Look at the X-Forwarded-Proto header", "Modify the front-end of the website so that the users send their IP in the requests", "Look at the X-Forwarded-For header", "Look at the client's cookie"], "correct_answer": "Look at the X-Forwarded-For header", "explanation": "<p>Correct option:</p>\n<p><strong>Look at the X-Forwarded-For header</strong></p>\n<p>The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to your server.</p>\n<p>Incorrect options:</p>\n<p><strong>Modify the front-end of the website so that the users send their IP in the requests</strong> - When a user makes a request the IP address is sent with the request to the server and the load balancer intercepts it. There is no need to modify the application.</p>\n<p><strong>Look at the X-Forwarded-Proto header</strong> - The X-Forwarded-Proto request header helps you identify the protocol (HTTP or HTTPS) that a client used to connect to your load balancer.</p>\n<p><strong>Look at the client's cookie</strong> - For this, we would need to modify the client-side logic and server-side logic, which would not be efficient.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>A startup is looking at moving their web application to AWS Cloud. The database will be on Amazon RDS and it should not be accessible to the public. The application needs to remain connected to database for the application to work. Also, RDS instance will need access to the internet to download patches every month.</p>\n<p>As a SysOps Administrator, how will you configure a solution for this requirement?</p>\n", "answers": ["Host the application servers in the public subnet and database in private subnet of the VPC. Configure Network Address Translation (NAT) gateway to provide access to internet for both the subnets. The route table of both the subnets will have an entry to NAT gateway", "Host the application servers in public subnet of the VPC and database in a private subnet. Public subnet will connect to internet using an Internet Gateway configured with the VPC. Database in the private subnet will use Network Address Translation (NAT) gateway, present in public subnet, to connect to internet", "Host the application servers in the public subnet and database in private subnet of the VPC. Public subnet will connect to internet using an Internet Gateway configured with the VPC. Use VPC-peering between the private and public subnets to open internet channel for database in private subnet", "Host the application servers in the public subnet and database in private subnet of the VPC. Public subnet will connect to internet using an Internet Gateway configured with the VPC. Private subnets can connect to internet if they are configured using IPv6 protocol"], "correct_answer": "Host the application servers in public subnet of the VPC and database in a private subnet. Public subnet will connect to internet using an Internet Gateway configured with the VPC. Database in the private subnet will use Network Address Translation (NAT) gateway, present in public subnet, to connect to internet", "explanation": "<p>Correct option:</p>\n<p><strong>Host the application servers in public subnet of the VPC and database in a private subnet. Public subnet will connect to internet using an Internet Gateway configured with the VPC. Database in the private subnet will use Network Address Translation (NAT) gateway, present in public subnet, to connect to internet</strong></p>\n<p>For a multi-tier website, with the application servers in a public subnet and the database servers in a private subnet, you can set up security and routing so that the application servers can communicate with the database servers.</p>\n<p>The instances in the public subnet can send outbound traffic directly to the Internet, whereas the instances in the private subnet can't. Instead, the instances in the private subnet can access the Internet by using a network address translation (NAT) gateway that resides in the public subnet. The database servers can connect to the Internet for software updates using the NAT gateway, but the Internet cannot establish connections to the database servers.</p>\n<p>Diagramatic representation of the above solution:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q23-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Host the application servers in the public subnet and database in private subnet of the VPC. Configure Network Address Translation (NAT) gateway to provide access to internet for both the subnets. The route table of both the subnets will have an entry to NAT gateway</strong> - NAT gateway is needed for instances in private subnet to connect to internet. NAT gateway is not used in public subnets, that have access to Internet Gateway.</p>\n<p><strong>Host the application servers in the public subnet and database in private subnet of the VPC. Public subnet will connect to internet using an Internet Gateway configured with the VPC. Use VPC-peering between the private and public subnets to open internet channel for database in private subnet</strong> - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. It is a communication channel between VPCs, not between subnets of a VPC.</p>\n<p><strong>Host the application servers in the public subnet and database in private subnet of the VPC. Public subnet will connect to internet using an Internet Gateway configured with the VPC. Private subnets can connect to internet if they are configured using IPv6 protocol</strong> - IPv6, like IPV4 is an internet protocol, used for communication over internet. IPV6 does not provide internet access, if your instances use IPV6 for communication, you need to configure egress-only Internet gateway to connect to the internet.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>An application hosted on Amazon EC2 instances polls messages from Amazon SQS queue for downstream processing. The team is now looking at configuring an Auto Scaling group to scale using the CloudWatch metrics for Amazon SQS queue to process messages without delays.</p>\n<p>As a Systems Administrator, which feature or dimension of an SQS queue will you pick to collect SQS data from CloudWatch metrics?</p>\n", "answers": ["Queue name of the SQS queue should be used to fetch the necessary data from CloudWatch metrics", "Queue ID should be used to fetch the SQS queue data from the CloudWatch metrics", "A key-value pair of name:<queue name> and value:<value> needs to be considered for fetching SQS queue data from CloudWatch metrics", "A composite key Queue Name - Queue ID is used to fetch SQS queue data from CloudWatch metrics"], "correct_answer": "Queue name of the SQS queue should be used to fetch the necessary data from CloudWatch metrics", "explanation": "<p>Correct option:</p>\n<p><strong>Queue name of the SQS queue should be used to fetch the necessary data from CloudWatch metrics</strong></p>\n<p>Amazon SQS sends a number of metrics to CloudWatch, some of which are <code>ApproximateAgeOfOldestMessage</code>,<code>ApproximateNumberOfMessagesDelayed</code>, <code>NumberOfMessagesDeleted</code> and so on. some metrics are calculated from a service perspective, and can include retries. AWS suggests not rely on the absolute values of these metrics, or use them to estimate current queue status.</p>\n<p>The only dimension that Amazon SQS sends to CloudWatch is QueueName. This means that all available statistics are filtered by QueueName.</p>\n<p>Incorrect options:</p>\n<p><strong>Queue ID should be used to fetch the SQS queue data from the CloudWatch metrics</strong></p>\n<p><strong>A key-value pair of name:&lt;queue name&gt; and value:&lt;value&gt; needs to be considered for fetching SQS queue data from CloudWatch metrics</strong></p>\n<p><strong>A composite key Queue Name - Queue ID is used to fetch SQS queue data from CloudWatch metrics</strong></p>\n<p>These three options contradict the explanation above, so these options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-available-cloudwatch-metrics.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-available-cloudwatch-metrics.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>A retail company runs its server infrastructure on a fleet of Amazon EC2 instances with Amazon RDS as the database service. For high availability of the entire architecture, multi-AZ deployments have been chosen for the RDS instance. A new version of Database engine has been released by the vendor and the company wants to test the release with production data and configurations before upgrading the production instance.</p>\n<p>How will you configure this requirement?</p>\n", "answers": ["Create a configuration similar to the one in production using CloudFormation templates. You can reuse these templates to create any number of instances whenever required", "Create a DB snapshot of your existing DB instance and create a new instance from the restored snapshot. Initiate a version upgrade on this new instance and safely experiment with the instance", "Create a read replica of the RDS instance in production. Upgrade the read replica to the latest version and experiment with this instance", "Procure an instance which has the new version of the database engine. Take the snapshot of your existing database and restore the snapshot to this instance. Test on this instance"], "correct_answer": "Create a DB snapshot of your existing DB instance and create a new instance from the restored snapshot. Initiate a version upgrade on this new instance and safely experiment with the instance", "explanation": "<p>Correct option:</p>\n<p><strong>Create a DB snapshot of your existing DB instance and create a new instance from the restored snapshot. Initiate a version upgrade on this new instance and safely experiment with the instance</strong></p>\n<p>You can trial test the new version before opting it for production systems. To do so, create a DB snapshot of your existing DB instance, restore from the DB snapshot to create a new DB instance, and then initiate a version upgrade for the new DB instance. You can then experiment safely on the upgraded copy of your DB instance before deciding whether or not to upgrade your original DB instance.</p>\n<p>Incorrect options:</p>\n<p><strong>Create a configuration similar to the one in production using CloudFormation templates. You can reuse these templates to create any number of instances whenever required</strong> - CloudFormation templates help in creating resources with same configuration multiple times, in multiple AWS accounts, if needed. Current use-case requires an exact copy of existing production database to trial test and CloudFormation cannot help in this scenario.</p>\n<p><strong>Create a read replica of the RDS instance in production. Upgrade the read replica to the latest version and experiment with this instance</strong></p>\n<p><strong>Procure an instance which has the new version of the database engine. Take the snapshot of your existing database and restore the snapshot to this instance. Test on this instance</strong></p>\n<p>These two options are incorrect because to trial test a production database, we need a running database, in the same status as the existing one. This running database instance will be upgraded and then tested thoroughly to know its viability for production. The read replica operates as a DB instance that allows just read-only connections. Applications can connect to a read replica just as they would to any DB instance. The order of activities should be exactly the way it will be in production, so the upgrade goes smoothly without any glitches when done on live system.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>A bug in an application code has resulted in an EC2 instance's CPU utilization touching almost 100 percent thereby freezing the instance. The instance needs a restart to work normally once it hits this point. It will take a few weeks for the team to fix the issue. Till the bug fix is deployed, you have been tasked to automate the instance restart at the first sign of the instance becoming unresponsive.</p>\n<p>As a SysOps Administrator, how will you configure a solution for this requirement?</p>\n", "answers": ["CPU utilization parameter of an Amazon EC2 instance is a pre-defined metric in CloudWatch and is available in basic monitoring of an instance. Configure the restart action against this metric to automate the instance restart process", "Create a custom code to send CPU utilization of the instance to CloudWatch metrics. Configure an action to restart the instance when the alarm is triggered", "Create a CloudWatch alarm for CPU Utilization of the Amazon EC2 instance, with basic monitoring enabled. Configure an AWS Lambda function against the alarm action. The Lambda function will restart the instance, automating the process", "Create a CloudWatch alarm for CPU Utilization of the Amazon EC2 instance, with detailed monitoring enabled. Configure an action to restart the instance when the alarm is triggered"], "correct_answer": "Create a CloudWatch alarm for CPU Utilization of the Amazon EC2 instance, with detailed monitoring enabled. Configure an action to restart the instance when the alarm is triggered", "explanation": "<p>Correct option:</p>\n<p><strong>Create a CloudWatch alarm for CPU Utilization of the Amazon EC2 instance, with detailed monitoring enabled. Configure an action to restart the instance when the alarm is triggered</strong></p>\n<p>CPUUtilization is an instance metric that is already defined with CloudWatch for monitoring Amazon EC2 instances.</p>\n<p>By default, your instance is enabled for basic monitoring. You can optionally enable detailed monitoring. After you enable detailed monitoring, the Amazon EC2 console displays monitoring graphs with a 1-minute period for the instance. In Basic monitoring, data is available automatically in 5-minute periods at no charge. For Detailed monitoring, data is available in 1-minute periods for an additional charge.</p>\n<p>If you enable detailed monitoring, you are charged per metric that is sent to CloudWatch. You are not charged for data storage.</p>\n<p>You can enable detailed monitoring on an instance as you launch it or after the instance is running or stopped. Enabling detailed monitoring on an instance does not affect the monitoring of the EBS volumes attached to the instance.</p>\n<p>Once the detailed monitoring is active, you can configure an action to restart the instance when the alarm is triggered based on the CPUUtilization metric.</p>\n<p>Steps to enable detail monitoring on the instance:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q26-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>CPU utilization parameter of an Amazon EC2 instance is a pre-defined metric in CloudWatch and is available in basic monitoring of an instance. Configure the restart action against this metric to automate the instance restart process</strong> - In Basic monitoring, data is available at 5-minute periods. Since the customer wants an immediate restart, basic monitoring is not the right choice here.</p>\n<p><strong>Create a custom code to send CPU utilization of the instance to CloudWatch metrics. Configure an action to restart the instance when the alarm is triggered</strong> - Custom code is not needed since CPU utilization is a pre-defined metric that CloudWatch can track for Amazon EC2 instances.</p>\n<p><strong>Create a CloudWatch alarm for CPU Utilization of the Amazon EC2 instance, with basic monitoring enabled. Configure an AWS Lambda function against the alarm action. The Lambda function will restart the instance, automating the process</strong> - Instance restart is a configurable item for an action when an alarm is triggered. This is a straightforward way compared to using Lambda functions.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>Owing to lapses in security, a development team has deleted a Secrets Manager secret. Now, when the team tried to create a new secret with the same name, they ended up with an error - <code>You can't create this secret because a secret with this name is already scheduled for deletion</code>. The secret has be created with the same name to avoid issues in their application.</p>\n<p>How will you recreate the secret with the same name?</p>\n", "answers": ["Use AWS Management Console to delete the key permanently. You will be allowed to create a new key with the same name after the older one is successfully deleted", "When you delete a secret, Secrets Manager deprecates it with a seven-day recovery window. It is not possible to create a new secret with the same name, in this time period", "Use AWS Command Line Interface (AWS CLI) to permanently delete a secret without any recover window, Run the DeleteSecret API call with the ForceDeleteWithoutRecovery parameter", "The secret key deletion is an asynchronous process. There might be a short delay before updates are received. Try after few minutes for successful completion"], "correct_answer": "Use AWS Command Line Interface (AWS CLI) to permanently delete a secret without any recover window, Run the DeleteSecret API call with the ForceDeleteWithoutRecovery parameter", "explanation": "<p>Correct option:</p>\n<p><strong>Use AWS Command Line Interface (AWS CLI) to permanently delete a secret without any recover window, Run the <code>DeleteSecret</code> API call with the <code>ForceDeleteWithoutRecovery</code> parameter</strong></p>\n<p>When you delete a secret, Secrets Manager deprecates it with a seven-day recovery window. This means that you can't recreate a secret using the same name using the AWS Management Console until seven days have passed. You can permanently delete a secret without any recover window using the AWS Command Line Interface (AWS CLI).</p>\n<p>Run the <code>DeleteSecret</code> API call with the <code>ForceDeleteWithoutRecovery</code> parameter to delete the secret permanently. If you receive errors when running AWS CLI commands, make sure that you’re using the most recent version of the AWS CLI. Secrets deleted using the <code>ForceDeleteWithoutRecovery</code> parameter can't be recovered or restored.</p>\n<p>Incorrect options:</p>\n<p><strong>Use AWS Management Console to delete the key permanently. You will be allowed to create a new key with the same name after the older one is successfully deleted</strong> - When you delete a secret, Secrets Manager deprecates it with a seven-day recovery window. This means that you can't recreate a secret using the same name using the AWS Management Console until seven days have passed.</p>\n<p><strong>When you delete a secret, Secrets Manager deprecates it with a seven-day recovery window. It is not possible to create a new secret with the same name, in this time period</strong> - This statement is true, but the secret can be deleted from CLI and hence this option is incorrect.</p>\n<p><strong>The secret key deletion is an asynchronous process. There might be a short delay before updates are received. Try after few minutes for successful completion</strong> - This is a made-up option, given only as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/delete-secrets-manager-secret/\">https://aws.amazon.com/premiumsupport/knowledge-center/delete-secrets-manager-secret/</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A developer is trying to access an Amazon S3 bucket for storing the images used by the web application. The S3 bucket has public read access enabled on it. However, when the developer tries to access the bucket, an error pops up - <code>403 Access Denied</code>. The confused developer has connected with you to know why he has no access to the public S3 bucket.</p>\n<p>As a SysOps Administrator, how will you troubleshoot this issue?</p>\n", "answers": ["Explicit deny statement in the bucket policy can cause access forbidden errors. Check the bucket policy of the S3 bucket", "AWS Organizations service control policy doesn't allow access to Amazon S3 bucket that the developer is trying to access. Service policy needs to be changed using AWS Organizations", "The resource owner which is the AWS account that created the S3 bucket, has access to the bucket. This is an error in creation, delete the S3 bucket and re-create it again", "Run the AWSSupport-TroubleshootS3PublicRead automation document on AWS Systems Manager to help diagnose issues with accessing objects from a public S3 bucket"], "correct_answer": "Run the AWSSupport-TroubleshootS3PublicRead automation document on AWS Systems Manager to help diagnose issues with accessing objects from a public S3 bucket", "explanation": "<p>Correct option:</p>\n<p><strong>Run the <code>AWSSupport-TroubleshootS3PublicRead</code> automation document on AWS Systems Manager to help diagnose issues with accessing objects from a public S3 bucket</strong></p>\n<p>Run the <code>AWSSupport-TroubleshootS3PublicRead</code> automation document on AWS Systems Manager to help you diagnose issues with accessing objects from a public S3 bucket. This document analyzes some permissions settings that affect the bucket and objects, such as the bucket policy and object access control lists (ACLs), among others.</p>\n<p>The <code>AWSSupport-TroubleshootS3PublicRead</code> document analyzes 403 errors from publicly readable objects. The document doesn't evaluate permissions for private objects.</p>\n<p>Complete Steps to run the AWSSupport-TroubleshootS3PublicRead automation document using the Systems Manager console:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q28-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403-public-read/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403-public-read/</a></p>\n<p>Incorrect options:</p>\n<p><strong>Explicit deny statement in the bucket policy can cause access forbidden errors. Check the bucket policy of the S3 bucket</strong></p>\n<p><strong>AWS Organizations service control policy doesn't allow access to Amazon S3 bucket that the developer is trying to access. Service policy needs to be changed using AWS Organizations</strong></p>\n<p>It is possible that either of the above could be true. To know exactly what is causing the error, AWS provides <code>AWSSupport-TroubleshootS3PublicRead</code> automation document on AWS Systems Manager. This is the optimal way of troubleshooting the current issue.</p>\n<p><strong>The resource owner which is the AWS account that created the S3 bucket, has access to the bucket. This is an error in creation, delete the S3 bucket and re-create it again</strong> - It is not mentioned in the use case, if it is resource owner trying to access the S3 bucket.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403-public-read/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-troubleshoot-403-public-read/</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>An e-commerce company has established a Direct Connect connection between AWS Cloud and their on-premises infrastructure. The development team needs to access the Amazon S3 bucket present in their AWS account to pull the customer data for an application hosted on the on-premises infrastructure.</p>\n<p>What is the right way of configuring this requirement?</p>\n", "answers": ["Create a dedicated or hosted connection. Establish a cross-network connection and then create a public virtual interface for your connection. Configure an end router for use with the public virtual interface", "Directly access the S3 bucket through a private virtual interface (VIF) using Direct Connect", "Create VPC gateway endpoint for the S3 bucket you need to access. Then use private virtual interface (VIF) using Direct Connect to access the bucket", "Create VPC interface endpoint for the S3 bucket you need to access. Then use private virtual interface (VIF) using Direct Connect to access the bucket"], "correct_answer": "Create a dedicated or hosted connection. Establish a cross-network connection and then create a public virtual interface for your connection. Configure an end router for use with the public virtual interface", "explanation": "<p>Correct option:</p>\n<p><strong>Create a dedicated or hosted connection. Establish a cross-network connection and then create a public virtual interface for your connection. Configure an end router for use with the public virtual interface</strong></p>\n<p>It's not possible to directly access an S3 bucket through a private virtual interface (VIF) using Direct Connect. This is true even if you have an Amazon Virtual Private Cloud (Amazon VPC) endpoint for Amazon S3 in your VPC, because VPC endpoint connections can't extend outside of a VPC. Additionally, Amazon S3 resolves to public IP addresses, even if you enable a VPC endpoint for Amazon S3.</p>\n<p>However, you can establish access to Amazon S3 using Direct Connect by following these steps (This configuration doesn't require a VPC endpoint for Amazon S3, because traffic doesn't traverse the VPC):</p>\n<ol>\n<li><p>Create a connection. You can request a dedicated connection or a hosted connection.</p></li>\n<li><p>Establish a cross-network connection with the help of your network provider, and then create a public virtual interface for your connection.</p></li>\n<li><p>Configure an end router for use with the public virtual interface.</p></li>\n</ol>\n<p>After the BGP is up and established, the Direct Connect router advertises all global public IP prefixes, including Amazon S3 prefixes. Traffic heading to Amazon S3 is routed through the Direct Connect public virtual interface through a private network connection between AWS and your data center or corporate network.</p>\n<p>Incorrect options:</p>\n<p><strong>Directly access the S3 bucket through a private virtual interface (VIF) using Direct Connect</strong> - Private virtual interface allows access to an Amazon VPC using private IP addresses. It's not possible to directly access an S3 bucket through a private virtual interface (VIF) using Direct Connect.</p>\n<p><strong>Create VPC gateway endpoint for the S3 bucket you need to access. Then use private virtual interface (VIF) using Direct Connect to access the bucket</strong> - VPC endpoint connections can't extend outside of a VPC. Additionally, Amazon S3 resolves to public IP addresses, even if you enable a VPC endpoint for Amazon S3.</p>\n<p><strong>Create VPC interface endpoint for the S3 bucket you need to access. Then use private virtual interface (VIF) using Direct Connect to access the bucket</strong> - VPC interface endpoint is not used for accessing Amazon S3 buckets, we need to use VPC gateway endpoint. As discussed above, VPC endpoint connections can't extend outside of a VPC. Additionally, Amazon S3 resolves to public IP addresses, even if you enable a VPC endpoint for Amazon S3.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-access-direct-connect/</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>A company uses Amazon S3 bucket replication to copy data from one S3 bucket into the other, for compliance purposes. The Technical Lead of the development team wants to be notified if replication of an object across S3 buckets fails.</p>\n<p>How will you configure this automatic notification?</p>\n", "answers": ["Enable S3 Replication with Notification, which allows you to set up notifications for objects that failed replication", "Amazon S3 publishes object events to CloudWatch. Configure Amazon Simple Notification Service (Amazon SNS) to send notifications for replication failure of objects", "Enable S3 Replication Time Control (S3 RTC), which allows you to set up notifications for eligible objects that failed replication", "Use Amazon Simple Queue Service (Amazon SQS) queue to copy objects from one S3 bucket to the other. If replication fails, messages in the queue can be configured to send notification using SNS"], "correct_answer": "Enable S3 Replication Time Control (S3 RTC), which allows you to set up notifications for eligible objects that failed replication", "explanation": "<p>Correct option:</p>\n<p><strong>Enable S3 Replication Time Control (S3 RTC), which allows you to set up notifications for eligible objects that failed replication</strong></p>\n<p>S3 Replication Time Control (S3 RTC) helps you meet compliance or business requirements for data replication and provides visibility into Amazon S3 replication times. S3 RTC replicates most objects that you upload to Amazon S3 in seconds, and 99.99 percent of those objects within 15 minutes.</p>\n<p>S3 RTC by default includes S3 replication metrics and S3 event notifications, with which you can monitor the total number of S3 API operations that are pending replication, the total size of objects pending replication, and the maximum replication time.</p>\n<p>You can track replication time for objects that did not replicate within 15 minutes by monitoring specific event notifications that S3 Replication Time Control (S3 RTC) publishes. These events are published when an object that was eligible for replication using S3 RTC didn't replicate within 15 minutes, and when that object replicates to the destination Region.</p>\n<p>Replication events are available within 15 minutes of enabling S3 RTC. Amazon S3 events are available through Amazon SQS, Amazon SNS, or AWS Lambda.</p>\n<p>Incorrect options:</p>\n<p><strong>Enable S3 Replication with Notification, which allows you to set up notifications for objects that failed replication</strong> - This is a made-up option and given only as a distractor.</p>\n<p><strong>Use Amazon Simple Queue Service (Amazon SQS) queue to copy objects from one S3 bucket to the other. If replication fails, messages in the queue can be configured to send notification using SNS</strong> - Amazon S3 offers direct replication feature. Hence, a custom logic to do the same does not make sense.</p>\n<p><strong>Amazon S3 publishes object events to CloudWatch. Configure Amazon Simple Notification Service (Amazon SNS) to send notifications for replication failure of objects</strong> - Amazon S3 replication events are only published if you have enabled S3 Replication Time Control, for replication.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-time-control.html#using-s3-events-to-track-rtc\">https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-time-control.html#using-s3-events-to-track-rtc</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>An IT company extensively uses Amazon S3 buckets for storage, hosting, backup and compliance specific replication. A Team Lead has reached out to you for creating a report that lists all the objects that have failed replication in the S3 buckets that the project manages. This process needs to be automated as the Team Lead needs this list on a daily basis.</p>\n<p>As a SysOps Administrator, how will you configure a solution for this request?</p>\n", "answers": ["Use Amazon S3 Select to list the objects that have failed replication in the S3 buckets", "Use Amazon S3 Inventory reports to list the objects that have failed replication in the S3 buckets", "Use Amazon S3 Storage Lens to report all the objects that failed replication process in the S3 buckets", "Configure Amazon Simple Queue Service (Amazon SQS) queue against the CloudWatch metrics for S3 replication. You can use custom code to aggregate these messages to get the final list of objects that failed replication"], "correct_answer": "Use Amazon S3 Inventory reports to list the objects that have failed replication in the S3 buckets", "explanation": "<p>Correct option:</p>\n<p><strong>Use Amazon S3 Inventory reports to list the objects that have failed replication in the S3 buckets</strong></p>\n<p>Amazon S3 Inventory is one of the tools Amazon S3 provides to help manage your storage. You can use it to audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs. You can also simplify and speed up business workflows and big data jobs using Amazon S3 inventory, which provides a scheduled alternative to the Amazon S3 synchronous List API operation.</p>\n<p>Amazon S3 Inventory provides comma-separated values (CSV), Apache optimized row columnar (ORC) or Apache Parquet (Parquet) output files that list your objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or a shared prefix (that is, objects that have names that begin with a common string). If configured on a weekly basis, a report is generated every Sunday (UTC timezone) after the initial report.</p>\n<p>You can configure multiple inventory lists for a bucket. You can configure what object metadata to include in the inventory, whether to list all object versions or only current versions, where to store the inventory list file output, and whether to generate the inventory on a daily or weekly basis. You can also specify that the inventory list file be encrypted.</p>\n<p>The inventory list contains a list of the objects in an S3 bucket and the metadata for each listed object.</p>\n<p>Metadata list that S3 Inventory has for each listed object in S3 bucket:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q31-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Use Amazon S3 Storage Lens to report all the objects that failed replication process in the S3 buckets</strong> - Amazon S3 Storage Lens aggregates your usage and activity metrics and displays the information in an interactive dashboard on the Amazon S3 console or through a metrics data export that can be downloaded in CSV or Parquet format. This is the wrong choice because we are not looking for usage metrics.</p>\n<p><strong>Use Amazon S3 Select to list the objects that have failed replication in the S3 buckets</strong> - With S3 Select, you can use a simple SQL expression to return only the data from the store you’re interested in, instead of retrieving the entire object. You cannot use S3 Select to list the objects that have failed replication in the S3 buckets.</p>\n<p><strong>Configure Amazon Simple Queue Service (Amazon SQS) queue against the CloudWatch metrics for S3 replication. You can use custom code to aggregate these messages to get the final list of objects that failed replication</strong> - CloudWatch metrics for replication are only available if S3 Replication Time Control (S3 RTC) is enabled. And metrics are generated for action on each object in S3 bucket. Since we need an aggregated list, we choose Amazon S3 Inventory that is tailor-made for such requirements.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>A data analytics company uses AWS CloudFormation templates to provision their AWS infrastructure for Amazon EC2, Amazon VPC, and Amazon S3 resources. Using cross-stack referencing, a systems administrator creates a stack called <code>NetworkStack</code> which will export the <code>subnetId</code> that can be used when creating EC2 instances in another stack.</p>\n<p>To use the exported value in another stack, which of the following functions must be used?</p>\n", "answers": ["!Ref", "!ImportValue", "!GetAtt", "!Sub"], "correct_answer": "!ImportValue", "explanation": "<p>Correct option:</p>\n<p><strong><code>!ImportValue</code></strong></p>\n<p>The intrinsic function <code>Fn::ImportValue</code> returns the value of an output exported by another stack. You typically use this function to create cross-stack references.</p>\n<p>Incorrect options:</p>\n<p><strong><code>!Ref</code></strong> - Returns the value of the specified parameter or resource.</p>\n<p><strong><code>!GetAtt</code></strong> - Returns the value of an attribute from a resource in the template.</p>\n<p><strong><code>!Sub</code></strong> - Substitutes variables in an input string with values that you specify.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>A telecommunications company runs its business on AWS Cloud with Amazon EC2 instances and Amazon S3 buckets. Of late, users are complaining of intermittently receiving 500 Internal Error response when accessing S3 bucket. The team is looking at a way to track the frequency of the error and fix the issue.</p>\n<p>What will you suggest to monitor and fix the error?</p>\n", "answers": ["Check if the S3 bucket has all the objects users are trying to access", "The users accessing the bucket do not have proper permissions to access the objects present in S3. Check the application logic for the IAM Role being assigned when accessing the S3 bucket", "Enable Amazon CloudWatch metrics that include a metric for 5xx server errors. Retrying generally fixes this error", "Objects encrypted by AWS KMS are not accessible to users unless permission for KMS key access is provided. Include logic to provide access to KMS keys"], "correct_answer": "Enable Amazon CloudWatch metrics that include a metric for 5xx server errors. Retrying generally fixes this error", "explanation": "<p>Correct option:</p>\n<p><strong>Enable Amazon CloudWatch metrics that include a metric for 5xx server errors. Retrying generally fixes this error</strong></p>\n<p>If you get intermittent 500 Internal Error responses from Amazon S3, you can retry the requests. These errors are rare and can occur during normal use of the service. It's a best practice to implement retry logic for requests that receive server or throttling errors (5xx errors). For better flow control, use an exponential backoff algorithm. Each AWS SDK uses automatic retry logic and an exponential backoff algorithm.</p>\n<p>To monitor the number of 500 Internal Error responses that you're getting, you can enable Amazon CloudWatch metrics. Amazon S3 CloudWatch request metrics include a metric for 5xx server errors.</p>\n<p>Incorrect options:</p>\n<p><strong>Check if the S3 bucket has all the objects users are trying to access</strong></p>\n<p><strong>The users accessing the bucket do not have proper permissions to access the objects present in S3. Check the application logic for the IAM Role being assigned when accessing the S3 bucket</strong></p>\n<p><strong>Objects encrypted by AWS KMS are not accessible to users unless permission for KMS key access is provided. Include logic to provide access to KMS keys</strong></p>\n<p>All the above errors indicate access issues, specifically when access to a resource is denied. Access Denied error is 403 or of the 4XX series.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudwatch-monitoring.html#s3-request-cloudwatch-metrics\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudwatch-monitoring.html#s3-request-cloudwatch-metrics</a></p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-intermittent-500-internal-error/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-intermittent-500-internal-error/</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>A video streaming solutions provider is migrating to AWS Cloud infrastructure for delivering its content to users across the world. The company wants to make sure that the solution supports at least a million requests per second for its EC2 server farm.</p>\n<p>As a SysOps Administrator, which type of Elastic Load Balancer would you recommend as part of the solution stack?</p>\n", "answers": ["Application Load Balancer", "Network Load Balancer", "Classic Load Balancer", "Infrastructure Load Balancer"], "correct_answer": "Network Load Balancer", "explanation": "<p>Correct option:</p>\n<p><strong>Network Load Balancer</strong></p>\n<p>Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data.</p>\n<p>Incorrect options:</p>\n<p><strong>Application Load Balancer</strong> - Application Load Balancer operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.</p>\n<p>Application Load Balancer is not a good fit for the low latency and high throughput scenario mentioned in the given use-case.</p>\n<p><strong>Classic Load Balancer</strong> - Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network. Classic Load Balancer is not a good fit for the low latency and high throughput scenario mentioned in the given use-case.</p>\n<p><strong>Infrastructure Load Balancer</strong> - There is no such thing as Infrastructure Load Balancer and this option just acts as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>A new systems administrator has joined a large healthcare services company recently. As part of his onboarding, the IT department is conducting a review of the checklist for tasks related to AWS Identity and Access Management.</p>\n<p>Which best practices would you recommend? (Select two)?</p>\n", "answers": ["Create a minimum number of accounts and share these account credentials among employees", "Grant maximum privileges to avoid assigning privileges again", "Enable MFA for privileged users", "Use user credentials to provide access specific permissions for Amazon EC2 instances", "Configure AWS CloudTrail to log all IAM actions"], "correct_answer": ["Enable MFA for privileged users", "Configure AWS CloudTrail to log all IAM actions"], "explanation": "<p>Correct options:</p>\n<p><strong>Enable MFA for privileged users</strong> - As per the AWS best practices, it is better to enable Multi Factor Authentication (MFA) for privileged users via an MFA-enabled mobile device or hardware MFA token.</p>\n<p><strong>Configure AWS CloudTrail to record all account activity</strong> - AWS recommends to turn on CloudTrail to log all IAM actions for monitoring and audit purposes.</p>\n<p>Incorrect options:</p>\n<p><strong>Create a minimum number of accounts and share these account credentials among employees</strong> - AWS recommends that user account credentials should not be shared between users. So, this option is incorrect.</p>\n<p><strong>Grant maximum privileges to avoid assigning privileges again</strong> - AWS recommends granting the least privileges required to complete a certain job and avoid giving excessive privileges which can be misused. So, this option is incorrect.</p>\n<p><strong>Use user credentials to provide access specific permissions for Amazon EC2 instances</strong> - It is highly recommended to use roles to grant access permissions for EC2 instances working on different AWS services. So, this option is incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/iam/\">https://aws.amazon.com/iam/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html</a></p>\n<p><a href=\"https://aws.amazon.com/cloudtrail/faqs/\">https://aws.amazon.com/cloudtrail/faqs/</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "checkbox"}, {"question": "<p>A media company uses S3 to aggregate the raw video footage from its reporting teams across the US. The company has recently expanded into new geographies in Europe and Australia. The technical teams at the overseas branch offices have reported huge delays in uploading large video files to the destination S3 bucket.</p>\n<p>Which of the following are the MOST cost-effective options to improve the file upload speed into S3? (Select two)</p>\n", "answers": ["Create multiple site-to-site VPN connections between the AWS Cloud and branch offices in Europe and Australia. Use these VPN connections for faster file uploads into S3", "Create multiple AWS direct connect connections between the AWS Cloud and branch offices in Europe and Australia. Use the direct connect connections for faster file uploads into S3", "Use Amazon S3 Transfer Acceleration to enable faster file uploads into the destination S3 bucket", "Use multipart uploads for faster file uploads into the destination S3 bucket", "Use AWS Global Accelerator for faster file uploads into the destination S3 bucket"], "correct_answer": ["Use Amazon S3 Transfer Acceleration to enable faster file uploads into the destination S3 bucket", "Use multipart uploads for faster file uploads into the destination S3 bucket"], "explanation": "<p>Correct options:</p>\n<p><strong>Use Amazon S3 Transfer Acceleration to enable faster file uploads into the destination S3 bucket</strong> - Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</p>\n<p><strong>Use multipart uploads for faster file uploads into the destination S3 bucket</strong> - Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput, therefore it facilitates faster file uploads.</p>\n<p>Incorrect options:</p>\n<p><strong>Create multiple AWS direct connect connections between the AWS Cloud and branch offices in Europe and Australia. Use the direct connect connections for faster file uploads into S3</strong> - AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations.</p>\n<p>Direct connect takes significant time (several months) to be provisioned and is an overkill for the given use-case.</p>\n<p><strong>Create multiple site-to-site VPN connections between the AWS Cloud and branch offices in Europe and Australia. Use these VPN connections for faster file uploads into S3</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet.</p>\n<p>VPN Connections are a good solution if you have low to modest bandwidth requirements and can tolerate the inherent variability in Internet-based connectivity. Site-to-site VPN will not help in accelerating the file transfer speeds into S3 for the given use-case.</p>\n<p><strong>Use AWS Global Accelerator for faster file uploads into the destination S3 bucket</strong> - AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances. AWS Global Accelerator will not help in accelerating the file transfer speeds into S3 for the given use-case.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p>\n", "section": "Domain 6: Networking", "type": "checkbox"}, {"question": "<p>A financial services startup is building an interactive tool for personal finance needs. The users would be required to capture their financial data via this tool. As this is sensitive information, the backup of the user data must be kept encrypted in S3. The startup does not want to provide its own encryption keys but still wants to maintain an audit trail of when an encryption key was used and by whom.</p>\n<p>Which of the following is the BEST solution for this use-case?</p>\n", "answers": ["Use SSE-S3 to encrypt the user data on S3", "Use SSE-KMS to encrypt the user data on S3", "Use SSE-C to encrypt the user data on S3", "Use client-side encryption with client provided keys and then upload the encrypted user data to S3"], "correct_answer": "Use SSE-KMS to encrypt the user data on S3", "explanation": "<p>Correct option:</p>\n<p><strong>Use SSE-KMS to encrypt the user data on S3</strong></p>\n<p>AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created.</p>\n<p>SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Therefore SSE-KMS is the correct solution for this use-case.</p>\n<p>Server Side Encryption in S3:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q37-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Use SSE-S3 to encrypt the user data on S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. However this option does not provide the ability to audit trail the usage of the encryption keys.</p>\n<p><strong>Use SSE-C to encrypt the user data on S3</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects. However this option does not provide the ability to audit trail the usage of the encryption keys.</p>\n<p><strong>Use client-side encryption with client provided keys and then upload the encrypted user data to S3</strong> - Using client-side encryption is ruled out as the startup does not want to provide the encryption keys.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>A data analytics company wants to seamlessly integrate its on-premises data center with AWS cloud-based IT systems which would be critical to manage as well as scale-up the complex planning and execution of every stage of its analytics workflows. As part of a pilot program, the company wants to integrate data files from its on-premises servers into AWS via an NFS interface.</p>\n<p>Which of the following AWS service is the MOST efficient solution for the given use-case?</p>\n", "answers": ["AWS Storage Gateway - File Gateway", "AWS Storage Gateway - Volume Gateway", "AWS Site-to-Site VPN", "AWS Storage Gateway - Tape Gateway"], "correct_answer": "AWS Storage Gateway - File Gateway", "explanation": "<p>Correct option:</p>\n<p><strong>AWS Storage Gateway - File Gateway</strong></p>\n<p>AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways – Tape Gateway, File Gateway, and Volume Gateway – that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access.</p>\n<p>AWS Storage Gateway's file interface, or file gateway, offers you a seamless way to connect to the cloud in order to store application data files and backup images as durable objects on Amazon S3 cloud storage. File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. As the company wants to integrate data files from its analytical instruments into AWS via an NFS interface, therefore AWS Storage Gateway - File Gateway is the correct answer.</p>\n<p>File Gateway Overview:\n<img src=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/images/file-gateway-concepts-diagram.png\"/>\nvia - <a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>AWS Storage Gateway - Volume Gateway</strong> - You can configure the AWS Storage Gateway service as a Volume Gateway to present cloud-based iSCSI block storage volumes to your on-premises applications. Volume Gateway does not support NFS interface, so this option is not correct.</p>\n<p><strong>AWS Storage Gateway - Tape Gateway</strong> - AWS Storage Gateway - Tape Gateway allows moving tape backups to the cloud. Tape Gateway does not support NFS interface, so this option is not correct.</p>\n<p><strong>AWS Site-to-Site VPN</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN (Site-to-Site VPN) connection. It uses internet protocol security (IPSec) communications to create encrypted VPN tunnels between two locations. You cannot use AWS Site-to-Site VPN to integrate data files via the NFS interface, so this option is not correct.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/storagegateway/\">https://aws.amazon.com/storagegateway/</a></p>\n<p><a href=\"https://aws.amazon.com/storagegateway/volume/\">https://aws.amazon.com/storagegateway/volume/</a></p>\n<p><a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p>\n<p><a href=\"https://aws.amazon.com/storagegateway/vtl/\">https://aws.amazon.com/storagegateway/vtl/</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>A social media company uses Amazon S3 to store the images uploaded by the users. These images are kept encrypted in S3 by using AWS-KMS and the company manages its own Customer Master Key (CMK) for encryption. A systems administrator accidentally deleted the CMK a day ago, thereby rendering the user's photo data unrecoverable. You have been contacted by the company to consult them on possible solutions to this issue.</p>\n<p>As a SysOps Administrator, which of the following steps would you recommend to solve this issue?</p>\n", "answers": ["Contact AWS support to retrieve the CMK from their backup", "The company should issue a notification on its web application informing the users about the loss of their data", "As the CMK was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the CMK deletion and recover the key", "The CMK can be recovered by the AWS root account user"], "correct_answer": "As the CMK was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the CMK deletion and recover the key", "explanation": "<p>Correct option:</p>\n<p><strong>As the CMK was deleted a day ago, it must be in the 'pending deletion' status and hence you can just cancel the CMK deletion and recover the key</strong></p>\n<p>AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. AWS KMS is a secure and resilient service that uses hardware security modules that have been validated under FIPS 140-2.</p>\n<p>Deleting a customer master key (CMK) in AWS Key Management Service (AWS KMS) is destructive and potentially dangerous. Therefore, AWS KMS enforces a waiting period. To delete a CMK in AWS KMS you schedule key deletion. You can set the waiting period from a minimum of 7 days up to a maximum of 30 days. The default waiting period is 30 days. During the waiting period, the CMK status and key state is Pending deletion. To recover the CMK, you can cancel key deletion before the waiting period ends. After the waiting period ends you cannot cancel key deletion, and AWS KMS deletes the CMK.</p>\n<p>How Deleting Customer Master Keys Works:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q39-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Contact AWS support to retrieve the CMK from their backup</strong></p>\n<p><strong>The CMK can be recovered by the AWS root account user</strong></p>\n<p>The AWS root account user cannot recover CMK and the AWS support does not have access to CMK via any backups. Both these options just serve as distractors.</p>\n<p><strong>The company should issue a notification on its web application informing the users about the loss of their data</strong> - This option is not needed as the data can be recovered via the cancel key deletion feature.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html\">https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A multi-national retail company wants to explore a hybrid cloud environment with AWS so that it can start leveraging AWS services for some of its daily workflows. The development team at the company wants to establish a dedicated, encrypted, low latency, and high throughput connection between its data center and AWS Cloud. The team has set aside sufficient time to account for the operational overhead of establishing this connection.</p>\n<p>As a SysOps Administrator, which of the following solutions would you recommend to the company?</p>\n", "answers": ["Use AWS Direct Connect to establish a connection between the data center and AWS Cloud", "Use site-to-site VPN to establish a connection between the data center and AWS Cloud", "Use AWS Direct Connect plus VPN to establish a connection between the data center and AWS Cloud", "Use VPC transit gateway to establish a connection between the data center and AWS Cloud"], "correct_answer": "Use AWS Direct Connect plus VPN to establish a connection between the data center and AWS Cloud", "explanation": "<p>Correct option:</p>\n<p><strong>Use AWS Direct Connect plus VPN to establish a connection between the data center and AWS Cloud</strong></p>\n<p>AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations.</p>\n<p>With AWS Direct Connect plus VPN, you can combine one or more AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based VPN connections.</p>\n<p>This solution combines the AWS managed benefits of the VPN solution with low latency, increased bandwidth, more consistent benefits of the AWS Direct Connect solution, and an end-to-end, secure IPsec connection. Therefore, AWS Direct Connect plus VPN is the correct solution for this use-case.</p>\n<p>AWS Direct Connect Plus VPN:\n<img src=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/images/image10.png\"/>\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Use site-to-site VPN to establish a connection between the data center and AWS Cloud</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity.</p>\n<p>However, Site-to-site VPN cannot provide low latency and high throughput connection, therefore this option is ruled out.</p>\n<p><strong>Use VPC transit gateway to establish a connection between the data center and AWS Cloud</strong> - A transit gateway is a network transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. A transit gateway by itself cannot establish a low latency and high throughput connection between a data center and AWS Cloud. Hence this option is incorrect.</p>\n<p><strong>Use AWS Direct Connect to establish a connection between the data center and AWS Cloud</strong> - AWS Direct Connect by itself cannot provide an encrypted connection between a data center and AWS Cloud, so this option is ruled out.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/directconnect/\">https://aws.amazon.com/directconnect/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>A junior administrator at a retail company is documenting the process flow to provision EC2 instances via the Amazon EC2 API. These instances are to be used for an internal application that processes HR payroll data. He wants to highlight those volume types that cannot be used as a boot volume.</p>\n<p>Can you help the intern by identifying those storage volume types that CANNOT be used as boot volumes while creating the instances? (Select two)</p>\n", "answers": ["General Purpose SSD (gp2)", "Throughput Optimized HDD (st1)", "Provisioned IOPS SSD (io1)", "Instance Store", "Cold HDD (sc1)"], "correct_answer": ["Throughput Optimized HDD (st1)", "Cold HDD (sc1)"], "explanation": "<p>Correct options:</p>\n<p><strong>Throughput Optimized HDD (st1)</strong></p>\n<p><strong>Cold HDD (sc1)</strong></p>\n<p>The EBS volume types fall into two categories:</p>\n<p>SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS.</p>\n<p>HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS.</p>\n<p>Throughput Optimized HDD (st1) and Cold HDD (sc1) volume types CANNOT be used as a boot volume, so these two options are correct.</p>\n<p>Please see this detailed overview of the volume types for EBS volumes.\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q41-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>General Purpose SSD (gp2)</strong></p>\n<p><strong>Provisioned IOPS SSD (io1)</strong></p>\n<p><strong>Instance Store</strong></p>\n<p>General Purpose SSD (gp2), Provisioned IOPS SSD (io1), and Instance Store can be used as a boot volume.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "checkbox"}, {"question": "<p>The development team at an e-commerce company uses Amazon MySQL RDS because it simplifies much of the time-consuming administrative tasks typically associated with databases. A new systems administrator has joined the team and wants to understand the replication capabilities for Multi-AZ as well as Read-replicas.</p>\n<p>Which of the following correctly summarizes these capabilities for the given database?</p>\n", "answers": ["Multi-AZ follows asynchronous replication and spans one Availability Zone within a single region. Read replicas follow synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region", "Multi-AZ follows synchronous replication and spans at least two Availability Zones within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region", "Multi-AZ follows asynchronous replication and spans at least two Availability Zones within a single region. Read replicas follow synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region", "Multi-AZ follows asynchronous replication and spans at least two Availability Zones within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region"], "correct_answer": "Multi-AZ follows synchronous replication and spans at least two Availability Zones within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region", "explanation": "<p>Correct option:</p>\n<p><strong>Multi-AZ follows synchronous replication and spans at least two Availability Zones within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</strong></p>\n<p>Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones within a single region.</p>\n<p>Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance.</p>\n<p>Amazon RDS replicates all databases in the source DB instance. Read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region.</p>\n<p>Exam Alert:</p>\n<p>Please review this comparison vis-a-vis Multi-AZ vs Read Replica for RDS:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q42-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n<p>Incorrect Options:</p>\n<p><strong>Multi-AZ follows asynchronous replication and spans one Availability Zone within a single region. Read replicas follow synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</strong></p>\n<p><strong>Multi-AZ follows asynchronous replication and spans at least two Availability Zones within a single region. Read replicas follow synchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</strong></p>\n<p><strong>Multi-AZ follows asynchronous replication and spans at least two Availability Zones within a single region. Read replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region</strong></p>\n<p>These three options contradict the explanation above, so these options are incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>A video streaming solutions company wants to use AWS Cloudfront to distribute its content only to its service subscribers.</p>\n<p>As a SysOps Administrator, which of the following solutions would you suggest in order to deliver restricted content to the subscribers? (Select two)</p>\n", "answers": ["Use CloudFront signed URLs", "Require HTTPS for communication between CloudFront and your custom origin", "Require HTTPS for communication between CloudFront and your S3 origin", "Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers", "Use CloudFront signed cookies"], "correct_answer": ["Use CloudFront signed URLs", "Use CloudFront signed cookies"], "explanation": "<p>Correct options:</p>\n<p><strong>Use CloudFront signed URLs</strong></p>\n<p>Many companies that distribute content over the internet want to restrict access to documents, business data, media streams, or content that is intended for selected users, for example, users who have paid a fee.</p>\n<p>To securely serve this private content by using CloudFront, you can do the following:</p>\n<p>Require that your users access your private content by using special CloudFront signed URLs or signed cookies.</p>\n<p>A signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content. So this is a correct option.</p>\n<p><strong>Use CloudFront signed cookies</strong></p>\n<p>CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website. So this is also a correct option.</p>\n<p>Incorrect options:</p>\n<p><strong>Require HTTPS for communication between CloudFront and your custom origin</strong></p>\n<p><strong>Require HTTPS for communication between CloudFront and your S3 origin</strong></p>\n<p>Requiring HTTPS for communication between CloudFront and your custom origin (or S3 origin) only enables secure access to the underlying content. You cannot use HTTPS to restrict access to your private content. So both these options are incorrect.</p>\n<p><strong>Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers</strong> - This option is just added as a distractor. You cannot use HTTPS to restrict access to your private content.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "checkbox"}, {"question": "<p>The technology team at a retail company uses CloudFormation to manage its AWS infrastructure. The team has created a network stack containing a VPC with subnets and a web application stack with EC2 instances and an RDS instance. The team wants to reference the VPC created in the network stack into its web application stack.</p>\n<p>As a SysOps Administrator, which of the following solutions would you recommend for the given use-case?</p>\n", "answers": ["Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack", "Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack", "Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack", "Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack"], "correct_answer": "Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack", "explanation": "<p>Correct option:</p>\n<p><strong>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</strong></p>\n<p>AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.</p>\n<p>How CloudFormation Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png\"/>\nvia - <a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n<p>You can create a cross-stack reference to export resources from one AWS CloudFormation stack to another. For example, you might have a network stack with a VPC and subnets and a separate public web application stack. To use the security group and subnet from the network stack, you can create a cross-stack reference that allows the web application stack to reference resource outputs from the network stack. With a cross-stack reference, owners of the web application stacks don't need to create or maintain networking rules or assets.</p>\n<p>To create a cross-stack reference, use the Export output field to flag the value of a resource output for export. Then, use the Fn::ImportValue intrinsic function to import the value.</p>\n<p>You cannot use the Ref intrinsic function to import the value.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q44-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</strong></p>\n<p><strong>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</strong></p>\n<p><strong>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</strong></p>\n<p>These three options contradict the explanation above, so these options are not correct.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>A healthcare company has developed its flagship application on AWS Cloud with data security requirements such that the encryption key must be stored in a custom application running on-premises. The company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption keys.</p>\n<p>Which of the following S3 encryption options allows the company to leverage Amazon S3 for storing data with given constraints?</p>\n", "answers": ["Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)", "Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)", "Server-Side Encryption with Customer-Provided Keys (SSE-C)", "Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3"], "correct_answer": "Server-Side Encryption with Customer-Provided Keys (SSE-C)", "explanation": "<p>Correct option:</p>\n<p><strong>Server-Side Encryption with Customer-Provided Keys (SSE-C)</strong></p>\n<p>You have the following options for protecting data at rest in Amazon S3:</p>\n<p>Server-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.</p>\n<p>Client-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n<p>For the given use-case, the company wants to manage the encryption keys via its custom application and let S3 manage the encryption, therefore you must use Server-Side Encryption with Customer-Provided Keys (SSE-C).</p>\n<p>Please review these three options for Server Side Encryption on S3:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q45-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect.</p>\n<p><strong>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)</strong> - Server-Side Encryption with Customer Master Keys (CMKs) stored in AWS Key Management Service (SSE-KMS) is similar to SSE-S3. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Additionally, you can create and manage customer-managed CMKs or use AWS managed CMKs that are unique to you, your service, and your Region.</p>\n<p><strong>Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3</strong> - You can encrypt the data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>An e-commerce company manages its IT infrastructure on AWS Cloud via Elastic Beanstalk. The development team at the company is planning to deploy the next version with MINIMUM application downtime and the ability to rollback quickly in case the deployment goes wrong.</p>\n<p>As a SysOps Administrator, which of the following options would you recommend to address the given use-case?</p>\n", "answers": ["Deploy the new application version using 'All at once' deployment policy", "Deploy the new application version using 'Rolling' deployment policy", "Deploy the new version to a separate environment via Blue/Green Deployment, and then swap Route 53 records of the two environments to redirect traffic to the new version", "Deploy the new application version using 'Rolling with additional batch' deployment policy"], "correct_answer": "Deploy the new version to a separate environment via Blue/Green Deployment, and then swap Route 53 records of the two environments to redirect traffic to the new version", "explanation": "<p>Correct option:</p>\n<p><strong>Deploy the new version to a separate environment via Blue/Green Deployment, and then swap Route 53 records of the two environments to redirect traffic to the new version</strong></p>\n<p>With deployment policies such as 'All at once', AWS Elastic Beanstalk performs an in-place update when you update your application versions and your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs (via Route 53) of the two environments to redirect traffic to the new version instantly. In case of any deployment issues, the rollback process is very quick via swapping the URLs for the two environments.</p>\n<p>Overview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q46-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Deploy the new application version using 'All at once' deployment policy</strong> - Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time. So this option is not correct.</p>\n<p><strong>Deploy the new application version using 'Rolling' deployment policy</strong> - This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. However rollback process is via manual redeploy, so it's not as quick as the Blue/Green deployment.</p>\n<p><strong>Deploy the new application version using 'Rolling with additional batch' deployment policy</strong> - This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. However rollback process is via manual redeploy, so it's not as quick as the Blue/Green deployment.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>A systems administrator at a company is working on a CloudFormation template to set up resources. Resources will be defined using code and provisioned based on certain conditions.</p>\n<p>Which section of a CloudFormation template does not allow for conditions?</p>\n", "answers": ["Outputs", "Resources", "Conditions", "Parameters"], "correct_answer": "Parameters", "explanation": "<p>Correct option:</p>\n<p><strong>Parameters</strong></p>\n<p>Parameters enable you to input custom values to your CloudFormation template each time you create or update a stack. Please see this note to understand how to define a parameter in a template:</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q47-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html</a></p>\n<p>The optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true.</p>\n<p>You might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment. In your template, you can add an EnvironmentType input parameter, which accepts either prod or test as inputs. For the production environment, you might include Amazon EC2 instances with certain capabilities; however, for the test environment, you want to use reduced capabilities to save money.</p>\n<p>Conditions cannot be used within the Parameters section. After you define all your conditions, you can associate them with resources and resource properties only in the Resources and Outputs sections of a template.</p>\n<p>Please review this note for more details:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q47-i2.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Resources</strong> - Resources section describes the resources that you want to provision in your AWS CloudFormation stacks. You can associate conditions with the resources that you want to conditionally create.</p>\n<p><strong>Conditions</strong> - You actually define conditions in this section of the CloudFormation template</p>\n<p><strong>Outputs</strong> - The optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find. You can associate conditions with the outputs that you want to conditionally create.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>A social media company is using AWS CloudFormation to manage its technology infrastructure. It has created a template to provision a stack with a VPC and a subnet. The output value of this subnet has to be used in another stack.</p>\n<p>As a SysOps Administrator, which of the following options would you suggest to provide this information to the other stack?</p>\n", "answers": ["Use 'Expose' field in the Output section of the stack's template", "Use 'Export' field in the Output section of the stack's template", "Use Fn::ImportValue", "Use Fn::Transform"], "correct_answer": "Use 'Export' field in the Output section of the stack's template", "explanation": "<p>Correct option:</p>\n<p><strong>Use 'Export' field in the Output section of the stack's template</strong></p>\n<p>To share information between stacks, export a stack's output values. Other stacks that are in the same AWS account and region can import the exported values.</p>\n<p>To export a stack's output value, use the Export field in the Output section of the stack's template. To import those values, use the Fn::ImportValue function in the template for the other stacks.</p>\n<p>Incorrect options:</p>\n<p><strong>Use 'Expose' field in the Output section of the stack's template</strong> - 'Expose' is a made-up option, and only given as a distractor.</p>\n<p><strong>Use Fn::ImportValue</strong> - To import the values exported by another stack, we use the Fn::ImportValue function in the template for the other stacks. This function is not useful for the current scenario.</p>\n<p><strong>Use Fn::Transform</strong> - The intrinsic function Fn::Transform specifies a macro to perform custom processing on part of a stack template. Macros enable you to perform custom processing on templates, from simple actions like find-and-replace operations to extensive transformations of entire templates. This function is not useful for the current scenario.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>A Silicon Valley based startup uses Elastic Beanstalk to manage its IT infrastructure on AWS Cloud and it would like to deploy the new application version to the EC2 instances. When the deployment is executed, some instances should serve requests with the old application version, while other instances should serve requests using the new application version until the deployment is completed.</p>\n<p>Which deployment meets this requirement without incurring additional costs?</p>\n", "answers": ["Immutable", "Rolling", "All at once", "Rolling with additional batches"], "correct_answer": "Rolling", "explanation": "<p>Correct option:</p>\n<p><strong>Rolling</strong></p>\n<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p>\n<p>The rolling deployment policy deploys the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch. The cost remains the same as the number of EC2 instances does not increase. This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time.</p>\n<p>Overview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q48-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Immutable</strong> - The 'Immutable' deployment policy ensures that your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails.</p>\n<p><strong>All at once</strong> - This policy deploys the new version to all instances simultaneously. Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time.</p>\n<p><strong>Rolling with additional batches</strong> - This policy deploys the new version in batches, but first launches a new batch of instances to ensure full capacity during the deployment process. This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. These increase the costs as you're adding extra instances during the deployment.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>The development team at your company wants to upload files to S3 buckets using the SSE-KMS encryption mechanism. However, the team is receiving permission errors while trying to push the objects over HTTP.</p>\n<p>Which of the following headers should the team include in the request?</p>\n", "answers": ["'x-amz-server-side-encryption': 'SSE-KMS'", "'x-amz-server-side-encryption': 'SSE-S3'", "'x-amz-server-side-encryption': 'aws:kms'", "'x-amz-server-side-encryption': 'AES256'"], "correct_answer": "'x-amz-server-side-encryption': 'aws:kms'", "explanation": "<p>Correct option:</p>\n<p><strong>'x-amz-server-side-encryption': 'aws:kms'</strong></p>\n<p>Server-side encryption is the encryption of data at its destination by the application or service that receives it. AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. Amazon S3 uses AWS KMS customer master keys (CMKs) to encrypt your Amazon S3 objects. AWS KMS encrypts only the object data. Any object metadata is not encrypted.</p>\n<p>If the request does not include the x-amz-server-side-encryption header, then the request is denied.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q50-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>'x-amz-server-side-encryption': 'SSE-S3'</strong> - This is an invalid header value. The correct value is 'x-amz-server-side-encryption': 'AES256'. This refers to Server-Side Encryption with Amazon S3-Managed Encryption Keys (SSE-S3).</p>\n<p><strong>'x-amz-server-side-encryption': 'SSE-KMS'</strong> - Invalid header value. SSE-KMS is an encryption option.</p>\n<p><strong>'x-amz-server-side-encryption': 'AES256'</strong> - This is the correct header value if you are using SSE-S3 server-side encryption.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>You are working as an AWS Certified SysOps Administrator at an e-commerce company and you want to build a fleet of EBS-optimized EC2 instances to handle the load of your new application. To meet the compliance guidelines, your organization wants any secret strings used in the application to be encrypted to prevent exposing values as clear text.</p>\n<p>The solution requires that decryption events be audited and API calls to be simple. How can this be achieved? (Select two)</p>\n", "answers": ["Encrypt first with KMS then store in SSM Parameter store", "Store the secret as SecureString in SSM Parameter Store", "Store the secret as PlainText in SSM Parameter Store", "Audit using CloudTrail", "Audit using SSM Audit Trail"], "correct_answer": ["Store the secret as SecureString in SSM Parameter Store", "Audit using CloudTrail"], "explanation": "<p>Correct options:</p>\n<p><strong>Store the secret as SecureString in SSM Parameter Store</strong></p>\n<p>With AWS Systems Manager Parameter Store, you can create SecureString parameters, which are parameters that have a plaintext parameter name and an encrypted parameter value. Parameter Store uses AWS KMS to encrypt and decrypt the parameter values of Secure String parameters. Also, if you are using customer-managed CMKs, you can use IAM policies and key policies to manage to encrypt and decrypt permissions. To retrieve the decrypted value you only need to do one API call.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q51-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html</a></p>\n<p><strong>Audit using CloudTrail</strong></p>\n<p>AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides an event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services.</p>\n<p>CloudTrail will allow you to see all API calls made to SSM and KMS.</p>\n<p><img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\"/></p>\n<p>Incorrect options:</p>\n<p><strong>Encrypt first with KMS then store in SSM Parameter store</strong> - This could work but will require two API calls to get the decrypted value instead of one. So this is not the right option.</p>\n<p><strong>Store the secret as PlainText in SSM Parameter Store</strong> - Plaintext parameters are not secure and shouldn't be used to store secrets.</p>\n<p><strong>Audit using SSM Audit Trail</strong> - This is a made-up option and has been added as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "checkbox"}, {"question": "<p>A healthcare company stores confidential data on an Amazon Simple Storage Service (S3) bucket. New security compliance guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage S3 encryption keys.</p>\n<p>Which of the following options should you use?</p>\n", "answers": ["SSE-S3", "SSE-C", "Client Side Encryption", "SSE-KMS"], "correct_answer": "SSE-S3", "explanation": "<p>Correct option:</p>\n<p><strong>SSE-S3</strong></p>\n<p>Using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.</p>\n<p>Incorrect options:</p>\n<p><strong>SSE-C</strong> - You manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects.</p>\n<p><strong>Client-Side Encryption</strong> - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n<p><strong>SSE-KMS</strong> - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage encryption keys yourself.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A social media company manages over 100 c4.large instances in the us-west-1 region. The EC2 instances run complex algorithms. The systems administrator would like to track CPU utilization of the EC2 instances as frequently as every 10 seconds.</p>\n<p>Which of the following represents the BEST solution for the given use-case?</p>\n", "answers": ["Enable EC2 detailed monitoring", "Create a high-resolution custom metric and push the data using a script triggered every 10 seconds", "Simply get it from the CloudWatch Metrics", "Open a support ticket with AWS"], "correct_answer": "Create a high-resolution custom metric and push the data using a script triggered every 10 seconds", "explanation": "<p>Correct option:</p>\n<p><strong>Create a high-resolution custom metric and push the data using a script triggered every 10 seconds</strong></p>\n<p>Using high-resolution custom metric, your applications can publish metrics to CloudWatch with 1-second resolution. You can watch the metrics scroll across your screen seconds after they are published and you can set up high-resolution CloudWatch Alarms that evaluate as frequently as every 10 seconds. You can alert with High-Resolution Alarms, as frequently as 10-second periods. High-Resolution Alarms allow you to react and take actions faster and support the same actions available today with standard 1-minute alarms.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q53-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/\">https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/</a></p>\n<p>Incorrect options:</p>\n<p><strong>Enable EC2 detailed monitoring</strong> - As part of basic monitoring, Amazon EC2 sends metric data to CloudWatch in 5-minute periods. To send metric data for your instance to CloudWatch in 1-minute periods, you can enable detailed monitoring on the instance, however, this comes at an additional cost.</p>\n<p><strong>Simply get it from the CloudWatch Metrics</strong> - You can get data from metrics. The basic monitoring data is available automatically in a 5-minute interval and detailed monitoring data is available in a 1-minute interval.</p>\n<p><strong>Open a support ticket with AWS</strong> - This option has been added as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/\">https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>A multi-national retail company uses AWS Organizations to manage its users across different divisions. Even though CloudTrail is enabled on the member AWS accounts, managers have noticed that access issues to CloudTrail logs across different divisions and AWS Regions is becoming a bottleneck in troubleshooting issues. They have decided to use the organization trail to keep things simple.</p>\n<p>What are the important points to remember when configuring an organization trail? (Select two)</p>\n", "answers": ["There is nothing called Organization Trail. The master account can, however, enable CloudTrail logging, to keep track of all activities across AWS accounts", "By default, CloudTrail tracks only bucket-level actions. To track object-level actions, you need to enable Amazon S3 data events", "Member accounts do not have access to organization trail, neither do they have access to the Amazon S3 bucket that logs the files", "Member accounts will be able to see the Organization trail, but cannot modify or delete it", "By default, CloudTrail event log files are not encrypted"], "correct_answer": ["By default, CloudTrail tracks only bucket-level actions. To track object-level actions, you need to enable Amazon S3 data events", "Member accounts will be able to see the Organization trail, but cannot modify or delete it"], "explanation": "<p>Correct option:</p>\n<p>If you have created an organization in AWS Organizations, you can also create a trail that will log all events for all AWS accounts in that organization. This is referred to as an organization trail.</p>\n<p><strong>By default, CloudTrail tracks only bucket-level actions. To track object-level actions, you need to enable Amazon S3 data events</strong> - This is a correct statement. AWS CloudTrail supports Amazon S3 Data Events, apart from bucket Events. You can record all API actions on S3 Objects and receive detailed information such as the AWS account of the caller, IAM user role of the caller, time of the API call, IP address of the API, and other details. All events are delivered to an S3 bucket and CloudWatch Events, allowing you to take programmatic actions on the events.</p>\n<p><strong>Member accounts will be able to see the organization trail, but cannot modify or delete it</strong> - Organization trails must be created in the master account, and when specified as applying to an organization, are automatically applied to all member accounts in the organization. Member accounts will be able to see the organization trail, but cannot modify or delete it. By default, member accounts will not have access to the log files for the organization trail in the Amazon S3 bucket.</p>\n<p>Organization trail:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q54-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>There is nothing called Organization Trail. The master account can, however, enable CloudTrail logging, to keep track of all activities across AWS accounts</strong> - This statement is incorrect. AWS offers Organization Trail for easy management and monitoring.</p>\n<p><strong>Member accounts do not have access to the organization trail, neither do they have access to the Amazon S3 bucket that logs the files</strong> - This statement is only partially correct. Member accounts will be able to see the organization trail, but cannot modify or delete it. By default, member accounts will not have access to the log files for the organization trail in the Amazon S3 bucket.</p>\n<p><strong>By default, CloudTrail event log files are not encrypted</strong> - This is an incorrect statement. By default, CloudTrail event log files are encrypted using Amazon S3 server-side encryption (SSE).</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html</a></p>\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2016/11/aws-cloudtrail-supports-s3-data-events/\">https://aws.amazon.com/about-aws/whats-new/2016/11/aws-cloudtrail-supports-s3-data-events/</a></p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/secure-s3-resources/\">https://aws.amazon.com/premiumsupport/knowledge-center/secure-s3-resources/</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "checkbox"}, {"question": "<p>A systems administrator has attached two policies to an IAM user. The first policy states that the user has explicitly been denied all access to EC2 instances. The second policy states that the user has been allowed permission for EC2:Describe action.</p>\n<p>When the user tries to use 'Describe' action on an EC2 instance using the CLI, what will be the output?</p>\n", "answers": ["The IAM user stands in an invalid state, because of conflicting policies", "The user will get access because it has an explicit allow", "The user will be denied access because one of the policies has an explicit deny on it", "The order of the policy matters. If policy 1 is before 2, then the user is denied access. If policy 2 is before 1, then the user is allowed access"], "correct_answer": "The user will be denied access because one of the policies has an explicit deny on it", "explanation": "<p>Correct option:</p>\n<p><strong>The user will be denied access because the policy has an explicit deny on it</strong> - User will be denied access because any explicit deny overrides the allow.</p>\n<p>Policy Evaluation explained:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q55-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>The IAM user stands in an invalid state, because of conflicting policies</strong> - This is an incorrect statement. Access policies can have allow and deny permissions on them and based on policy rules they are evaluated. A user account does not get invalid because of policies.</p>\n<p><strong>The user will get access because it has an explicit allow</strong> - As discussed above, explicit deny overrides all other permissions and hence the user will be denied access.</p>\n<p><strong>The order of the policy matters. If policy 1 is before 2, then the user is denied access. If policy 2 is before 1, then the user is allowed access</strong> - If policies that apply to a request include an Allow statement and a Deny statement, the Deny statement trumps the Allow statement. The request is explicitly denied.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A systems administration intern is trying to configure what an Amazon EC2 should do when it interrupts a Spot Instance.</p>\n<p>Which of the following CANNOT be configured as an interruption behavior?</p>\n", "answers": ["Stop the Spot Instance", "Hibernate the Spot Instance", "Terminate the Spot Instance", "Reboot the Spot Instance"], "correct_answer": "Reboot the Spot Instance", "explanation": "<p>Correct option:</p>\n<p>A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price.  Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated.</p>\n<p>You can specify that Amazon EC2 should do one of the following when it interrupts a Spot Instance:</p>\n<p>Stop the Spot Instance</p>\n<p>Hibernate the Spot Instance</p>\n<p>Terminate the Spot Instance</p>\n<p>The default is to terminate Spot Instances when they are interrupted.</p>\n<p><strong>Reboot the Spot Instance</strong> - This is an invalid option.</p>\n<p>Incorrect options:</p>\n<p>It is always possible that Spot Instances might be interrupted. Therefore, you must ensure that your application is prepared for a Spot Instance interruption.</p>\n<p><strong>Stop the Spot Instance</strong> - This is a valid option. Amazon EC2 can be configured to stop the instance when an interruption occurs on Spot instances.</p>\n<p><strong>Hibernate the Spot Instance</strong> - This is a valid option. Amazon EC2 can be configured to hibernate the instance when an interruption occurs on Spot instances.</p>\n<p><strong>Terminate the Spot Instance</strong> - This is a valid option. Amazon EC2 can be configured to hibernate the instance when an interruption occurs on Spot instances. The default behavior is to terminate Spot Instances when they are interrupted.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>An e-commerce company runs their database workloads on Provisioned IOPS SSD (io1) volumes.</p>\n<p>As a SysOps Administrator, which of the following options would you identify as an INCORRECT configuration for io1 EBS volume types?</p>\n", "answers": ["100 GiB size volume with 1000 IOPS", "100 GiB size volume with 5000 IOPS", "100 GiB size volume with 7500 IOPS", "100 GiB size volume with 3000 IOPS"], "correct_answer": "100 GiB size volume with 7500 IOPS", "explanation": "<p>Correct option:</p>\n<p><strong>100 GiB size volume with 7500 IOPS</strong> - This is an incorrect configuration. The maximum ratio of provisioned IOPS to requested volume size (in GiB) is 50:1. So, for a 100 GiB volume size, max IOPS possible is 100*50 = 5000 IOPS.</p>\n<p>Overview of Provisioned IOPS SSD (io1) volumes:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q57-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n<p>Incorrect options:</p>\n<p>Provisioned IOPS SSD (io1) volumes allow you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time. An io1 volume can range in size from 4 GiB to 16 TiB. The maximum ratio of provisioned IOPS to the requested volume size (in GiB) is 50:1. For example, a 100 GiB volume can be provisioned with up to 5,000 IOPS.</p>\n<p><strong>100 GiB size volume with 1000 IOPS</strong> - As explained above, up to 5000 IOPS is a valid configuration for the given use-case.</p>\n<p><strong>100 GiB size volume with 5000 IOPS</strong> - As explained above, up to 5000 IOPS is a valid configuration for the given use-case.</p>\n<p><strong>100 GiB size volume with 3000 IOPS</strong> - As explained above, up to 5000 IOPS is a valid configuration for the given use-case.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>As part of the systems administration work, an AWS Certified SysOps Administrator is creating policies and attaching them to IAM identities. After creating necessary Identity-based policies, he is now creating Resource-based policies.</p>\n<p>Which is the only resource-based policy that the IAM service supports?</p>\n", "answers": ["AWS Organizations Service Control Policies (SCP)", "Trust policy", "Access control list (ACL)", "Permissions boundary"], "correct_answer": "Trust policy", "explanation": "<p>Correct option:</p>\n<p>You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions.\nResource-based policies are JSON policy documents that you attach to a resource such as an Amazon S3 bucket. These policies grant the specified principal permission to perform specific actions on that resource and define under what conditions this applies.</p>\n<p><strong>Trust policy</strong> - Trust policies define which principal entities (accounts, users, roles, and federated users) can assume the role. An IAM role is both an identity and a resource that supports resource-based policies. For this reason, you must attach both a trust policy and an identity-based policy to an IAM role. The IAM service supports only one type of resource-based policy called a role trust policy, which is attached to an IAM role.</p>\n<p>Incorrect options:</p>\n<p><strong>AWS Organizations Service Control Policies (SCP)</strong> - If you enable all features of AWS organization, then you can apply service control policies (SCPs) to any or all of your accounts. SCPs are JSON policies that specify the maximum permissions for an organization or organizational unit (OU). The SCP limits permissions for entities in member accounts, including each AWS account root user. An explicit deny in any of these policies overrides the allow.</p>\n<p><strong>Access control list (ACL)</strong> - Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource. ACLs cannot be used to control access for a principal within the same account. Amazon S3, AWS WAF, and Amazon VPC are examples of services that support ACLs.</p>\n<p><strong>Permissions boundary</strong> - AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_resource-based\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_resource-based</a></p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>Which of the following security credentials can only be generated by the AWS Account root user? </p>\n", "answers": ["CloudFront Key Pairs", "EC2 Instance Key Pairs", "IAM User Access Keys", "IAM User passwords"], "correct_answer": "CloudFront Key Pairs", "explanation": "<p>Correct option:</p>\n<p>For Amazon CloudFront, you use key pairs to create signed URLs for private content, such as when you want to distribute restricted content that someone paid for.</p>\n<p><strong>CloudFront Key Pairs</strong> - IAM users can't create CloudFront key pairs. You must log in using root credentials to create key pairs.</p>\n<p>Incorrect options:</p>\n<p><strong>EC2 Instance Key Pairs</strong> - You use key pairs to access Amazon EC2 instances, such as when you use SSH to log in to a Linux instance. These key pairs can be created from the IAM user login and do not need root user access.</p>\n<p><strong>IAM User Access Keys</strong> - Access keys consist of two parts: an access key ID and a secret access key. You use access keys to sign programmatic requests that you make to AWS if you use AWS CLI commands (using the SDKs) or using AWS API operations. IAM users can create their own Access Keys, does not need root access.</p>\n<p><strong>IAM User passwords</strong> - Every IAM user has access to his own credentials and can reset the password whenever they need to.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A streaming services company has created an audio streaming application and it would like their Australian users to be served by the company's Australian servers. Other users around the globe should not be able to access the servers through DNS queries.</p>\n<p>Which Route 53 routing policy meets this requirement?</p>\n", "answers": ["Latency", "Geolocation", "Failover", "Weighted"], "correct_answer": "Geolocation", "explanation": "<p>Correct option:</p>\n<p><strong>Geolocation</strong></p>\n<p>Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. You can also use geolocation routing to restrict distribution of content to only the locations in which you have distribution rights</p>\n<p>You can create a default record that handles both queries from IP addresses that aren't mapped to any location and queries that come from locations that you haven't created geolocation records for. If you don't create a default record, Route 53 returns a \"no answer\" response for queries from those locations.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q60-i1.jpg\"/></p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q60-i2.jpg\"/></p>\n<p>via - <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Failover</strong> - Failover routing lets you route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy.</p>\n<p><strong>Latency</strong> - If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency.</p>\n<p><strong>Weighted</strong> - Use this policy to route traffic to multiple resources in proportions that you specify.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p>\n", "section": "Domain 2: High Availability", "type": "radio"}, {"question": "<p>Your application is hosted by a provider on yourapp.freehosting.com. You would like to have your users access your application using www.yourdomain.com, which you own and manage under Route 53.</p>\n<p>What Route 53 record should you create?</p>\n", "answers": ["Create an A record", "Create a CNAME record", "Create a PTR record", "Create an Alias Record"], "correct_answer": "Create a CNAME record", "explanation": "<p>Correct option:</p>\n<p><strong>Create a CNAME record</strong></p>\n<p>A CNAME record maps DNS queries for the name of the current record, such as acme.example.com, to another domain (example.com or example.net) or subdomain (acme.example.com or zenith.example.org).</p>\n<p>CNAME records can be used to map one domain name to another. Although you should keep in mind that the DNS protocol does not allow you to create a CNAME record for the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You cannot create a CNAME record for example.com, but you can create CNAME records for www.example.com, newproduct.example.com, and so on.</p>\n<p>Please review the major differences between CNAME and Alias Records:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q61-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Create an A record</strong> - Used to point a domain or subdomain to an IP address. 'A record' cannot be used to map one domain name to another.</p>\n<p><strong>Create a PTR record</strong> - A Pointer (PTR) record resolves an IP address to a fully-qualified domain name (FQDN) as an opposite to what A record does. PTR records are also called Reverse DNS records. 'PTR record' cannot be used to map one domain name to another.</p>\n<p><strong>Create an Alias Record</strong> - Alias records let you route traffic to selected AWS resources, such as CloudFront distributions and Amazon S3 buckets. They also let you route traffic from one record in a hosted zone to another record. 3rd party websites do not qualify for these as we have no control over those. 'Alias record' cannot be used to map one domain name to another.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>A retail company has branch offices in multiple locations and the development team has configured an Application Load Balancer across targets in multiple Availability Zones. The team wants to analyze the incoming requests for latencies and the client's IP address patterns.</p>\n<p>Which feature of the Load Balancer can be used to collect the required information?</p>\n", "answers": ["CloudTrail logs", "CloudWatch metrics", "ALB request tracing", "ALB access logs"], "correct_answer": "ALB access logs", "explanation": "<p>Correct option:</p>\n<p><strong>ALB access logs</strong></p>\n<p>Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues. Access logging is an optional feature of Elastic Load Balancing that is disabled by default.</p>\n<p>Access logs for your Application Load Balancer:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q62-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>CloudTrail logs</strong> - Elastic Load Balancing is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Elastic Load Balancing. CloudTrail captures all API calls for Elastic Load Balancing as events. You can use AWS CloudTrail to capture detailed information about the calls made to the Elastic Load Balancing API and store them as log files in Amazon S3. You can use these CloudTrail logs to determine which API calls were made, the source IP address where the API call came from, who made the call, when the call was made, and so on.</p>\n<p><strong>CloudWatch metrics</strong> - Elastic Load Balancing publishes data points to Amazon CloudWatch for your load balancers and your targets. CloudWatch enables you to retrieve statistics about those data points as an ordered set of time-series data, known as metrics. You can use metrics to verify that your system is performing as expected. This is the right feature if you wish to track a certain metric.</p>\n<p><strong>ALB request tracing</strong> - You can use request tracing to track HTTP requests. The load balancer adds a header with a trace identifier to each request it receives. Request tracing will not help you to analyze latency specific data.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>A systems administrator at a company is trying to create a digital signature for SSH'ing into the Amazon EC2 instances.</p>\n<p>Which of the following entities can be used to facilitate this use-case?</p>\n", "answers": ["Multi-Factor Authentication (MFA)", "Access keys", "Key pairs", "Root user credentials"], "correct_answer": "Key pairs", "explanation": "<p>Correct option:</p>\n<p><strong>Key pairs</strong> - Key pairs consist of a public key and a private key. You use the private key to create a digital signature, and then AWS uses the corresponding public key to validate the signature. Key pairs are used only for Amazon EC2 and Amazon CloudFront. AWS does not provide key pairs for your account; you must create them. You can create Amazon EC2 key pairs from the Amazon EC2 console, CLI, or API. Key pairs make a robust combination for accessing an instance securely, a better option than using passwords.</p>\n<p>Incorrect options:</p>\n<p><strong>Multi-Factor Authentication (MFA)</strong> - Multi-factor authentication (MFA) provides an extra level of security that you can apply to your AWS account. With MFA enabled, when you sign in to the AWS website, you are prompted for your user name and password, and an authentication code from an MFA device. Together, they provide increased security for your AWS account settings and resources. Its an added layer of protection for AWS account users.</p>\n<p><strong>Access keys</strong> - Access keys consist of two parts: an access key ID and a secret access key. You use access keys to sign programmatic requests that you make to AWS if you use AWS CLI commands (using the SDKs) or using AWS API operations. These credentials are for accessing AWS services programmatically and not for accessing the EC2 instance directly.</p>\n<p><strong>Root user credentials</strong> - Root user credentials are the Email ID and password used to create the AWS account. This user has full privileges on the account created and has access to all services under his account. The root user can create access keys or key pairs from his account. But, the root account credentials cannot directly be used to access EC2 instances or create digital signatures.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A financial services firm wants to run its applications on single-tenant hardware to meet security guidelines.</p>\n<p>Which of the following is the MOST cost-effective way of isolating the Amazon EC2 instances to a single tenant?</p>\n", "answers": ["Dedicated Instances", "Spot Instances", "Dedicated Hosts", "On-Demand Instances"], "correct_answer": "Dedicated Instances", "explanation": "<p>Correct option:</p>\n<p><strong>Dedicated Instances</strong> - Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.</p>\n<p>A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.</p>\n<p>Differences between Dedicated Hosts and Dedicated Instances:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q64-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances</a></p>\n<p>Incorrect options:</p>\n<p><strong>Spot Instances</strong> -  A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price.  Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of the client and hence is not the correct option.</p>\n<p><strong>Dedicated Hosts</strong> - An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement.</p>\n<p><strong>On-Demand Instances</strong> - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements.</p>\n<p>High Level Overview of EC2 Instance Purchase Options:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt3-q64-i2.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>A healthcare solutions company is undergoing a compliance audit by the regulator. The company has hundreds of IAM users that make API calls but specifically it needs to be determined who is making KMS API calls.</p>\n<p>Which of the following services should the compliance team use?</p>\n", "answers": ["CloudTrail", "CloudWatch Metrics", "X-Ray", "Config"], "correct_answer": "CloudTrail", "explanation": "<p>Correct option:</p>\n<p><strong>CloudTrail</strong></p>\n<p>With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - “Who made an API call to modify this resource?”. CloudTrail provides event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. You cannot use CloudTrail to maintain a history of resource configuration changes.</p>\n<p>How CloudTrail Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png\"/>\nvia - <a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n<p>Exam Alert:</p>\n<p>You may see scenario-based questions asking you to select one of CloudWatch vs CloudTrail vs Config. Just remember this thumb rule -</p>\n<p>Think resource performance monitoring, events, and alerts; think CloudWatch.</p>\n<p>Think account-specific activity and audit; think CloudTrail.</p>\n<p>Think resource-specific history, audit, and compliance; think Config.</p>\n<p>Incorrect options:</p>\n<p><strong>CloudWatch Metrics</strong> - CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health.</p>\n<p>Amazon CloudWatch allows you to monitor AWS cloud resources and the applications you run on AWS. Metrics are provided automatically for a number of AWS products and services. CloudWatch cannot help determine the source for KMS API calls.</p>\n<p><strong>X-Ray</strong> - AWS X-Ray helps developers analyze and debug distributed applications. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray cannot help determine the source for KMS API calls.</p>\n<p><strong>Config</strong> - AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”. Config cannot help determine the source for KMS API calls.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n<p><a href=\"https://aws.amazon.com/cloudwatch/\">https://aws.amazon.com/cloudwatch/</a></p>\n<p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}]; 

    let correctCount = 0;
    let incorrectCount = 0;
    let totalQuestions = allQuizData.length;

    function shuffleArray(array) {
        for (let i = array.length - 1; i > 0; i--) {
            const j = Math.floor(Math.random() * (i + 1));
            [array[i], array[j]] = [array[j], array[i]];
        }
        return array;
    }

    function updateScoreDisplay() {
        document.getElementById('total-questions-display').textContent = totalQuestions;
        document.getElementById('correct-answers-display').textContent = correctCount;
        document.getElementById('wrong-answers-display').textContent = incorrectCount;
    }

    function renderQuiz() {
        const quizContainer = document.getElementById('quiz-container');
        quizContainer.innerHTML = '';
        correctCount = 0;
        incorrectCount = 0;

        const shuffledQuestions = shuffleArray([...allQuizData]);

        let questionCounter = 1;
        const sectionsMap = new Map();

        shuffledQuestions.forEach(qData => {
            const sectionName = qData.section || 'Uncategorized';
            if (!sectionsMap.has(sectionName)) {
                sectionsMap.set(sectionName, []);
            }
            sectionsMap.get(sectionName).push(qData);
        });

        sectionsMap.forEach((questionsInSection, sectionName) => {
            const sectionHeaderHtml = `<h2 class="section-header">${sectionName}</h2>`;
            quizContainer.insertAdjacentHTML('beforeend', sectionHeaderHtml);

            questionsInSection.forEach(qData => {
                const questionId = `q_${qData.id || questionCounter}`;
                const questionHtml = qData.question; // Giữ nguyên HTML
                const correctAnswers = qData.correct_answer;
                const explanation = qData.explanation;
                const questionType = qData.type;

                const shuffledAnswers = shuffleArray([...qData.answers]);

                let optionsListHtml = "";
                shuffledAnswers.forEach((ansText, i) => {
                    const inputId = `${questionId}_option_${i}`;
                    optionsListHtml += `
                    <label for="${inputId}" class="option-label">
                        <input type="${questionType}" id="${inputId}" name="${questionId}" value="${ansText.replace(/"/g, '&quot;')}" />
                        <span>${ansText}</span>
                    </label>
                    `;
                });
                
                const encodedCorrectAnswers = encodeURIComponent(JSON.stringify(correctAnswers));
                const formHtml = `
                <form id="form_${questionId}" class="question-container" data-question-id="${questionId}" data-correct-answer="${encodedCorrectAnswers}" data-question-type="${questionType}">
                    <p class="question-prompt">Câu hỏi ${questionCounter}:</p>
                    <div class="question-content">${questionHtml}</div> <div class="options-container">
                        ${optionsListHtml}
                    </div>
                    <div class="buttons-container">
                        <button type="button" class="submit-btn" onclick="checkAnswer('${questionId}')">Kiểm tra câu trả lời</button>
                        <button type="button" class="explanation-btn" onclick="showExplanation('${questionId}')">Xem giải thích</button>
                    </div>
                    <div id="feedback_${questionId}" class="feedback-message"></div>
                    <div id="explanation_content_${questionId}" style="display: none;">${explanation}</div>
                </form>
                `;
                quizContainer.insertAdjacentHTML('beforeend', formHtml);
                questionCounter++;
            });
        });
        updateScoreDisplay();
    }

    function checkAnswer(questionId) {
        const form = document.getElementById(`form_${questionId}`);
        const questionType = form.dataset.questionType;
        const correctAnswers = JSON.parse(decodeURIComponent(form.dataset.correctAnswer)); 
        const feedbackDiv = document.getElementById(`feedback_${questionId}`);
        const submitBtn = form.querySelector('.submit-btn');

        let selectedOptions = [];
        if (questionType === 'radio') {
            const selectedRadio = form.querySelector(`input[name="${questionId}"]:checked`);
            if (selectedRadio) {
                selectedOptions.push(selectedRadio.value);
            }
        } else { /* checkbox */
            form.querySelectorAll(`input[name="${questionId}"]:checked`).forEach(checkbox => {
                selectedOptions.push(checkbox.value);
            });
        }

        if (selectedOptions.length === 0) {
            feedbackDiv.className = 'incorrect-feedback';
            feedbackDiv.innerHTML = 'Vui lòng chọn ít nhất một câu trả lời!';
            return; 
        }

        if (form.dataset.answered === 'true') { 
             return;
        }

        form.querySelectorAll(`input[name="${questionId}"]`).forEach(input => {
            input.disabled = true;
        });
        submitBtn.disabled = true;
        submitBtn.style.opacity = '0.6';
        submitBtn.style.cursor = 'not-allowed';

        let isCorrect = false;
        if (questionType === 'radio') {
            isCorrect = (selectedOptions[0] === correctAnswers);
        } else { /* checkbox */
            const sortedSelected = selectedOptions.sort();
            const sortedCorrect = correctAnswers.sort();
            isCorrect = (sortedSelected.length === sortedCorrect.length &&
                         sortedSelected.every((val, index) => val === sortedCorrect[index]));
        }

        form.querySelectorAll('.option-label').forEach(label => {
            label.classList.remove('correct', 'incorrect');
        });

        if (isCorrect) {
            feedbackDiv.className = 'correct-feedback';
            feedbackDiv.innerHTML = 'Chính xác!';
            correctCount++;
        } else {
            feedbackDiv.className = 'incorrect-feedback';
            feedbackDiv.innerHTML = 'Sai rồi. Câu trả lời đúng là: ' + 
                                    (Array.isArray(correctAnswers) ? correctAnswers.join('; ') : correctAnswers);
            incorrectCount++;
        }
        
        form.dataset.answered = 'true';

        form.querySelectorAll(`input[name="${questionId}"]`).forEach(input => {
            const label = input.closest('.option-label');
            if (label) {
                if (questionType === 'radio') {
                    if (input.value === correctAnswers) {
                        label.classList.add('correct');
                    } else if (input.checked) { 
                        label.classList.add('incorrect');
                    }
                } else { // checkbox
                    if (correctAnswers.includes(input.value)) {
                        label.classList.add('correct');
                    } else if (input.checked) { 
                        label.classList.add('incorrect');
                    }
                }
            }
        });
        updateScoreDisplay();
    }

    const modal = document.getElementById('explanationModal');
    const modalContentBody = document.getElementById('modalExplanationContent');
    const closeBtn = document.querySelector('.close-button');

    function showExplanation(questionId) {
        const explanationText = document.getElementById(`explanation_content_${questionId}`).innerHTML;
        modalContentBody.innerHTML = explanationText; // Hiển thị HTML giải thích
        modal.style.display = 'block';
    }

    closeBtn.onclick = function() {
        modal.style.display = 'none';
    }

    window.onclick = function(event) {
        if (event.target == modal) {
            modal.style.display = 'none';
        }
    }
    // Chạy hàm renderQuiz khi trang được tải hoặc làm mới
    document.addEventListener('DOMContentLoaded', renderQuiz);
    
        </script>
    </body>
    </html>
    
