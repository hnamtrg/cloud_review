
    <!DOCTYPE html>
    <html lang="vi">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Practice Test #1 - AWS Certified SysOps Administrator Associate</title>
        <style>
            
    body {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        line-height: 1.6;
        margin: 0;
        padding-top: 60px; /* Space for score-stats-container */
        background-color: #020617;
        color: #c7d1dd;
        font-size: 16px;
    }
    main {
        max-width: 850px;
        margin: 0 auto;
        padding: 20px;
    }
    h1 {
        text-align: center;
        color: #e0e7ff;
        margin-bottom: 20px;
        font-size: 2em;
    }
    .quiz-description {
        background-color: #0f172a;
        padding: 15px;
        border-radius: 8px;
        margin-bottom: 30px;
        border: 1px solid #1e293b;
        color: #a0aec0;
    }
    .question-container {
        background-color: #0f172a;
        border: 1px solid #1e293b;
        border-radius: 8px;
        padding: 20px;
        margin-bottom: 25px;
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    }
    .question-prompt {
        font-weight: 600;
        margin-bottom: 15px;
        color: #e0e7ff;
        font-size: 1.1em;
    }
    .options-container {
        display: flex;
        flex-direction: column;
        gap: 10px;
        margin-bottom: 15px;
    }
    .option-label {
        display: flex;
        align-items: center;
        gap: 10px;
        padding: 10px 15px;
        border: 1px solid #334155;
        border-radius: 6px;
        cursor: pointer;
        transition: background-color 0.2s, border-color 0.2s;
    }
    .option-label:hover {
        background-color: #1e293b;
    }
    input[type="radio"],
    input[type="checkbox"] {
        appearance: none; /* Hide default radio/checkbox */
        width: 20px;
        height: 20px;
        border: 2px solid #64748b;
        border-radius: 50%; /* For radio */
        background-color: transparent;
        display: grid;
        place-content: center;
        flex-shrink: 0;
        cursor: pointer;
    }
    input[type="checkbox"] {
        border-radius: 4px; /* For checkbox */
    }
    input[type="radio"]::before,
    input[type="checkbox"]::before {
        content: '';
        width: 10px;
        height: 10px;
        border-radius: 50%; /* For radio */
        transform: scale(0);
        transition: transform 0.2s ease-in-out;
        background-color: #3b82f6; /* Active color */
    }
    input[type="checkbox"]::before {
        border-radius: 2px; /* For checkbox */
    }
    input[type="radio"]:checked::before,
    input[type="checkbox"]:checked::before {
        transform: scale(1);
    }
    input[type="radio"]:checked,
    input[type="checkbox"]:checked {
        border-color: #3b82f6;
    }

    .submit-btn, .explanation-btn {
        background-color: #22c55e; /* Success green */
        color: white;
        border: none;
        padding: 10px 20px;
        border-radius: 6px;
        cursor: pointer;
        font-size: 1em;
        transition: background-color 0.2s;
        margin-top: 15px;
        width: fit-content;
    }
    .submit-btn:hover {
        background-color: #16a34a;
    }
    .explanation-btn {
        background-color: #3b82f6; /* Info blue */
        margin-left: 10px;
    }
    .explanation-btn:hover {
        background-color: #2563eb;
    }

    .correct-feedback {
        border: 2px solid #22c55e; /* Green for correct */
        background-color: #dcfce7;
        color: #15803d;
        padding: 10px;
        border-radius: 6px;
        margin-top: 10px;
    }
    .incorrect-feedback {
        border: 2px solid #ef4444; /* Red for incorrect */
        background-color: #fee2e2;
        color: #b91c1c;
        padding: 10px;
        border-radius: 6px;
        margin-top: 10px;
    }
    .option-label.correct {
        border-color: #22c55e;
        background-color: #2f453a;
    }
    .option-label.incorrect {
        border-color: #ef4444;
        background-color: #3f2f31;
    }

    /* Modal styles */
    .modal {
        display: none; /* Hidden by default */
        position: fixed; /* Stay in place */
        z-index: 1; /* Sit on top */
        left: 0;
        top: 0;
        width: 100%; /* Full width */
        height: 100%; /* Full height */
        overflow: auto; /* Enable scroll if needed */
        background-color: rgba(0,0,0,0.7); /* Black w/ opacity */
    }
    .modal-content {
        background-color: #0f172a;
        margin: 15% auto; /* 15% from the top and centered */
        padding: 20px;
        border: 1px solid #888;
        width: 80%; /* Could be more or less, depending on screen size */
        border-radius: 10px;
        position: relative;
        color: #e0e7ff;
    }
    .close-button {
        color: #aaa;
        float: right;
        font-size: 28px;
        font-weight: bold;
    }
    .close-button:hover,
    .close-button:focus {
        color: #fff;
        text-decoration: none;
        cursor: pointer;
    }
    .modal-body {
        margin-top: 20px;
    }
    .section-header {
        font-size: 1.4em;
        font-weight: bold;
        margin-top: 30px;
        margin-bottom: 20px;
        padding-bottom: 10px;
        border-bottom: 2px solid #64748b;
        color: #e0e7ff;
    }
    #score-stats-container {
        position: fixed;
        z-index: 10;
        top: 0;
        height: auto; /* Allow height to adjust */
        width: 100%;
        background-color: #020617;
        padding: 8px 16px; /* Tăng padding trên dưới để có thêm không gian */
        color: #e0e7ff;
        font-weight: 600;
        display: flex; /* Dùng flexbox */
        flex-wrap: wrap; /* Cho phép các mục xuống dòng nếu không đủ chỗ */
        align-items: center;
        justify-content: space-around; /* Phân bổ không gian đều */
        box-shadow: 0 2px 5px rgba(0,0,0,0.3);
        gap: 15px; /* Khoảng cách giữa các mục */
    }
    #score-stats-container div {
        flex-shrink: 0; /* Ngăn các mục co lại */
        white-space: nowrap; /* Giữ các nhãn trên một dòng */
    }
    .question-prompt img {
        max-width: 100%; /* Đảm bảo hình ảnh không tràn ra ngoài */
        height: auto;
        display: block; /* Để kiểm soát margin dễ dàng hơn */
        margin-top: 10px; /* Khoảng cách giữa văn bản và hình ảnh */
        border-radius: 5px;
    }
    
        </style>
    </head>
    <body>
        <section id="score-stats-container">
            <div class="score-item">Tổng số câu hỏi: <span id="total-questions-display">0</span></div>
            <div class="score-item">Trả lời đúng: <span id="correct-answers-display">0</span></div>
            <div class="score-item">Trả lời sai: <span id="wrong-answers-display">0</span></div>
        </section>
        <main>
            <h1>Practice Test #1 - AWS Certified SysOps Administrator Associate</h1>
            <div class="quiz-description">
                <h3>Giới thiệu về bài kiểm tra này:</h3>
                <p>About this practice exam: - questions order and response orders are randomized - you can only review the answer after finishing the exam due to how Udemy works - it consists of 65 questions, the duration is 130 minutes, the passing score is 720 ====== In case of an issue with a question: - ask a question in the Q&A - please take a screenshot of the question (because they're randomized) and attach it - we will get back to you as soon as possible and fix the issue Good luck, and happy learning!</p>
                <p><strong>Điểm đậu:</strong> 72%</p>
            </div>
            <section id="quiz-container"></section> 

            <div id="explanationModal" class="modal">
                <div class="modal-content">
                    <span class="close-button">&times;</span>
                    <h2>Giải thích</h2>
                    <div class="modal-body" id="modalExplanationContent">
                        </div>
                </div>
            </div>
        </main>
        <script>
            
    const allQuizData = [{"question": "<p>A retail company has built its server infrastructure on Amazon EC2 instances that run on Windows OS. The development team has defined a few custom metrics that need to be collected by the unified CloudWatch agent.</p>\n<p>As a SysOps Administrator, can you identify the correct configuration to be used for this scenario?</p>\n", "answers": ["Configure the CloudWatch agent with collectd protocol to collect the necessary system metrics", "CloudWatch agent can be configured with either StatsD protocol or collectd protocol to collect the necessary system metrics on windows servers", "Configure the CloudWatch agent with StatsD protocol to collect the necessary system metrics", "Unified CloudWatch agent cannot be custom configured"], "correct_answer": "Configure the CloudWatch agent with StatsD protocol to collect the necessary system metrics", "explanation": "<p>Correct option:</p>\n<p><strong>Configure the CloudWatch agent with StatsD protocol to collect the necessary system metrics</strong> - You can retrieve custom metrics from your applications or services using the StatsD and collectd protocols. StatsD is supported on both Linux servers and servers running Windows Server. collectd is supported only on Linux servers. Here, the instances are running on Windows servers, hence StatsD is the right protocol.</p>\n<p>More information on Collecting Metrics and Logs from Amazon EC2 Instances:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q2-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Configure the CloudWatch agent with collectd protocol to collect the necessary system metrics</strong> - collectd is supported only on Linux servers and hence it is not the correct choice here.</p>\n<p><strong>CloudWatch agent can be configured with either StatsD protocol or collectd protocol to collect the necessary system metrics on windows servers</strong> - StatsD is supported on both Linux servers and servers running Windows Server. collectd is supported only on Linux servers.</p>\n<p><strong>Unified CloudWatch agent cannot be custom configured</strong> - This is an incorrect statement and used only as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>A company has recently moved its server infrastructure to Amazon EC2 instances. The company needs to use CloudWatch metrics to track the state of each of the instances.</p>\n<p>Which of the following is the right way to configure the instances for CloudWatch monitoring to work?</p>\n", "answers": ["Configure CloudWatch from AWS Console for the instances that need to be monitored by CloudWatch. AWS automatically installs and configure the agent for the mentioned instances", "Install CloudWatch Agent on all the instances and attach an IAM role to the EC2 instances to be able to run the CloudWatch agent", "Install CloudWatch Agent on all the instances and attach an IAM user to the EC2 instances to be able to run the CloudWatch agent", "Install CloudWatch Agent on all the instances and attach necessary Security Groups to the EC2 instances to be able to run the CloudWatch agent"], "correct_answer": "Install CloudWatch Agent on all the instances and attach an IAM role to the EC2 instances to be able to run the CloudWatch agent", "explanation": "<p>Correct option:</p>\n<p><strong>Install CloudWatch Agent on all the instances and attach an IAM role to the EC2 instances to be able to run the CloudWatch agent</strong> - Access to AWS resources requires permissions. You create an IAM role, an IAM user, or both to grant permissions that the CloudWatch agent needs to write metrics to CloudWatch. If you're going to use the agent on Amazon EC2 instances, you must create an IAM role.</p>\n<p>You must attach the CloudWatchAgentServerRole IAM role to the EC2 instance to be able to run the CloudWatch agent on the instance. This role enables the CloudWatch agent to perform actions on the instance.</p>\n<p>Incorrect options:</p>\n<p><strong>Configure CloudWatch from AWS Console for the instances that need to be monitored by CloudWatch. AWS automatically installs and configure the agent for the mentioned instances</strong> - This is an incorrect statement. CloudWatch Agent has to be installed by the customer manually using the CLI on the instances.</p>\n<p><strong>Install CloudWatch Agent on all the instances and attach an IAM user to the EC2 instances to be able to run the CloudWatch agent</strong> - If you're going to use the CloudWatch agent on Amazon EC2 instances, you should create an IAM role.</p>\n<p><strong>Install CloudWatch Agent on all the instances and attach necessary Security Groups to the EC2 instances to be able to run the CloudWatch agent</strong> - CloudWatch agent gets the necessary permissions for collecting the metrics from EC2 instances using either an IAM role or an IAM user. Security Groups cannot be used for configuring CloudWatch agents.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance-fleet.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance-fleet.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>A junior developer working on configuring CloudWatch alarms is unable to figure out why a particular CloudWatch Alarm is constantly in the ALARM state.</p>\n<p>As a SysOps Administrator, which of these options would you suggest as a fix for the issue?</p>\n", "answers": ["Alarms continue to evaluate metrics against the configured threshold, even after they have already triggered. You can adjust the alarm threshold if you do not want it to be in ALARM state", "Once an alarm is triggered and an action is performed, the application logic has to reset the alarm to its normal state. This code has to be included by the development team", "Custom alarms once triggered remain in Alarm state till they are manually disabled from either the AWS Console or through application code", "CloudWatch alarm has been incorrectly configured and needs to be deleted and re-configured for fixing the persistent error"], "correct_answer": "Alarms continue to evaluate metrics against the configured threshold, even after they have already triggered. You can adjust the alarm threshold if you do not want it to be in ALARM state", "explanation": "<p>Correct option:</p>\n<p><strong>Alarms continue to evaluate metrics against the configured threshold, even after they have already triggered. You can adjust the alarm threshold if you do not want it to be in ALARM state</strong></p>\n<p>Alarms continue to evaluate metrics against your chosen threshold, even after they have already triggered. This allows you to view its current up-to-date state at any time. You may notice that one of your alarms stays in the ALARM state for a long time. If your metric value is still in breach of your threshold, the alarm will remain in the ALARM state until it no longer breaches the threshold. This is normal behavior. If you want your alarm to treat this new level as OK, you can adjust the alarm threshold accordingly.</p>\n<p>Incorrect options:</p>\n<p>Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms. Amazon CloudWatch can monitor AWS resources such as Amazon EC2 instances, Amazon DynamoDB tables, and Amazon RDS DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate. You can use Amazon CloudWatch to gain system-wide visibility into resource utilization, application performance, and operational health. You can use these insights to react and keep your application running smoothly.</p>\n<p><strong>Once an alarm is triggered and an action is performed, the application logic has to reset the alarm to its normal state. This code has to be included by the development team</strong></p>\n<p><strong>Custom alarms once triggered remain in Alarm state till they are manually disabled from either the AWS Console or through application code</strong></p>\n<p><strong>CloudWatch alarm has been incorrectly configured and needs to be deleted and re-configured for fixing the persistent error</strong></p>\n<p>These three options are all incorrect as these options contradict the explanation provided above.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/cloudwatch/faqs/\">https://aws.amazon.com/cloudwatch/faqs/</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>A development team has configured its AWS VPC with one public and one private subnet. The public subnet has an Amazon EC2 instance that hosts the application. The private subnet has the RDS database that the application needs to communicate with.</p>\n<p>Which of the following would you identify as the correct way to configure a solution for the given requirement?</p>\n", "answers": ["Elastic IP can be configured to initiate communication between private and public subnets", "Configure a VPC peering for enabling communication between the subnets", "Create a Security Group that allows connection from different subnets inside a VPC", "Subnets inside a VPC can communicate with each other without the need for any further configuration. Hence, no additional configurations are needed"], "correct_answer": "Subnets inside a VPC can communicate with each other without the need for any further configuration. Hence, no additional configurations are needed", "explanation": "<p>Correct option:</p>\n<p><strong>Subnets inside a VPC can communicate with each other without the need for any further configuration. Hence, no additional configurations are needed</strong> - Subnets inside a VPC can communicate with each other without any additional configurations.</p>\n<p>A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS Cloud. A subnet is a range of IP addresses in your VPC. You can launch AWS resources into a specified subnet. Use a public subnet for resources that must be connected to the internet, and a private subnet for resources that won't be connected to the internet.</p>\n<p>A route table contains a set of rules, called routes , that are used to determine where network traffic from your VPC is directed. You can explicitly associate a subnet with a particular route table. Otherwise, the subnet is implicitly associated with the main route table.</p>\n<p>The first entry in the Main route table is the default entry for local routing in the VPC; this entry enables the instances (potentially belonging to different subnets) in the VPC to communicate with each other.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q3-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Elastic IP can be configured to initiate communication between private and public subnets</strong> - An Elastic IP address is a reserved public IP address that you can assign to any EC2 instance in a particular region until you choose to release it. Elastic IP is not needed for resources to talk across subnets in the same VPC.</p>\n<p><strong>Configure a VPC peering for enabling communication between the subnets</strong> - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. It is not needed for resources inside the same VPC.</p>\n<p><strong>Create a Security Group that allows connection from different subnets inside a VPC</strong> - A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. Security groups act at the instance level, not the subnet level.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/how-it-works.html#what-is-route-tables\">https://docs.aws.amazon.com/vpc/latest/userguide/how-it-works.html#what-is-route-tables</a></p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>A university has just registered for an AWS account to help provide the necessary cloud infrastructure for the students planning to implement a Big Data analytics workflow as part of their thesis. The university has hired you to set up this infrastructure and help their technology team understand the basics of the AWS Virtual Private Cloud (VPC).</p>\n<p>As a SysOps Administrator, which of these would you identify as the correct options regarding VPC configurations? (Select three)</p>\n", "answers": ["Subnets, like VPCs, can span across Availability Zones, but will remain in a single AWS Region", "A private subnet, by default, does not have inbound data traffic from the internet. You create a route to the Internet Gateway in the subnet route table to allow access to the private subnet", "Regardless of the type of subnet, the internal IPv4 address range of the subnet is always private", "When you create a VPC, you must specify a range of IPv4 addresses for the VPC in the form of a Classless Inter-Domain Routing (CIDR) block", "By default, all subnets can route between each other, whether they are private or public", "If a subnet has a route to an internet gateway, along with traffic that can be routed to a virtual private gateway for a Site-to-Site VPN connection, the subnet is known as a VPN-only subnet"], "correct_answer": ["Regardless of the type of subnet, the internal IPv4 address range of the subnet is always private", "When you create a VPC, you must specify a range of IPv4 addresses for the VPC in the form of a Classless Inter-Domain Routing (CIDR) block", "By default, all subnets can route between each other, whether they are private or public"], "explanation": "<p>Correct option:</p>\n<p><strong>Regardless of the type of subnet, the internal IPv4 address range of the subnet is always private</strong> - Regardless of the type of subnet, the internal IPv4 address range of the subnet is always private. AWS never announces these address blocks to the internet.</p>\n<p><strong>When you create a VPC, you must specify a range of IPv4 addresses for the VPC in the form of a Classless Inter-Domain Routing (CIDR) block</strong> - When you create a VPC, you must specify a range of IPv4 addresses for the VPC in the form of a Classless Inter-Domain Routing (CIDR) block; for example, 10.0.0.0/16. This is the primary CIDR block for your VPC.</p>\n<p><strong>By default, all subnets can route between each other, whether they are private or public</strong> - Subnets created in a VPC can communicate with each other. The main route table facilitates this communication.</p>\n<p>Incorrect options:</p>\n<p><strong>Subnets, like VPCs, can span across Availability Zones, but will remain in a single AWS Region</strong> - Subnets must reside entirely within one Availability Zone and cannot span across AZs.</p>\n<p><strong>A private subnet, by default, does not have inbound data traffic from the internet. You create a route to the Internet Gateway in the subnet route table to allow access to the private subnet</strong> - If a subnet doesn't have a route to the internet gateway, the subnet is known as a private subnet. A private subnet leverages a public subnet to connect to the internet via the NAT Gateway. A private subnet cannot directly connect to the public internet (In the diagram shown below, subnet 2 is a private subnet).</p>\n<p><strong>If a subnet has a route to an internet gateway, along with traffic that can be routed to a virtual private gateway for a Site-to-Site VPN connection, the subnet is known as a VPN-only subnet</strong> - If a subnet doesn't have a route to the internet gateway, but has its traffic routed to a virtual private gateway for a Site-to-Site VPN connection, the subnet is known as a VPN-only subnet (In the diagram shown below, subnet 3 is a VPN-only subnet).</p>\n<p>Example VPC that has been configured with subnets in multiple Availability Zones:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q5-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html</a></p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html</a></p>\n", "section": "Domain 6: Networking", "type": "checkbox"}, {"question": "<p>A large online business uses multiple Amazon EBS volumes for their storage requirements. According to the company guidelines, the EBS snapshots have to be taken every few minutes to retain the business-critical data in case of failure.</p>\n<p>As a SysOps Administrator, can you suggest an effective way of addressing this requirement?</p>\n", "answers": ["Use Amazon CloudWatch events to schedule automated EBS Snapshots", "Use AWS Lambda functions to initiate automatic EBS snapshots every few minutes", "Use Amazon SNS Notification service to trigger AWS Lambda function that can initiate the EBS snapshots", "Automated EBS snapshots is a configurable item from Amazon EC2 configuration screen on AWS console"], "correct_answer": "Use Amazon CloudWatch events to schedule automated EBS Snapshots", "explanation": "<p>Correct option:</p>\n<p><strong>Use Amazon CloudWatch events to schedule automated EBS Snapshots</strong> - You can run CloudWatch Events rules according to a schedule. It is possible to create an automated snapshot of an existing Amazon Elastic Block Store (Amazon EBS) volume on a schedule. You can choose a fixed rate to create a snapshot every few minutes or use a cron expression to specify that the snapshot is made at a specific time of day.</p>\n<p>Snapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved. This minimizes the time required to create the snapshot and saves on storage costs by not duplicating data. Each snapshot contains all of the information that is needed to restore your data (from the moment when the snapshot was taken) to a new EBS volume.</p>\n<p>Steps to create a rule that takes snapshots on a schedule:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q6-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/TakeScheduledSnapshot.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/TakeScheduledSnapshot.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Use AWS Lambda functions to initiate automatic EBS snapshots every few minutes</strong> - AWS Lambda is not a self invoking function and needs a service to invoke it. Writing code to self invoke in the same Lambda function will result in too many parallel invocations and will turn out to be a very expensive solution. Hence, this option is incorrect.</p>\n<p><strong>Use Amazon SNS Notification service to trigger AWS Lambda function that can initiate the EBS snapshots</strong> - Amazon SNS and AWS Lambda are integrated so you can invoke Lambda functions with Amazon SNS notifications. Lambda function can be coded to take a snapshot of EBS volume every few minutes. However, this process is neither direct nor is cost effective way of achieving the stated requirement.</p>\n<p><strong>Automated EBS snapshots is a configurable item from Amazon EC2 configuration screen on AWS console</strong> - This is an incorrect statement and given only as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/TakeScheduledSnapshot.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/TakeScheduledSnapshot.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>A media company stores all their articles on Amazon S3 buckets. As a security measure, they have server access logging enabled for all the buckets. The company is looking at a solution that can regularly check if logging is enabled for all the existing buckets and for any new ones they create. If the solution can also automate the remedy, it will be a perfect fit for their requirement.</p>\n<p>Which of the following would you suggest to address the given use-case?</p>\n", "answers": ["Create a Lambda function that will check the logging status of all the S3 buckets and raise an Amazon SNS notification, if a remedy is needed", "Enable AWS CloudTrail to track the logging information for all the S3 buckets. Currently, AWS does not provide an automatic remediation process, hence, use a Lambda function to rectify any aberrations found during the checks", "Use AWS Config rules to check whether or not an S3 bucket has logging enabled, and carry out the necessary remediation if needed", "Amazon S3 server access logging is checked by AWS Trusted Advisor, as part of the best practices check it performs. Configure a remedy action with Trusted Advisor for all the resources that fails this best practice check"], "correct_answer": "Use AWS Config rules to check whether or not an S3 bucket has logging enabled, and carry out the necessary remediation if needed", "explanation": "<p>Correct option:</p>\n<p><strong>Use AWS Config rules to check whether or not an S3 bucket has logging enabled, and carry out the necessary remediation if needed</strong></p>\n<p>AWS Config keeps track of the configuration of your AWS resources and their relationships to other resources. It can also evaluate those AWS resources for compliance. This service uses rules that can be configured to evaluate AWS resources against desired configurations.</p>\n<p>For example, there are AWS Config rules that check whether or not your Amazon S3 buckets have logging enabled or your IAM users have an MFA device enabled. AWS Config rules use AWS Lambda functions to perform the compliance evaluations, and the Lambda functions return the compliance status of the evaluated resources as compliant or noncompliant. The non-compliant resources are remediated using the remediation action associated with the AWS Config rule. With the Auto-Remediation feature of AWS Config rules, the remediation action can be executed automatically when a resource is found non-compliant.</p>\n<p>AWS Config Auto Remediation feature has auto remediate feature for any non-compliant S3 buckets using the following AWS Config rules:</p>\n<p>s3-bucket-logging-enabled\ns3-bucket-server-side-encryption-enabled\ns3-bucket-public-read-prohibited\ns3-bucket-public-write-prohibited</p>\n<p>These AWS Config rules act as controls to prevent any non-compliant S3 activities.</p>\n<p>Incorrect options:</p>\n<p><strong>Create a Lambda function that will check the logging status of all the S3 buckets and raise an Amazon SNS notification, if a remedy is needed</strong> - AWS Lambda cannot poll by itself and needs a polling mechanism or a service to invoke it. Any logic written to self invoke in the same Lambda function will result in Lambda taking up all the resources available and running continuously, which will also become an expensive solution.</p>\n<p><strong>Enable AWS CloudTrail to track the logging information for all the S3 buckets. Currently, AWS does not provide an automatic remediation process, hence, use a Lambda function to rectify any aberrations found during the checks</strong> - As discussed above, remediation action is possible with AWS Config and hence is the right solution here.</p>\n<p><strong>Amazon S3 server access logging is checked by AWS Trusted Advisor, as part of the best practices check it performs. Configure a remedy action with Trusted Advisor for all the resources that fails this best practice check</strong> - AWS Trusted Advisor checks the configuration of Amazon Simple Storage Service (Amazon S3) buckets that have server access logging enabled. It recommends action after these checks but cannot automate a remedial action. Hence, Trusted advisor is not an optimal solution for the current scenario.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/\">https://aws.amazon.com/blogs/mt/aws-config-auto-remediation-s3-compliance/</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>A large IT project has multiple teams working on it. The teams share access across the resources - Amazon EC2 instances, Amazon S3 buckets and RDS database. A junior developer of a team ended up deleting data from a bucket that was used by various teams. This resulted in significant wastage of time and resources to mitigate the situation.</p>\n<p>As a SysOps Administrator, you have been hired to secure the data present in S3 buckets to prevent the recurrence of such events. Which of these options would you suggest to meet the given requirements?</p>\n", "answers": ["Enable S3 Object Lock to locks all the objects that are used in the project", "Use Amazon S3 bucket owner condition to restrict access to unintended users", "Use Amazon S3 replication to replicate critical objects to avoid losing them from unintended deletes", "Enable S3 versioning on all the buckets used in the project"], "correct_answer": "Enable S3 versioning on all the buckets used in the project", "explanation": "<p>Correct option:</p>\n<p><strong>Enable S3 versioning on all the buckets used in the project</strong> - Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures. When you enable versioning for a bucket, if Amazon S3 receives multiple write requests for the same object simultaneously, it stores all of the objects.</p>\n<p>Versioning-enabled buckets enable you to recover objects from accidental deletion or overwrite. For example:</p>\n<ol>\n<li><p>If you delete an object, instead of removing it permanently, Amazon S3 inserts a delete marker, which becomes the current object version. You can always restore the previous version.</p></li>\n<li><p>If you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version.</p></li>\n</ol>\n<p>Incorrect options:</p>\n<p><strong>Enable S3 Object Lock to locks all the objects that are used in the project</strong> - With S3 Object Lock, you can store objects using a write-once-read-many (WORM) model. You can use it to prevent an object from being deleted or overwritten for a fixed amount of time or indefinitely. This is not ideal for the given requirement where developers from different teams need to change the objects in the S3 buckets.</p>\n<p><strong>Use Amazon S3 bucket owner condition to restrict access to unintended users</strong> - Amazon S3 bucket owner condition ensures that the buckets you use in your S3 operations belong to the AWS accounts that you expect. Bucket owner condition enables you to verify that the target bucket is owned by the expected AWS account. As much as this is a useful feature, it's not helpful for the current scenario.</p>\n<p><strong>Use Amazon S3 replication to replicate critical objects to avoid losing them from unintended deletes</strong> - Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Replication is generally used to safeguard from failure or copy data across different environments (like production, testing, development). This is not helpful for the given scenario.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-owner-condition.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-owner-condition.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>A financial analytics company stores their confidential reports in an Amazon S3 bucket. These reports are no more valid or useful for the company after 5 years. Manual deletion is often delayed which results in higher storage costs for the company.</p>\n<p>As a SysOps Administrator, what would you do to delete the expired reports on-time to save costs?</p>\n", "answers": ["Configure the Amazon S3 bucket default settings to specify the \"Retain Until Date\" for all the objects in the bucket", "Disable versioning on the S3 bucket for which the retention period is being set, to avoid creating retention periods for all versions of the object. Then, configure the retention period in the object lock settings to 5 years", "Use S3 replication to replicate the latest data to another bucket and delete the entire bucket", "Configure the \"Retain Until Date\" in the object lock settings to a date that is 5 years away from the current date"], "correct_answer": "Configure the \"Retain Until Date\" in the object lock settings to a date that is 5 years away from the current date", "explanation": "<p>Correct option:</p>\n<p><strong>Configure the \"Retain Until Date\" in the object lock settings to a date that is 5 years away from the current date</strong> - A retention period protects an object version for a fixed amount of time. When you place a retention period on an object version, Amazon S3 stores a timestamp in the object version's metadata to indicate when the retention period expires. After the retention period expires, the object version can be overwritten or deleted unless you also placed a legal hold on the object version.</p>\n<p>You can place a retention period on an object version either explicitly or through a bucket default setting. When you apply a retention period to an object version explicitly, you specify a Retain Until Date for the object version. Amazon S3 stores the Retain Until Date setting in the object version's metadata and protects the object version until the retention period expires.</p>\n<p>Incorrect options:</p>\n<p><strong>Configure the Amazon S3 bucket default settings to specify the \"Retain Until Date\" for all the objects in the bucket</strong> - When you use bucket default settings, you don't specify a Retain Until Date. Instead, you specify a duration, in either days or years, for which every object version placed in the bucket should be protected.</p>\n<p><strong>Disable versioning on the S3 bucket for which the retention period is being set, to avoid creating retention periods for all versions of the object. Then, configure the retention period in the object lock settings to 5 years</strong> - This statement is incorrect. Object Lock works only in versioned buckets, and retention periods and legal holds apply to individual object versions. When you lock an object version, Amazon S3 stores the lock information in the metadata for that object version. Placing a retention period or legal hold on an object protects only the version specified in the request.</p>\n<p><strong>Use S3 replication to replicate the latest data to another bucket and delete the entire bucket</strong> - This method is resource, time and cost-intensive since the replication has to be done quite often to delete the old objects. This is an inelegant way to address the given requirement.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>The technology team at a retail company has set the <code>DisableApiTermination</code> attribute for a business-critical Amazon EC2 Windows instance to prevent termination of the instance via an API. This instance is behind an Auto Scaling Group (ASG) and the <code>InstanceInitiatedShutdownBehavior</code> attribute is set for the instance. A developer has initiated shutdown from the instance using operating system commands.</p>\n<p>What will be the outcome of the above scenario?</p>\n", "answers": ["The instance will not shutdown because DisableApiTermination attribute is set", "The instance will be terminated", "The operating system of the instance will send an Amazon SNS notification to the concerned person, that was configured when DisableApiTermination attribute was set. The operating system will hold the shutdown for few configured minutes and then progress with instance shutdown", "ASG cannot terminate an instance whose DisableApiTermination attribute is set"], "correct_answer": "The instance will be terminated", "explanation": "<p>Correct option:</p>\n<p><strong>The instance will be terminated</strong> - By default, you can terminate your instance using the Amazon EC2 console, command line interface, or API. To prevent your instance from being accidentally terminated using Amazon EC2, you can enable termination protection for the instance. The DisableApiTermination attribute controls whether the instance can be terminated using the console, CLI, or API. By default, termination protection is disabled for your instance. You can set the value of this attribute when you launch the instance, while the instance is running, or while the instance is stopped (for Amazon EBS-backed instances).</p>\n<p>The DisableApiTermination attribute does not prevent you from terminating an instance by initiating shutdown from the instance (using an operating system command for system shutdown) when the <code>InstanceInitiatedShutdownBehavior</code> attribute is set.</p>\n<p>Incorrect options:</p>\n<p><strong>The instance will not shutdown because <code>DisableApiTermination</code> attribute is set</strong> - As discussed above, this flag is only for controlling instance termination from console, command line interface, or API. If does not protect from shutdown commands issued from the operating system of the instance if the <code>InstanceInitiatedShutdownBehavior</code> attribute is set..</p>\n<p><strong>The operating system of the instance will send an Amazon SNS notification to the concerned person, that was configured when <code>DisableApiTermination</code> attribute was set. The operating system will hold the shutdown for few configured minutes and then progress with instance shutdown</strong> - This is a made-up option and given only as a distractor.</p>\n<p><strong>ASG cannot terminate an instance whose <code>DisableApiTermination</code> attribute is set</strong> - This statement is false. The DisableApiTermination attribute does not prevent Amazon EC2 Auto Scaling from terminating an instance.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/terminating-instances.html#Using_ChangingDisableAPITermination\">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/terminating-instances.html#Using_ChangingDisableAPITermination</a></p>\n", "section": "Domain 2: High Availability", "type": "radio"}, {"question": "<p>A production-ready application has just been deployed to Amazon EC2 instance that uses MySQL RDS as the database. The team is looking at making the RDS deployment highly available and failure-proof.</p>\n<p>As a SysOps Administrator, can you suggest an easy and effective way of configuring this requirement?</p>\n", "answers": ["Configure automated backups for the RDS instance, to retrieve data and instance status, if needed after a failure", "Scale up your DB instance when you are approaching storage capacity limits", "Configure your JVM with a TTL value of no more than 60 seconds, to help you re-establish the connection to your database, in case of failure", "Configure the RDS to be a multi Availability Zone (AZ) deployment"], "correct_answer": "Configure the RDS to be a multi Availability Zone (AZ) deployment", "explanation": "<p>Correct option:</p>\n<p><strong>Configure the RDS to be a multi Availability Zone (AZ) deployment</strong> - Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments.</p>\n<p>In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups. Running a DB instance with high availability can enhance availability during planned system maintenance, and help protect your databases against DB instance failure and Availability Zone disruption.</p>\n<p>Using the RDS console, you can create a Multi-AZ deployment by simply specifying Multi-AZ when creating a DB instance. You can use the console to convert existing DB instances to Multi-AZ deployments by modifying the DB instance and specifying the Multi-AZ option. You can also specify a Multi-AZ deployment with the AWS CLI or Amazon RDS API. Use the create-db-instance or modify-db-instance CLI command, or the CreateDBInstance or ModifyDBInstance API operation.</p>\n<p>Incorrect options:</p>\n<p><strong>Configure automated backups for the RDS instance, to retrieve data and instance status, if needed after a failure</strong> - The automated backup feature of Amazon RDS enables point-in-time recovery for your database instance. Amazon RDS will backup your database and transaction logs and store both for a user-specified retention period. Backups do not make the architecture highly available, a critical database should be deployed as a multi-AZ deployment, to cater to failures.</p>\n<p><strong>Scale up your DB instance when you are approaching storage capacity limits</strong> - This is vertical scaling and is not helpful when the requirement is high availability since there is still only one instance.</p>\n<p><strong>Configure your JVM with a TTL value of no more than 60 seconds, to help you re-establish the connection to your database, in case of failure</strong> - This change is part of high availability configuration and is needed when failover happens. But, multi -Z deployment is a pre-requisite for the DB architecture to be highly available.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p>\n", "section": "Domain 2: High Availability", "type": "radio"}, {"question": "<p>A healthcare company aggregates the daily data via a nightly batch job that runs for two hours and then sends the output to downstream applications. Currently, this job is run on On-Demand instances. The company is looking at reducing costs for this job since it is not critical to business and the job can be re-initiated if it fails.</p>\n<p>Which of the following would you recommend as the most optimal option for this use-case?</p>\n", "answers": ["Opt for Spot block instances", "Opt for Spot instances", "Purchase reserved instances that offer instances at high discounts", "Use a combination of On-Demand and Reserved instances to reduce cost while keeping a minimum capacity to run without interruption"], "correct_answer": "Opt for Spot block instances", "explanation": "<p>Correct option:</p>\n<p><strong>Opt for Spot block instances</strong> - Spot Instances with a specified duration are known as Spot blocks. Spot blocks are designed not to be interrupted and will run continuously for the duration you select. In rare situations, Spot blocks may be interrupted due to Amazon EC2 capacity needs. In these cases, AWS provides a two-minute warning before they terminate an instance, and you are not charged for the terminated instances even if you used them.</p>\n<p>Spot block instances are the right choice since they are Spot instances with the least possible cost offering and they have a very high certainty of not being stopped. The use case also mentions that the job is not business-critical and can be re-initiated in case of failure without loss to business hence using Spot blocks is a perfect choice here.</p>\n<p>Incorrect options:</p>\n<p><strong>Opt for Spot instances</strong> - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot instance could have been the right choice if Spot blocks were not given in the choice.</p>\n<p><strong>Purchase reserved instances that offer instances at high discounts</strong> - Reserved Instances provide you with significant savings on your Amazon EC2 costs compared to On-Demand Instance pricing. However, you can purchase a Reserved Instance for a one-year or three-year commitment, with the three-year commitment offering a bigger discount. Since the time period is not mentioned in the use case, using Reserved instances is not a good option since they come with an upfront term commitment.</p>\n<p><strong>Use a combination of On-Demand and Reserved instances to reduce cost while keeping a minimum capacity to run without interruption</strong> - On-Demand instances are the costliest of all the instance purchasing options available. Since the job is not business critical, it is better to not opt for On-Demand instances.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>AWS Shared Responsibility Model discusses the responsibilities that customers and AWS share for different services and resources.</p>\n<p>For an abstracted service like Amazon S3, which of the following is the responsibility of AWS?</p>\n", "answers": ["Managing the data present in S3 Buckets", "Choosing encryption options for data present in S3 buckets", "Defining rules to move data across different S3 storage classes", "Maintaining the operating systems and platforms for Amazon S3"], "correct_answer": "Maintaining the operating systems and platforms for Amazon S3", "explanation": "<p>Correct option:</p>\n<p><strong>Maintaining the operating systems and platforms for Amazon S3</strong> - Security and Compliance is a shared responsibility between AWS and the customer. AWS is responsible for “Security of the Cloud” and the customer is responsible for “Security in the Cloud”.</p>\n<p>Customer responsibility will be determined by the AWS Cloud services that a customer selects. This determines the amount of configuration work that the customer must perform as part of their security responsibilities.</p>\n<p>For abstracted services, such as Amazon S3 and Amazon DynamoDB, AWS operates the infrastructure layer, the operating system, and platforms, and customers access the endpoints to store and retrieve data. Customers are responsible for managing their data (including encryption options), classifying their assets, and using IAM tools to apply the appropriate permissions.</p>\n<p>AWS Shared Responsibility Model:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q13-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/compliance/shared-responsibility-model/\">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p>\n<p>Incorrect options:</p>\n<p><strong>Managing the data present in S3 Buckets</strong> - This is the responsibility of the customer. Managing the storage infrastructure and redundantly storing data across AZs is the responsibility of AWS.</p>\n<p><strong>Choosing encryption options for data present in S3 buckets</strong> - Choosing an encryption option that suits the business requirements is the responsibility of the customer. Providing the different encryption options, maintaining and managing the security keys is the responsibility of AWS.</p>\n<p><strong>Defining rules to move data across different S3 storage classes</strong> - Defining rules to move data across storage classes is the responsibility of the customer. But, maintaining the backbone infrastructure of these storage classes - including hardware and software is the responsibility of AWS.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/compliance/shared-responsibility-model/\">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A systems administration team is configuring Amazon EC2 metrics that are sent to Amazon CloudWatch for monitoring purposes. The team is looking for a metric that can help them identify the processing power required to run an application on the selected instance.</p>\n<p>Which of the below metric should be used for this requirement?</p>\n", "answers": ["CPUCreditUsage metric should be used to identify the processing power required", "CPUUtilization metric should be used to identify the processing power required", "ResourceCount is the correct metric to identify the processing power required", "CPUProcessPower metric should be used to identify the processing power required"], "correct_answer": "CPUUtilization metric should be used to identify the processing power required", "explanation": "<p>Correct option:\n<strong><code>CPUUtilization</code> metric should be used to identify the processing power required</strong> - <code>CPUUtilization</code> specifies the percentage of allocated EC2 compute units that are currently in use on the instance. This metric identifies the processing power required to run an application on a selected instance. This metric is expressed in Percent.</p>\n<p>Depending on the instance type, tools in your operating system can show a lower percentage than CloudWatch when the instance is not allocated a full processor core.</p>\n<p>Incorrect options:</p>\n<p><strong><code>CPUCreditUsage</code> metric should be used to identify the processing power required</strong> - This metric identifies the number of CPU credits spent by the instance for CPU utilization. CPU credit metrics are available at a five-minute frequency only. If you specify a period greater than five minutes, use the Sum statistic instead of the Average statistic. The units of this metric are Credits (vCPU-minutes).</p>\n<p><strong><code>ResourceCount</code> is the correct metric to identify the processing power required</strong> - <code>ResourceCount</code> metric defines the number of the specified resources running in your account. The resources are defined by the dimensions associated with the metric.</p>\n<p><strong><code>CPUProcessPower</code> metric should be used to identify the processing power required</strong> - <code>CPUProcessPower</code> is a made-up option, given only as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>As a SysOps Administrator, you have been contacted by a team for troubleshooting a security issue they seem to be facing. A security check red flag is being raised for the security groups created by AWS Directory Services. The flag message says \"Security Groups - Unrestricted Access.\"</p>\n<p>How will you troubleshoot this issue?</p>\n", "answers": ["The security group configurations have to be checked and edited to cater to AWS security standards", "Use AWS Trusted Advisor to know the exact reason for this error and take action as recommended by the Trusted Advisor", "Ignore or suppress the red flag since it is safe to do so, in this scenario", "AWS Directory Service might have been initiated from an account that does not have proper permissions. Check the permissions on the IAM roles and IAM users used to initiate the service"], "correct_answer": "Ignore or suppress the red flag since it is safe to do so, in this scenario", "explanation": "<p>Correct option:</p>\n<p><strong>Ignore or suppress the red flag since it is safe to do so, in this scenario</strong> - AWS Directory Services is a managed service that automatically creates an AWS security group in your VPC with network rules for traffic in and out of AWS managed domain controllers. The default inbound rules allow traffic from any source (0.0.0.0/0) to ports required by Active Directory. These rules do not introduce security vulnerabilities, as traffic to the domain controllers is limited to traffic from your VPC, other peered VPCs, or networks connected using AWS Direct Connect, AWS Transit Gateway or Virtual Private Network.</p>\n<p>In addition, the ENIs the security group is attached to, do not and cannot have Elastic IPs attached to them, limiting inbound traffic to local VPC and VPC routed traffic.</p>\n<p>Incorrect options:</p>\n<p><strong>The security group configurations have to be checked and edited to cater to AWS security standards</strong></p>\n<p><strong>Use AWS Trusted Advisor to know the exact reason for this error and take action as recommended by the Trusted Advisor</strong></p>\n<p><strong>AWS Directory Service might have been initiated from an account that does not have proper permissions. Check the permissions on the IAM roles and IAM users used to initiate the service</strong></p>\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/faqs/\">https://aws.amazon.com/premiumsupport/faqs/</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>A video streaming app uses Amazon Kinesis Data Streams for streaming data. The systems administration team needs to be informed of the shard capacity when it is reaching its limits.</p>\n<p>How will you configure this requirement?</p>\n", "answers": ["Configure Amazon CloudWatch Events to pick data from Amazon Inspector", "Use CloudWatch ServiceLens to monitor data on service limits of various AWS services", "Monitor Trusted Advisor service check results with Amazon CloudWatch Events", "Configure Amazon CloudTrail to generate logs for the service limits. CloudTrail and CloudWatch are integrated and hence alarm can be generated for customized service checks"], "correct_answer": "Monitor Trusted Advisor service check results with Amazon CloudWatch Events", "explanation": "<p>Correct option:</p>\n<p><strong>Monitor Trusted Advisor service check results with Amazon CloudWatch Events</strong> - AWS Trusted Advisor checks for service usage that is more than 80% of the service limit.</p>\n<p>A partial list of Trusted Advisor service limit checks:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q17-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>\n<p>You can use Amazon CloudWatch Events to detect and react to changes in the status of Trusted Advisor checks. Then, based on the rules that you create, CloudWatch Events invokes one or more target actions when a status check changes to the value you specify in a rule. Depending on the type of status change, you might want to send notifications, capture status information, take corrective action, initiate events, or take other actions.</p>\n<p>Incorrect options:</p>\n<p><strong>Configure Amazon CloudWatch Events to pick data from Amazon Inspector</strong> - Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances. Not the right service for the given requirement.</p>\n<p><strong>Use CloudWatch ServiceLens to monitor data on service limits of various AWS services</strong> - CloudWatch ServiceLens enhances the observability of your services and applications by enabling you to integrate traces, metrics, logs, and alarms into one place. So, ServiceLens can be used once we define the alarms in CloudWatch, not without it.</p>\n<p><strong>Configure Amazon CloudTrail to generate logs for the service limits. CloudTrail and CloudWatch are integrated and hence alarm can be generated for customized service checks</strong> - AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail however, does not monitor service limits.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html\">https://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ServiceLens.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ServiceLens.html</a></p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>A development team has written configurable scripts that need to be run every day to monitor the business endpoints and APIs. The team wants to integrate these scripts with Amazon CloudWatch service to help in overall monitoring and analysis.</p>\n<p>What is the right way of configuring this requirement?</p>\n", "answers": ["Use CloudWatch Synthetics to create canaries which create CloudWatch metrics to track and monitor the services", "Configure a CloudWatch Composite Alarm and integrate the configurable script, written by the team, with the CloudWatch logs", "CloudWatch Dashboard settings can be used to integrate the use-written scripts into Alarms generated and managed by CloudWatch", "Use CloudWatch ServiceLens to integrate the custom script into CloudWatch system for generating metrics and logs"], "correct_answer": "Use CloudWatch Synthetics to create canaries which create CloudWatch metrics to track and monitor the services", "explanation": "<p>Correct option:</p>\n<p><strong>Use CloudWatch Synthetics to create canaries, which create CloudWatch metrics to track and monitor the services</strong> - You can use Amazon CloudWatch Synthetics to create canaries, configurable scripts that run on a schedule, to monitor your endpoints and APIs. Canaries follow the same routes and perform the same actions as a customer, which makes it possible for you to continually verify your customer experience even when you don't have any customer traffic on your applications. By using canaries, you can discover issues before your customers do.</p>\n<p>Canaries are Node.js scripts. They create Lambda functions in your account that use Node.js as a framework. Canaries work over both HTTP and HTTPS protocols.</p>\n<p>UI canaries offer programmatic access to a headless Google Chrome Browser via Puppeteer. For more information about Puppeteer, see Puppeteer.</p>\n<p>Canaries check the availability and latency of your endpoints and can store load time data and screenshots of the UI. They monitor your REST APIs, URLs, and website content, and they can check for unauthorized changes from phishing, code injection and cross-site scripting.</p>\n<p>You can run a canary once or on a regular schedule. Scheduled canaries can run 24 hours a day, as often as once per minute.</p>\n<p>Incorrect options:</p>\n<p><strong>Configure a CloudWatch Composite Alarm and integrate the configurable script, written by the team, with the CloudWatch logs</strong> - A composite alarm includes a rule expression that takes into account the alarm states of other alarms that you have created. The composite alarm goes into ALARM state only if all conditions of the rule are met. The alarms specified in a composite alarm's rule expression can include metric alarms and other composite alarms. Not the right choice for the current scenario.</p>\n<p><strong>CloudWatch Dashboard settings can be used to integrate the user-written scripts into Alarms generated and managed by CloudWatch</strong> - This is a made-up option, given only as a distractor.</p>\n<p><strong>Use CloudWatch ServiceLens to integrate the custom script into CloudWatch system for generating metrics and logs</strong> - CloudWatch ServiceLens enhances the observability of your services and applications by enabling you to integrate traces, metrics, logs, and alarms into one place. ServiceLens integrates CloudWatch with AWS X-Ray to provide an end-to-end view of your application to help you more efficiently pinpoint performance bottlenecks and identify impacted users. A very useful service, but not for our current requirement.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Synthetics_Canaries.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Synthetics_Canaries.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ServiceLens.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ServiceLens.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>An e-commerce web application is built on a fleet of Amazon EC2 instances with an Auto Scaling Group. The application performance remains consistent throughout the day. But, for a few weeks now, users have been complaining about lagging screens and failing orders between 5-6 PM almost every day. Server logs show a sharp spike in user activity for this one hour every day.</p>\n<p>What is an optimal way to fix the issue while keeping the application available?</p>\n", "answers": ["Modify the Auto Scaling Group launch configuration to include more number of instances", "You can choose to manually add few more instances to the ASG to deal with the sudden spike", "Configure an Elastic Load Balancer, to replace the ASG, and move all the instances to ELB", "Create a scheduled scaling action to scale up before the traffic spike hits the servers"], "correct_answer": "Create a scheduled scaling action to scale up before the traffic spike hits the servers", "explanation": "<p>Correct option:</p>\n<p><strong>Create a scheduled scaling action to scale up before the traffic spike hits the servers</strong> - Scheduled scaling allows you to set your own scaling schedule. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the predictable traffic patterns of your web application. Scaling actions are performed automatically as a function of time and date.</p>\n<p>To configure your Auto Scaling group to scale based on a schedule, you create a scheduled action. The scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create a scheduled scaling action, you specify the start time when the scaling action should take effect, and the new minimum, maximum, and desired sizes for the scaling action. At the specified time, Amazon EC2 Auto Scaling updates the group with the values for minimum, maximum, and desired size that are specified by the scaling action.</p>\n<p>You can create scheduled actions for scaling one time only, or for scaling on a recurring schedule.</p>\n<p>Incorrect options:</p>\n<p><strong>Modify the Auto Scaling Group launch configuration to include more number of instances</strong> - An Auto Scaling group is associated with one launch configuration at a time, and you can't modify a launch configuration after you've created it.</p>\n<p><strong>You can choose to manually add few more instances to the ASG to deal with the sudden spike</strong> - At any time, you can change the size of an existing Auto Scaling group manually. You can either update the desired capacity of the Auto Scaling group, or update the instances that are attached to the Auto Scaling group. But, this is not an optimal solution, since it requires user intervention on daily basis and an elegant and effective method is already available.</p>\n<p><strong>Configure an Elastic Load Balancer, to replace the ASG, and move all the instances to ELB</strong> - Elastic Load Balancer can balance the incoming traffic across instances. It cannot scale-out and launch new instances in the absence of an attached Auto Scaling Group.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/scaling_plan.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/scaling_plan.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-manual-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-manual-scaling.html</a></p>\n", "section": "Domain 2: High Availability", "type": "radio"}, {"question": "<p>A financial services company runs its server infrastructure on a fleet of Amazon EC2 instances running behind an Auto Scaling Group (ASG). The SysOps Administrator has configured the instances to be protected from termination during scale-in.</p>\n<p>A scale-in event has occurred. What is the outcome of the event?</p>\n", "answers": ["The desired capacity of the ASG is decremented, but ASG will not be able to terminate any instance", "The minimum capacity of the ASG is decremented, but ASG will not be able to terminate any instance", "The desired capacity of the ASG is decremented and the instances are terminated based on the configuration", "When all instances are termination protected, scale-in event is not generated"], "correct_answer": "The desired capacity of the ASG is decremented, but ASG will not be able to terminate any instance", "explanation": "<p>Correct option:</p>\n<p><strong>The desired capacity of the ASG is decremented, but ASG will not be able to terminate any instance</strong> - To control whether an Auto Scaling group can terminate a particular instance when scaling in, use instance scale-in protection. You can enable the instance scale-in protection setting on an Auto Scaling group or an individual Auto Scaling instance.</p>\n<p>If all instances in an Auto Scaling group are protected from termination during scale in, and a scale-in event occurs, its desired capacity is decremented. However, the Auto Scaling group can't terminate the required number of instances until their instance scale-in protection settings are disabled.</p>\n<p>Instance scale-in protection does not protect Auto Scaling instances from the following:</p>\n<ol>\n<li><p>Manual termination through the Amazon EC2 console, the <code>terminate-instances</code> command, or the TerminateInstances action. To protect Auto Scaling instances from manual termination, enable Amazon EC2 termination protection.</p></li>\n<li><p>Health check replacement if the instance fails health checks. To prevent Amazon EC2 Auto Scaling from terminating unhealthy instances, suspend the ReplaceUnhealthy process.</p></li>\n<li><p>Spot Instance interruptions. A Spot Instance is terminated when capacity is no longer available or the Spot price exceeds your maximum price.</p></li>\n</ol>\n<p>Incorrect options:</p>\n<p><strong>The minimum capacity of the ASG is decremented, but ASG will not be able to terminate any instance</strong></p>\n<p><strong>The desired capacity of the ASG is decremented and the instances are terminated based on the configuration</strong></p>\n<p><strong>When all instances are termination protected, scale-in event is not generated</strong></p>\n<p>These three options contradict the explanation above, so these options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a></p>\n", "section": "Domain 2: High Availability", "type": "radio"}, {"question": "<p>A multi-national company extensively uses AWS CloudFormation to model and provision its AWS resources. A human error had earlier deleted a critical service from the CloudFormation stack that resulted in business loss. The company is looking at a quick and effective solution to lock the critical resources from any updates or deletes.</p>\n<p>As a SysOps Administrator, what will you suggest to address this requirement?</p>\n", "answers": ["Use nested stacks that will retain the configuration in the parent configuration even if the child configuration is lost or cannot be used", "Use Stack policies to protect critical stack resources from unintentional updates", "Use revision controls to protect critical stack resources from unintentional updates", "Use parameter constraints to specify the Identities that can update the Stack"], "correct_answer": "Use Stack policies to protect critical stack resources from unintentional updates", "explanation": "<p>Correct option:</p>\n<p><strong>Use Stack policies to protect critical stack resources from unintentional updates</strong></p>\n<p>Stack policies help protect critical stack resources from unintentional updates that could cause resources to be interrupted or even replaced. A stack policy is a JSON document that describes what update actions can be performed on designated resources. Specify a stack policy whenever you create a stack that has critical resources.</p>\n<p>During a stack update, you must explicitly specify the protected resources that you want to update; otherwise, no changes are made to protected resources.</p>\n<p>Example Stack policy:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q20-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Use nested stacks that will retain the configuration in the parent configuration even if the child configuration is lost or cannot be used</strong> - Nested stacks are stacks that create other stacks. As your infrastructure grows, common patterns can emerge in which you declare the same components in each of your templates. You can separate these common components and create dedicated templates for them. Nested stacks make it easy to manage resources, but it does not protect them from updation.</p>\n<p><strong>Use revision controls to protect critical stack resources from unintentional updates</strong> - Your stack templates describe the configuration of your AWS resources, such as their property values. To review changes and to keep an accurate history of your resources, use code reviews and revision controls. Although it's a useful feature, it is not relevant for the current scenario.</p>\n<p><strong>Use parameter constraints to specify the Identities that can update the Stack</strong> - With constraints, you can describe allowed input values so that AWS CloudFormation catches any invalid values before creating a stack. You can set constraints such as a minimum length, maximum length, and allowed patterns. However, you cannot protect resources from deletion.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#nested\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#nested</a></p>\n", "section": "DDomain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>A firm uses Amazon EC2 instances for running its flagship application. With new business expansion plans, the firm is looking at a bigger footprint for its AWS infrastructure. The development team needs to share Amazon Machine Images (AMIs) across AZs, AWS accounts and Regions.</p>\n<p>What are the key points to be considered before planning the expansion? (Select two)</p>\n", "answers": ["You can only share AMIs that have unencrypted volumes and volumes that are encrypted with a AWS-managed CMK", "You can only share AMIs that have unencrypted volumes and volumes that are encrypted with a customer-managed CMK", "You do not need to share the Amazon EBS snapshots that an AMI references in order to share the AMI", "You need to share any CMKs used to encrypt snapshots and any Amazon EBS snapshots that the AMI references", "AMIs are regional resources and can be shared across Regions"], "correct_answer": ["You can only share AMIs that have unencrypted volumes and volumes that are encrypted with a customer-managed CMK", "You do not need to share the Amazon EBS snapshots that an AMI references in order to share the AMI"], "explanation": "<p>Correct options:</p>\n<p><strong>You can only share AMIs that have unencrypted volumes and volumes that are encrypted with a customer-managed CMK</strong> - You can only share AMIs that have unencrypted volumes and volumes that are encrypted with a customer-managed CMK. If you share an AMI with encrypted volumes, you must also share any CMKs used to encrypt them.</p>\n<p><strong>You do not need to share the Amazon EBS snapshots that an AMI references in order to share the AMI</strong> - You do not need to share the Amazon EBS snapshots that an AMI references in order to share the AMI. Only the AMI itself needs to be shared; the system automatically provides the instance access to the referenced Amazon EBS snapshots for the launch.</p>\n<p>Incorrect options:</p>\n<p><strong>You can only share AMIs that have unencrypted volumes and volumes that are encrypted with an AWS-managed CMK</strong> - You cannot share an AMI that has volumes that are encrypted with an AWS-managed CMK.</p>\n<p><strong>You need to share any CMKs used to encrypt snapshots and any Amazon EBS snapshots that the AMI references</strong> - You do not need to share the Amazon EBS snapshots that an AMI references in order to share the AMI.</p>\n<p><strong>AMIs are regional resources and can be shared across Regions</strong> - AMIs are a regional resource. Therefore, sharing an AMI makes it available in that Region. To make an AMI available in a different Region, copy the AMI to the Region and then share it. Sharing an AMI from different Regions is not available.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html</a></p>\n", "section": "DDomain 3: Deployment and Provisioning", "type": "checkbox"}, {"question": "<p>A high traffic web application is built on Amazon Aurora for MySQL. To cater to read-only traffic, the systems administrator has been tasked to create the read replicas for the primary database. After initializing replica creation, Amazon RDS starts the replication process.</p>\n<p>Which of the following parameter shows that replica and source database have synced up completely?</p>\n", "answers": ["When the AuroraBinlogReplicaLag metric reaches -1, it implies that replica has caught up to the source DB instance", "When the AuroraReplicaLag metric reaches -1, it implies that replica has caught up to the source DB instance", "When the AuroraReplicaLag metric reaches 0, it implies that replica has caught up to the source DB instance", "When the AuroraBinlogReplicaLag metric reaches 0, it implies that replica has caught up to the source DB instance"], "correct_answer": "When the AuroraBinlogReplicaLag metric reaches 0, it implies that replica has caught up to the source DB instance", "explanation": "<p>Correct option:</p>\n<p><strong>When the AuroraBinlogReplicaLag metric reaches 0, it implies that the replica has caught up to the source DB instance</strong> - After you create a MySQL read replica and the replica is available, Amazon RDS first replicates the changes made to the source DB instance from the time the read replica create operation started. During this phase, the replication lag time for the read replica is greater than 0. You can monitor this lag time in Amazon CloudWatch by viewing the Amazon RDS AuroraBinlogReplicaLag metric.</p>\n<p>The AuroraBinlogReplicaLag metric reports the value of the Seconds_Behind_Master field of the MySQL SHOW SLAVE STATUS command. When the AuroraBinlogReplicaLag metric reaches 0, the replica has caught up to the source DB instance. If the AuroraBinlogReplicaLag metric returns -1, replication might not be active.</p>\n<p>Incorrect options:</p>\n<p>**When the AuroraBinlogReplicaLag metric reaches -1, it implies that replica has caught up to the source DB instance - A AuroraBinlogReplicaLag value of -1 can also mean that the Seconds_Behind_Master value can't be determined or is NULL.</p>\n<p>The AuroraBinlogReplicaLag metric returns -1 during a network outage or when a patch is applied during the maintenance window. In this case, wait for network connectivity to be restored or for the maintenance window to end before you check the AuroraBinlogReplicaLag metric again.</p>\n<p><strong>When the AuroraReplicaLag metric reaches -1, it implies that replica has caught up to the source DB instance</strong> - This is an incorrect metric for the given requirement. You can monitor how far an Aurora Replica is lagging behind the primary instance of your Aurora MySQL DB cluster by monitoring the Amazon CloudWatch AuroraReplicaLag metric.</p>\n<p><strong>When the AuroraReplicaLag metric reaches 0, it implies that replica has caught up to the source DB instance</strong> - This is an incorrect metric for the given requirement. You can monitor how far an Aurora Replica is lagging behind the primary instance of your Aurora MySQL DB cluster by monitoring the Amazon CloudWatch AuroraReplicaLag metric.</p>\n<p>References:</p>\n<p><a href=\"https://docs.amazonaws.cn/en_us/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.html\">https://docs.amazonaws.cn/en_us/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Monitoring.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Monitoring.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>As a SysOps Administrator, you maintain the Development account of a large team that comprises of both developers and testers. The Development account has two IAM groups: Developers and Testers. Users in both groups have permissions to work in the Development account and access resources there. From time to time, a developer must update the live S3 Bucket in the production account.</p>\n<p>How will you configure the permissions for developers to access the production environment?</p>\n", "answers": ["Create a Role in Development account, that defines the Production account as a trusted entity and specify a permissions policy that allows trusted users to update the bucket. Then, modify the IAM group policy in Development account, so that testers are denied access to the newly created role. Developers can use the newly created role to access the live S3 buckets in production environment", "Use Inline policies to be sure that the permissions in a policy are not inadvertently assigned to an identity other than the one they're intended for", "Create a Role in Production account, that defines the Development account as a trusted entity and specify a permissions policy that allows trusted users to update the bucket. Then, modify the IAM group policy in Development account, so that testers are denied access to the newly created role. Developers can use the newly created role to access the live S3 buckets in production environment", "Create a Role in Production account, that defines the Development account as a trusted entity and specify a permissions policy that allows trusted users to update the bucket. Developers can use the newly created role to access the live S3 buckets in production environment"], "correct_answer": "Create a Role in Production account, that defines the Development account as a trusted entity and specify a permissions policy that allows trusted users to update the bucket. Then, modify the IAM group policy in Development account, so that testers are denied access to the newly created role. Developers can use the newly created role to access the live S3 buckets in production environment", "explanation": "<p>Correct option:</p>\n<p><strong>Create a Role in Production account, that defines the Development account as a trusted entity and specify a permissions policy that allows trusted users to update the bucket. Then, modify the IAM group policy in Development account, so that testers are denied access to the newly created role. Developers can use the newly created role to access the live S3 buckets in production environment</strong> -</p>\n<p>First, you use the AWS Management Console to establish trust between the Production account and the Development account. You start by creating an IAM role. When you create the role, you define the Development account as a trusted entity and specify a permissions policy that allows trusted users to update the production bucket.</p>\n<p>You need to then modify the IAM group policy so that Testers are explicitly denied access to the created role.</p>\n<p>Finally, as a Developer, you use the created role to update the bucket in the Production account.</p>\n<p>Incorrect options:</p>\n<p><strong>Create a Role in Development account, that defines the Production account as a trusted entity and specify a permissions policy that allows trusted users to update the bucket. Then, modify the IAM group policy in Development account, so that testers are denied access to the newly created role. Developers can use the newly created role to access the live S3 buckets in production environment</strong> - Role has to be created in Production account since the resource to be accessed is in this account.</p>\n<p><em>Use Inline policies to be sure that the permissions in a policy are not inadvertently assigned to an identity other than the one they're intended for</em>* - An inline policy is a policy that's embedded in an IAM identity (a user, group, or role). That is, the policy is an inherent part of the identity. You can create a policy and embed it in an identity, either when you create the identity or later.</p>\n<p>Inline policies are useful if you want to maintain a strict one-to-one relationship between a policy and the identity that it's applied to. For example, you want to be sure that the permissions in a policy are not inadvertently assigned to an identity other than the one they're intended for. When you use an inline policy, the permissions in the policy cannot be inadvertently attached to the wrong identity.</p>\n<p><strong>Create a Role in Production account, that defines the Development account as a trusted entity and specify a permissions policy that allows trusted users to update the bucket. Developers can use the newly created role to access the live S3 buckets in production environment</strong> - This option does not deny access to Testers, so it is not correct.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#customer-managed-policies\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#customer-managed-policies</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A junior systems administrator has created read replicas for Amazon Aurora for MYSQL. The created read replicas are running into errors consistently.</p>\n<p>As a SysOps Administrator, which of the following items would you suggest while troubleshooting read replica errors? (Select two)</p>\n", "answers": ["To safely write to tables on a read replica, create indexes on the table after setting the read_only parameter to 0", "Though read replicas can work on both transactional and nontransactional storage engines, nontransactional engines are error-prone because of the way memory is managed on these engines", "Statements containing non-deterministic functions like SYSDATE() should be predefined in the configuration to successfully create the read replica", "Writing to tables on a read replica can break the replication", "If the value for the max_allowed_packet parameter for a read replica is less than the max_allowed_packet parameter for the source DB instance, replica errors occur"], "correct_answer": ["Writing to tables on a read replica can break the replication", "If the value for the max_allowed_packet parameter for a read replica is less than the max_allowed_packet parameter for the source DB instance, replica errors occur"], "explanation": "<p>Correct option:</p>\n<p><strong>Writing to tables on a read replica can break the replication</strong> - If you're writing to tables on the read replica, it can break replication.</p>\n<p><strong>If the value for the <code>max_allowed_packet</code> parameter for a read replica is less than the <code>max_allowed_packet</code> parameter for the source DB instance, replica errors occur</strong> - The max_allowed_packet parameter is a custom parameter that you can set in a DB parameter group. The max_allowed_packet parameter is used to specify the maximum size of data manipulation language (DML) that can be run on the database. If the max_allowed_packet value for the source DB instance is larger than the max_allowed_packet value for the read replica, the replication process can throw an error and stop replication.</p>\n<p>Diagnosing MySQL read replication failure:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q24-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html#CHAP_Troubleshooting.MySQL.ReplicaLag\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html#CHAP_Troubleshooting.MySQL.ReplicaLag</a></p>\n<p>Incorrect options:</p>\n<p><strong>To safely write to tables on  read replica, create indexes on the table after setting the read_only parameter to 0</strong> - As discussed above, writing to tables in read replica breaks the replication process. Setting the read_only paramater to 0 will not help.</p>\n<p><strong>Though read replicas can work on both transactional and nontransactional storage engines, nontransactional engines are error-prone because of the way memory is managed on these engines</strong> - Read replicas can only work on a transactional storage engine. Using a nontransactional storage engine such as MyISAM can break the replication process.</p>\n<p><strong>Statements containing non-deterministic functions like SYSDATE() should be predefined in the configuration to successfully create the read replica</strong> - This statement is incorrect. Using unsafe nondeterministic queries such as SLEEP(), SYSDATE(), SYSTEM_USER(), etc can break the replication. There is no option to predefine such functions in the configuration.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html#CHAP_Troubleshooting.MySQL.ReplicaLag\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Troubleshooting.html#CHAP_Troubleshooting.MySQL.ReplicaLag</a></p>\n<p><a href=\"https://dev.mysql.com/doc/refman/8.0/en/replication-rbr-safe-unsafe.html\">https://dev.mysql.com/doc/refman/8.0/en/replication-rbr-safe-unsafe.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "checkbox"}, {"question": "<p>A developer has created rules for different events on Amazon EventBridge with AWS Lambda function as a target. The developer has also created an IAM Role with the necessary permissions and associated it with the rule. The rule however is failing, and on initial analysis, it is clear that the IAM Role associated with the rule is not being used when calling the Lambda function.</p>\n<p>What could have gone wrong with the configuration and how can you fix the issue?</p>\n", "answers": ["For Lambda functions configured as a target to EventBridge, you need to provide resource-based policy. IAM Roles will not work", "The IAM Role is wrongly configured. Delete the existing Role and recreate with necessary permissions and associate the newly created Role with the EventBridge rule", "For Lambda, EventBridge relies on Access Control Lists (ACLs) to define permissions. IAM Roles will not work for Lambda when configured as a target to an EventBridge rule", "AWS Command Line Interface (CLI) should not be used to add permissions to EventBridge targets"], "correct_answer": "For Lambda functions configured as a target to EventBridge, you need to provide resource-based policy. IAM Roles will not work", "explanation": "<p>Correct option:</p>\n<p><strong>For Lambda functions configured as a target to EventBridge, you need to provide resource-based policy. IAM Roles will not work</strong> - IAM roles for rules are only used for events related to Kinesis Streams. For Lambda functions and Amazon SNS topics, you need to provide resource-based permissions.</p>\n<p>When a rule is triggered in EventBridge, all the targets associated with the rule are invoked. Invocation means invoking the AWS Lambda functions, publishing to the Amazon SNS topics, and relaying the event to the Kinesis streams. In order to be able to make API calls against the resources you own, EventBridge needs the appropriate permissions. For Lambda, Amazon SNS, Amazon SQS, and Amazon CloudWatch Logs resources, EventBridge relies on resource-based policies. For Kinesis streams, EventBridge relies on IAM roles.</p>\n<p>Incorrect options:</p>\n<p><strong>The IAM Role is wrongly configured. Delete the existing Role and recreate with necessary permissions and associate the newly created Role with the EventBridge rule</strong> - This option has been added as a distractor.</p>\n<p><strong>For Lambda, EventBridge relies on Access Control Lists (ACLs) to define permissions. IAM Roles will not work for Lambda when configured as  target to an EventBridge rule</strong> - Access Control Lists are not used with EventBridge and ACLs are defined at the account level and not at the individual user level.</p>\n<p><strong>AWS Command Line Interface (CLI) should not be used to add permissions to EventBridge targets</strong> - This statement is incorrect. AWS CLI can be used to add permissions to targets for EventBridge rules.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/resource-based-policies-eventbridge.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/resource-based-policies-eventbridge.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-troubleshooting.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-troubleshooting.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>A healthcare web application has been deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The application worked well in the development and test environments. In production, however, users are getting logged off and are being asked to log in several times in an hour.</p>\n<p>How will you fix this issue and what precaution needs to be taken to avoid recurrence of the issue?</p>\n", "answers": ["Enable Sticky Sessions on Application Load Balancer", "Use Slow Start Mode when registering the targets to ALB. This assures that the instances get enough time to warm up and hence will not lose the cached data", "Routing configuration of a Load Balancer is used to route traffic to targets. Use this configuration to set the protocol and port number to the correct one", "Enable logging on ALB and check the logs to see the error being generated"], "correct_answer": "Enable Sticky Sessions on Application Load Balancer", "explanation": "<p>Correct option:</p>\n<p><strong>Enable Sticky Sessions on Application Load Balancer</strong> - Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies.</p>\n<p>When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that encodes information about the selected target, encrypts the cookie, and includes the cookie in the response to the client. The client should include the cookie that it receives in subsequent requests to the load balancer. When the load balancer receives a request from a client that contains the cookie, if sticky sessions are enabled for the target group and the request goes to the same target group, the load balancer detects the cookie and routes the request to the same target. If the cookie is present but cannot be decoded, or if it refers to a target that was deregistered or is unhealthy, the load balancer selects a new target and updates the cookie with information about the new target.</p>\n<p>You enable sticky sessions at the target group level. You can also set the duration for the stickiness of the load balancer-generated cookie in seconds. The duration is set with each request. Therefore, if the client sends a request before each duration period expires, the sticky session continues.</p>\n<p>Incorrect options:</p>\n<p><strong>Use Slow Start Mode when registering the targets to ALB. This assures that the instances get enough time to warm up and hence will not lose the cached data</strong> - By default, a target starts to receive its full share of requests as soon as it is registered with a target group and passes an initial health check. Using slow start mode gives targets time to warm up before the load balancer sends them a full share of requests. However, this option cannot be used to address the given use-case.</p>\n<p><strong>Routing configuration of a Load Balancer is used to route traffic to targets. Use this configuration to set the protocol and port number to the correct one</strong> - By default, a load balancer routes requests to its targets using the protocol and port number that you specified when you created the target group. Alternatively, you can override the port used for routing traffic to a target when you register it with the target group. This, however, has nothing to do with users getting logged off every few minutes.</p>\n<p><strong>Enable logging on ALB and check the logs to see the error being generated</strong> - The current use case points at issues with session data and hence using sticky sessions is the right answer here.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions</a></p>\n", "section": "Domain 2: High Availability", "type": "radio"}, {"question": "<p>Your company has decided that certain users should have Multi-Factor Authentication (MFA) enabled for their sign-in credentials. A newly hired manager has a Gemalto MFA device that he used in his earlier company. He has approached you to configure it for his AWS account.</p>\n<p>How will you configure his existing Gemalto MFA device so he can seamlessly connect with AWS services in the new company?</p>\n", "answers": ["You can re-use an existing Gemalto device with AWS MFA, as Gemalto devices do not share any secrets between multiple parties", "AWS MFA relies on knowing a unique secret associated with your hardware MFA. This has to be generated again with AWS MFA for the Gemalto device to work with AWS", "Security constraints mandate that sharing of secrets between multiple parties can only happen in edge cases. Hence, formal approval is needed between AWS and the previous company to use the same Gemalto device", "AWS MFA does not support the use of your existing Gemalto device"], "correct_answer": "AWS MFA does not support the use of your existing Gemalto device", "explanation": "<p>Correct option:</p>\n<p><strong>AWS MFA does not support the use of your existing Gemalto device</strong> - AWS MFA relies on knowing a unique secret associated with your hardware MFA (Gemalto) device in order to support its use. Because of security constraints that mandate such secrets never be shared between multiple parties, AWS MFA cannot support the use of your existing Gemalto device. Only a compatible hardware MFA device purchased from Gemalto can be used with AWS MFA. You can re-use an existing U2F security key with AWS MFA, as U2F security keys do not share any secrets between multiple parties.</p>\n<p>Incorrect options:</p>\n<p><strong>You can re-use an existing Gemalto device with AWS MFA, as Gemalto devices do not share any secrets between multiple parties</strong> - As discussed above, you cannot re-use an existing Gemalto device with AWS MFA because secrets cannot be shared with multiple parties.</p>\n<p><strong>AWS MFA relies on knowing a unique secret associated with your hardware MFA. This has to be generated again with AWS MFA for the Gemalto device to work with AWS</strong> - As discussed above, an existing  Gemalto device cannot be used with AWS MFA.</p>\n<p><strong>Security constraints mandate that sharing of secrets between multiple parties can only happen in edge cases. Hence, formal approval is needed between AWS and the previous company to use the same Gemalto device</strong> - This is a made-up option, given only as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/iam/faqs/\">https://aws.amazon.com/iam/faqs/</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A data analytics company has its server infrastructure built on Amazon EC2 instances fronted with Elastic Load Balancers (ELBs). The ELBs are maintained in two AZs with each ELB having two EC2 instances registered with it. Both the instances in one AZ have been recorded as unhealthy.</p>\n<p>What is the status of traffic that flows to the ELB connected to unhealthy instances?</p>\n", "answers": ["HTTP 503: Service unavailable will be received as response", "The Load Balancer routes requests to the unhealthy targets", "The Load Balancer will display an `unhealthy' status and will not accept any incoming requests", "HTTP 403: Forbidden will be returned"], "correct_answer": "The Load Balancer routes requests to the unhealthy targets", "explanation": "<p>Correct option:</p>\n<p><strong>The Load Balancer routes requests to the unhealthy targets</strong> - If there is at least one healthy target in a target group, the load balancer routes requests only to the healthy targets. If a target group contains only unhealthy targets, the load balancer routes requests to the unhealthy targets. Hence, it is advised to configure an Auto Scaling Group, if the instances are hosting a business-critical application.</p>\n<p>Incorrect options:</p>\n<p><strong>HTTP 503: Service unavailable will be received as response</strong> - 503 error is returned if the target groups for the load balancer have no registered targets.</p>\n<p><strong>The Load Balancer will display an `unhealthy' status and will not accept any incoming requests</strong> - This is a made-up option, given only as a distractor.</p>\n<p><strong>HTTP 403: Forbidden will be returned</strong> - 403 error is returned if you configured an AWS WAF web access control list (web ACL) to monitor requests to your Application Load Balancer and it blocked the request.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html</a></p>\n", "section": "Domain 2: High Availability", "type": "radio"}, {"question": "<p>A highly critical financial services application is being moved to AWS Cloud from the on-premises data center. The application uses a fleet of Amazon EC2 instances provisioned in different geographical areas. The Chief Technology Officer (CTO) of the company needs to understand the communication network used between instances at various locations when they interact using public IP addresses.</p>\n<p>Which of the following options would you identify as correct? (Select two)</p>\n", "answers": ["Traffic between EC2 instances in different AWS Regions stays within the AWS network, if there is an Inter-Region VPC Peering connection between the VPCs where the two instances reside", "Traffic between two EC2 instances always stays within the AWS network, even when it goes over public IP addresses by using AWS Global Infrastructure", "Traffic between EC2 instances in different AWS Regions where there is no Inter-Region VPC Peering connection between the VPCs where these instances reside, will use edge locations to communicate without going over internet", "Traffic between two EC2 instances in the same AWS Region stays within the AWS network, even when it goes over public IP addresses", "Direct Connect is the default way of communication where there is no Inter-Region VPC Peering connection between the VPCs. All traffic between instances will use Direct Connect and does not go over internet"], "correct_answer": ["Traffic between EC2 instances in different AWS Regions stays within the AWS network, if there is an Inter-Region VPC Peering connection between the VPCs where the two instances reside", "Traffic between two EC2 instances in the same AWS Region stays within the AWS network, even when it goes over public IP addresses"], "explanation": "<p>Correct option:</p>\n<p><strong>Traffic between EC2 instances in different AWS Regions stays within the AWS network, if there is an Inter-Region VPC Peering connection between the VPCs where the two instances reside</strong></p>\n<p><strong>Traffic between two EC2 instances in the same AWS Region stays within the AWS network, even when it goes over public IP addresses</strong></p>\n<p>When two instances communicate using public IP addresses, the following three scenarios are possible:\n1. Traffic between two EC2 instances in the same AWS Region stays within the AWS network, even when it goes over public IP addresses.</p>\n<ol>\n<li><p>Traffic between EC2 instances in different AWS Regions stays within the AWS network if there is an Inter-Region VPC Peering connection between the VPCs where the two instances reside.</p></li>\n<li><p>Traffic between EC2 instances in different AWS Regions where there is no Inter-Region VPC Peering connection between the VPCs where these instances reside, is not guaranteed to stay within the AWS network.</p></li>\n</ol>\n<p>Incorrect options:</p>\n<p><strong>Traffic between two EC2 instances always stays within the AWS network, even when it goes over public IP addresses by using AWS Global Infrastructure</strong></p>\n<p><strong>Traffic between EC2 instances in different AWS Regions where there is no Inter-Region VPC Peering connection between the VPCs where these instances reside will use edge locations to communicate without going over the internet</strong></p>\n<p>These two options contradict the explanation provided above, so both options are incorrect.</p>\n<p><strong>Direct Connect is the default way of communication where there is no Inter-Region VPC Peering connection between the VPCs. All traffic between instances will use Direct Connect and does not go over the internet</strong> - AWS Direct Connect is a network service that provides an alternative to using the Internet to utilize AWS cloud services. AWS Direct Connect enables customers to have low latency and private connections to AWS for workloads that require higher speed or lower latency than the internet. Direct Connect is a paid service and is available only if the customer opts for it.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/vpc/faqs/\">https://aws.amazon.com/vpc/faqs/</a></p>\n", "section": "Domain 6: Networking", "type": "checkbox"}, {"question": "<p>A personal care web application is hosted on Amazon EC2 instance in two different Availability Zones (AZs). The application uses Internet Protocol version 6 (IPv6) for communication. The EC2 instances are placed in private subnets. The instances need Internet access to download software updates twice a month.</p>\n<p>Which configuration will help achieve this requirement without exposing the instances to the outside world?</p>\n", "answers": ["Configure a Carrier gateway, that allows outbound communication over IPv6 from instances in your VPC to the internet", "Configure an Internet Gateway to allow outbound communication on IPv6. Associate the IPv6 address to an Elastic IP address to make it public", "Configure Egress-only Internet Gateway, that allows outbound communication over IPv6 from instances in your VPC to the internet", "Configure an Internet Gateway to allow outbound communication on IPv6 for the instances in the private subnet for your VPC. Public subnets are by default connected to the internet and do not need any extra configuration"], "correct_answer": "Configure Egress-only Internet Gateway, that allows outbound communication over IPv6 from instances in your VPC to the internet", "explanation": "<p>Correct option:</p>\n<p><strong>Configure Egress-only Internet Gateway, that allows outbound communication over IPv6 from instances in your VPC to the internet</strong> - An egress-only internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the internet, and prevents the internet from initiating an IPv6 connection with your instances.</p>\n<p>An egress-only internet gateway is for use with IPv6 traffic only. To enable outbound-only internet communication over IPv4, use a NAT gateway instead.</p>\n<p>An example Egress-only Internet Gateway architecture:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q30-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Configure a Carrier gateway, that allows outbound communication over IPv6 from instances in your VPC to the internet</strong> - Carrier gateways are only available for VPCs that contain subnets in a Wavelength Zone. The carrier gateway provides connectivity between your Wavelength Zone and the telecommunication carrier, and devices on the telecommunication carrier network. A carrier gateway supports only IPv4 traffic.</p>\n<p><strong>Configure an Internet Gateway to allow outbound communication on IPv6. Associate the IPv6 address to an Elastic IP address to make it public</strong> - IPv6 addresses are globally unique and are therefore public by default. You do not associate them with an Elastic IP address.</p>\n<p><strong>Configure an Internet Gateway to allow outbound communication on IPv6 for the instances in the private subnet for your VPC. Public subnets are by default connected to the internet and do not need any extra configuration</strong> - Internet Gateway is configured to public subnets of a VPC and not to a private subnet directly.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/Carrier_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/Carrier_Gateway.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>An IT services company runs its technology infrastructure on AWS Cloud. The company runs audits for all the development and testing teams against the standards set by the organization. During a recent audit, the company realized that most of the patch compliance standards are not being followed by the teams. The teams have however tagged all their AWS resources as per the guidelines.</p>\n<p>As a SysOps Administrator, which of the following would you recommend as an easy way of fixing the issue as quickly as possible?</p>\n", "answers": ["Use Amazon Inspector to automate the process of patching instances that helps improve the security and compliance of the instances", "Use Amazon Patch Manager to automate the process of patching instances", "Use AWS Systems Manager Automation to simplify the patch application process across all instances", "Use AWS Systems Manager Patch Manager to automate the process of patching managed instances"], "correct_answer": "Use AWS Systems Manager Patch Manager to automate the process of patching managed instances", "explanation": "<p>Correct option:</p>\n<p><strong>Use AWS Systems Manager Patch Manager to automate the process of patching managed instances</strong></p>\n<p>AWS Systems Manager Patch Manager automates the process of patching managed instances with both security related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. You can use Patch Manager to install Service Packs on Windows instances and perform minor version upgrades on Linux instances. You can patch fleets of EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type.</p>\n<p>Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. (Tags are keys that help identify and sort your resources within your organization.) You can add tags to your patch baselines themselves when you create or update them.</p>\n<p>Patch Manager provides options to scan your instances and report compliance on a schedule, install available patches on a schedule, and patch or scan instances on demand whenever you need to.</p>\n<p>Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon EventBridge to provide a secure patching experience that includes event notifications and the ability to audit usage.</p>\n<p>Incorrect options:</p>\n<p><strong>Use Amazon Inspector to automate the process of patching instances that helps improve the security and compliance of the instances</strong> - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. Inspector is not a patch management service.</p>\n<p><strong>Use Amazon Patch Manager to automate the process of patching instances</strong> - This is a made-up option and given only as a distractor.</p>\n<p><strong>Use AWS Systems Manager Automation to simplify the patch application process across all instances</strong> - Systems Manager Automation simplifies common maintenance and deployment tasks of EC2 instances and other AWS resources. Automation enables you to do the following: Build Automation workflows to configure and manage instances and AWS resources, Create custom workflows or use pre-defined workflows maintained by AWS, Receive notifications about Automation tasks and workflows by using Amazon EventBridge, Monitor Automation progress and execution details by using the Amazon EC2 or the AWS Systems Manager console. Systems Manager Automation, however, does not include patch management.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p>\n<p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>A retail company is working on moving their technology infrastructure to AWS Cloud. The company has developed several custom scripts to monitor the instances hosting their applications and want to reuse these scripts on AWS Cloud. The development team is looking at a way to disable the pre-existing Amazon EC2 status checks.</p>\n<p>As a SysOps Administrator, which of the following will you suggest to meet the given requirement?</p>\n", "answers": ["Status checks are built into Amazon EC2, so they cannot be disabled, but can be deleted from active configuration", "Amazon EC2 automated checks to identify hardware issues cannot be disabled. Automated checks for software issues can however be disabled", "Amazon EC2 status checks are interwoven into CloudWatch metrics. You can disable EC2 instance status checks from the CloudWatch metrics console", "Status checks are built into Amazon EC2, so they cannot be disabled or deleted"], "correct_answer": "Status checks are built into Amazon EC2, so they cannot be disabled or deleted", "explanation": "<p>Correct option:</p>\n<p><strong>Status checks are built into Amazon EC2, so they cannot be disabled or deleted</strong></p>\n<p>With instance status monitoring, you can quickly determine whether Amazon EC2 has detected any problems that might prevent your instances from running applications. Amazon EC2 performs automated checks on every running EC2 instance to identify hardware and software issues. You can view the results of these status checks to identify specific and detectable problems.</p>\n<p>Status checks are performed every minute, returning a pass or a fail status. If all checks pass, the overall status of the instance is OK. If one or more checks fail, the overall status is impaired. Status checks are built into Amazon EC2, so they cannot be disabled or deleted.</p>\n<p>When a status check fails, the corresponding CloudWatch metric for status checks is incremented. You can use these metrics to create CloudWatch alarms that are triggered based on the result of the status checks. For example, you can create an alarm to warn you if status checks fail on a specific instance. You can also create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically recovers the instance if it becomes impaired due to an underlying issue.</p>\n<p>Incorrect options:</p>\n<p><strong>Status checks are built into Amazon EC2, so they cannot be disabled, but can be deleted from active configuration</strong></p>\n<p><strong>Amazon EC2 automated checks to identify hardware issues cannot be disabled. Automated checks for software issues can however be disabled</strong></p>\n<p><strong>Amazon EC2 status checks are interwoven into CloudWatch metrics. You can disable EC2 instance status checks from the CloudWatch metrics console</strong></p>\n<p>These three options contradict the explanation given above, so these options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>An e-commerce company relies heavily on the AWS Systems Manager for automating various management tasks for the fleet of Amazon EC2 instances that host their applications.</p>\n<p>Which of the following should you use to recover an impaired instance automatically?</p>\n", "answers": ["Use the AWSSupport-ExecuteEC2Rescue document to recover impaired instances", "Use the AWS-UpdateCloudFormationStackWithApproval document to update impaired instances", "Use the AWS-UpdateWindowsAmi document to recover impaired instances", "Automatic recovery of impaired instances is not possible currently"], "correct_answer": "Use the AWSSupport-ExecuteEC2Rescue document to recover impaired instances", "explanation": "<p>Correct option:</p>\n<p><strong>Use the <code>AWSSupport-ExecuteEC2Rescue</code> document to recover impaired instances</strong></p>\n<p>A Systems Manager Automation document defines the Automation workflow (the actions that Systems Manager performs on your managed instances and AWS resources). Automation includes several pre-defined Automation documents that you can use to perform common tasks like restarting one or more EC2 instances or creating an Amazon Machine Image (AMI).</p>\n<p>Use the <code>AWSSupport-ExecuteEC2Rescue</code> document to recover impaired instances. An instance can become unreachable for a variety of reasons, including network misconfigurations, RDP issues, or firewall settings. Troubleshooting and regaining access to the instance previously required dozens of manual steps before you could regain access. The AWSSupport-ExecuteEC2Rescue document lets you regain access by specifying an instance ID and clicking a button.</p>\n<p>Incorrect options:</p>\n<p><strong>Use the <code>AWS-UpdateCloudFormationStackWithApproval</code> document to update impaired instances</strong> - The <code>AWS-UpdateCloudFormationStackWithApproval</code> document is used to update resources that were deployed by using CloudFormation template.</p>\n<p><strong>Use the <code>AWS-UpdateWindowsAmi</code> document to recover impaired instances</strong> - You use the <code>AWS-UpdateLinuxAmi</code> and <code>AWS-UpdateWindowsAmi</code> documents to create golden AMIs from a source AMI.</p>\n<p><strong>Automatic recovery of impaired instances is not possible currently</strong> - This is an incorrect statement, added only as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>A retail company stores its business critical files on an Amazon S3 bucket that is also configured as a website endpoint. The company needs a robust configuration that will allow access only through CloudFront. No user or team member should be able to access the files directly from Amazon S3 URL.</p>\n<p>As a SysOps Administrator, which of the following would you suggest to address this requirement?</p>\n", "answers": ["Create an Origin Access Identity (OAI) and configure S3 bucket permissions so that CloudFront can use the OAI to access the files in your bucket", "Configure a Security Group with CloudFront to restrict access to users", "Configure a Network Access Control List (ACL) with CloudFront to restrict access to users", "Setup the Amazon S3 bucket as a custom origin with CloudFront. Restrict the access to content by setting up custom headers"], "correct_answer": "Setup the Amazon S3 bucket as a custom origin with CloudFront. Restrict the access to content by setting up custom headers", "explanation": "<p>Correct option:</p>\n<p><strong>Setup the Amazon S3 bucket as a custom origin with CloudFront. Restrict the access to content by setting up custom headers</strong></p>\n<p>If you use an Amazon S3 bucket configured as a website endpoint, you must set it up with CloudFront as a custom origin. You can’t use the origin access identity feature. However, you can restrict access to content on a custom origin by setting up custom headers and configuring your origin to require them.</p>\n<p>To require that users access content through CloudFront, change the following settings in your CloudFront distributions:\n1. Origin Custom Headers : Configure CloudFront to forward custom headers to your origin.\n2. Viewer Protocol Policy : Configure your distribution to require viewers to use HTTPS to access CloudFront.\n3. Origin Protocol Policy : Configure your distribution to require CloudFront to use the same protocol as viewers to forward requests to the origin.</p>\n<p>After you've made these changes, update your application on your custom origin to only accept requests that include the custom headers that you’ve configured CloudFront to send.</p>\n<p>More info on restricting access to files on custom origins:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q34-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html#forward-custom-headers-restrict-access\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html#forward-custom-headers-restrict-access</a></p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q34-i2.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Create an Origin Access Identity (OAI) and configure S3 bucket permissions so that CloudFront can use the OAI to access the files in your bucket</strong> - As explained above, if you use an Amazon S3 bucket configured as a website endpoint, you can’t use the origin access identity feature.</p>\n<p><strong>Configure a Security Group with CloudFront to restrict access to users</strong> - A Security Group acts as a virtual firewall for Amazon EC2 instances to control incoming and outgoing traffic. Security Groups cannot be used with CloudFront.</p>\n<p><strong>Configure a Network Access Control List (ACL) with CloudFront to restrict access to users</strong> - A Network Access Control List (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. Network ACLs are not used with CloudFront.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html#forward-custom-headers-restrict-access\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html#forward-custom-headers-restrict-access</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html#concept_S3Origin_website\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3AndCustomOrigins.html#concept_S3Origin_website</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>As a SysOps Administrator, you create and maintain various system configurations for the teams you work with. You have created a CloudFront distribution with origin as an Amazon S3 bucket. The configuration has worked fine so far. However, for a few hours now, an error similar to this has cropped up - <code>The authorization header is malformed; the region '&lt;AWS Region&gt;' is wrong; expecting '&lt;AWS Region&gt;'</code>.</p>\n<p>What is the reason for this error and how will you fix it?</p>\n", "answers": ["This error indicates that the CloudFront distribution and Amazon S3 are not in the same AWS Region. Move one resource so that, both the CloudFront distribution and Amazon S3 are in the same AWS Region", "This error indicates that the API key used for authorization is from an AWS Region that is different from the Region that S3 bucket is created in", "This error indicates the configured Amazon S3 bucket has been moved from one AWS Region to the other. That is, deleted from one AWS Region and created with the same name in another. To fix this error, update your CloudFront distribution so that it finds the S3 bucket in the bucket's current AWS Region", "This error indicates that when CloudFront forwarded a request to the origin, the origin didn’t respond before the request expired. This could be an access issue caused by a firewall or a Security Group not allowing access to CloudFront to access S3 resources"], "correct_answer": "This error indicates the configured Amazon S3 bucket has been moved from one AWS Region to the other. That is, deleted from one AWS Region and created with the same name in another. To fix this error, update your CloudFront distribution so that it finds the S3 bucket in the bucket's current AWS Region", "explanation": "<p>Correct option:</p>\n<p><strong>This error indicates the configured Amazon S3 bucket has been moved from one AWS Region to the other. That is, deleted from one AWS Region and created with the same name in another. To fix this error, update your CloudFront distribution so that it finds the S3 bucket in the bucket's current AWS Region</strong> - If CloudFront requests an object from your origin, and the origin returns an HTTP 4xx or 5xx status code, there's a problem with communication between CloudFront and your origin.</p>\n<p>Your CloudFront distribution might send error responses with HTTP status code 400 Bad Request, and a message similar to the following: <code>The authorization header is malformed; the region '&lt;AWS Region&gt;' is wrong; expecting '&lt;AWS Region&gt;'</code>.</p>\n<p>This problem can occur in the following scenario: 1)Your CloudFront distribution's origin is an Amazon S3 bucket, 2)You moved the S3 bucket from one AWS Region to another. That is, you deleted the S3 bucket, then later you created a new bucket with the same bucket name, but in a different AWS Region than where the original S3 bucket was located.</p>\n<p>To fix this error, update your CloudFront distribution so that it finds the S3 bucket in the bucket's current AWS Region.</p>\n<p>Incorrect options:</p>\n<p><strong>This error indicates that the CloudFront distribution and Amazon S3 are not in the same AWS Region. Move one resource so that, both the CloudFront distribution and Amazon S3 are in the same AWS Region</strong> - Amazon CloudFront uses a global network of edge locations and regional edge caches for content delivery. You can configure CloudFront to server content from particular Regions, but CloudFront is not Region specific.</p>\n<p><strong>This error indicates that the API key used for authorization is from an AWS Region that is different from the Region that S3 bucket is created in</strong> - This is a made-up option, given only as a distractor.</p>\n<p><strong>This error indicates that when CloudFront forwarded a request to the origin, the origin didn’t respond before the request expired. This could be an access issue caused by a firewall or a Security Group not allowing access to CloudFront to access S3 resources</strong> - When CloudFront forwards a request to the origin, and the origin didn’t respond before the request expired, a Gateway Timeout error is generated.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/http-400-bad-request.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/http-400-bad-request.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>An application runs on a fleet of Amazon EC2 instances running behind an Application Load Balancer. An Auto Scaling Group (ASG) helps keep the application available and flexible to traffic changes. The EC2 instances need to connect to Amazon RDS instances for fetching data. EC2 Instances also need internet access to be able to download the patches needed for their software. To meet the security guidelines of the company - the Load Balancer, Auto Scaling Group with the EC2 instances and RDS - are all placed into different subnets of the VPC.</p>\n<p>Which of the following represents the best configuration to help connect the EC2 instances to internet?</p>\n", "answers": ["Create and attach an Egress-only Internet Gateway to the VPC and then update the route table of the instance subnet to route internet traffic via the Egress-only Internet Gateway", "Configure an Elastic network interface for all the instances that need to communicate with the internet. Attach this Elastic network interface to the public subnet of the VPC to route internet traffic", "Create and attach an Internet Gateway to the VPC. Update the route table of the subnet that hosts the EC2 instances, to route internet traffic via the Internet Gateway", "Create a carrier gateway and attach the carrier gateway to your VPC. You can then connect the subnets you wish to route to the carrier gateway"], "correct_answer": "Create and attach an Internet Gateway to the VPC. Update the route table of the subnet that hosts the EC2 instances, to route internet traffic via the Internet Gateway", "explanation": "<p>Correct option:</p>\n<p><strong>Create and attach an Internet Gateway to the VPC. Update the route table of the subnet that hosts the EC2 instances, to route internet traffic via the Internet Gateway</strong></p>\n<p>An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet.</p>\n<p>An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. An internet gateway supports IPv4 and IPv6 traffic. It does not cause availability risks or bandwidth constraints on your network traffic. There's no additional charge for having an internet gateway in your account.</p>\n<p>Reference diagram to configure an Internet Gateway on a VPC:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q36-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Create and attach an Egress-only Internet Gateway to the VPC and then update the route table of the instance subnet to route internet traffic via the Egress-only Internet Gateway</strong> - An Egress-only internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the internet, and prevents the internet from initiating an IPv6 connection with your instances. An egress-only internet gateway is for use with IPv6 traffic only.</p>\n<p><strong>Configure an Elastic network interface for all the instances that need to communicate with the internet. Attach this Elastic network interface to the public subnet of the VPC to route internet traffic</strong> - An elastic network interface is a virtual network interface that can include the following attributes: a primary private IPv4 address, one or more secondary private IPv4 addresses,one Elastic IP address per private IPv4 address, one public IPv4 address, which can be auto-assigned to the network interface for eth0 when you launch an instance, one or more IPv6 addresses, one or more security groups, a MAC address\n, a source/destination check flag, a description.</p>\n<p>You can create a network interface, attach it to an instance, detach it from an instance, and attach it to another instance. When you move a network interface from one instance to another, network traffic is redirected to the new instance.</p>\n<p>This option is a distractor, as there is no such thing as attaching a Elastic network interface to the public subnet of the VPC.</p>\n<p><strong>Create a carrier gateway and attach the carrier gateway to your VPC. You can then connect the subnets you wish to route to the carrier gateway</strong> - A carrier gateway serves two purposes. It allows inbound traffic from a carrier network in a specific location, and it allows outbound traffic to the carrier network and the internet. There is no inbound connection configuration from the internet to a Wavelength Zone through the carrier gateway. Carrier gateways are only available for VPCs that contain subnets in a Wavelength Zone.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/Carrier_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/Carrier_Gateway.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>As part of the ongoing system maintenance, a SysOps Administrator has decided to increase the storage capacity of an EBS volume that is attached to an Amazon EC2 instance. However, the increased size is not reflected in the file system.</p>\n<p>What has gone wrong in the configuration and how can it be fixed?</p>\n", "answers": ["EBS volume needs to be detached and attached back again to the instance for the modifications to show", "EBS volume might be encrypted. Encrypted EBS volumes will not show modifications done when still attached to the instance. Detach the EBS volume and attach it back", "Linux servers automatically pick the modifications done to EBS volumes, but Windows servers do not offer this feature. Use the Windows Disk Management utility to increase the disk size to the new modified volume size", "After you increase the size of an EBS volume, you must extend the file system to a larger size"], "correct_answer": "After you increase the size of an EBS volume, you must extend the file system to a larger size", "explanation": "<p>Correct option:</p>\n<p><strong>After you increase the size of an EBS volume, you must extend the file system to a larger size</strong> - After you increase the size of an EBS volume, you must use the file system specific commands to extend the file system to the larger size. You can resize the file system as soon as the volume enters the optimizing state.</p>\n<p>The process for extending a file system on Linux is as follows:</p>\n<ol>\n<li><p>Your EBS volume might have a partition that contains the file system and data. Increasing the size of a volume does not increase the size of the partition. Before you extend the file system on a resized volume, check whether the volume has a partition that must be extended to the new size of the volume.</p></li>\n<li><p>Use a file system-specific command to resize each file system to the new volume capacity.</p></li>\n</ol>\n<p>Incorrect options:</p>\n<p><strong>EBS volume needs to be detached and attached back again to the instance for the modifications to show</strong> - This is incorrect and has been added as a distractor.</p>\n<p><strong>EBS volume might be encrypted. Encrypted EBS volumes will not show modifications done when still attached to the instance. Detach the EBS volume and attach it back</strong> - EBS volume encryption has no bearing on the given scenario.</p>\n<p><strong>Linux servers automatically pick the modifications done to EBS volumes, but Windows servers do not offer this feature. Use the Windows Disk Management utility to increase the disk size to the new modified volume size</strong> - As discussed above, You need to manually extend the size of the file system after increasing the size of EBS volume.</p>\n<p>On Windows file system, after you increase the size of an EBS volume, use the Windows Disk Management utility or PowerShell to extend the disk size to the new size of the volume. You can begin resizing the file system as soon as the volume enters the optimizing state.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/recognize-expanded-volume-windows.html\">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/recognize-expanded-volume-windows.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>An hour after launching an important feature on its website, an analytics company has realized that an important page has issues that need to be addressed. The web application is hosted on an Amazon EC2 instance with CloudFront being used to reduce latency for the users. Few users have already accessed this page and the company wants to pull it down as soon as possible.</p>\n<p>What should the company do to quickly remove the file from the CloudFront distribution?</p>\n", "answers": ["Invalidate the file from CloudFront distribution so that the file is removed immediately", "Specify a default root object to show only this object and not the faulty web page", "By default, CloudFront caches files in edge locations for 24 hours. So, it's not possible to remove the file before this time", "Use CloudFront policies to control what the users can see"], "correct_answer": "Invalidate the file from CloudFront distribution so that the file is removed immediately", "explanation": "<p>Correct option:</p>\n<p><strong>Invalidate the file from CloudFront distribution so that the file is removed immediately</strong></p>\n<p>If you need to remove a file from CloudFront edge caches before it expires, you can do one of the following:</p>\n<ol>\n<li><p>Invalidate the file from edge caches. The next time a viewer requests the file, CloudFront returns to the origin to fetch the latest version of the file.</p></li>\n<li><p>Use file versioning to serve a different version of the file that has a different name.</p></li>\n</ol>\n<p>You can use the CloudFront console to create and run an invalidation, display a list of the invalidations that you submitted previously, and display detailed information about an individual invalidation. You can also copy an existing invalidation, edit the list of file paths, and run the edited invalidation. You can't remove invalidations from the list.</p>\n<p>When you submit an invalidation request to CloudFront, CloudFront forwards the request to all edge locations within a few seconds, and each edge location starts processing the invalidation immediately. As a result, you can’t cancel an invalidation after you submit it.</p>\n<p>Incorrect options:</p>\n<p><strong>Specify a default root object to show only this object and not the faulty web page</strong> - You can configure CloudFront to return a specific object (the default root object) when a user requests the root URL for your web distribution instead of requesting an object in your distribution. Specifying a default root object lets you avoid exposing the contents of your distribution. Although a useful feature, it is not helpful for the given use-case.</p>\n<p><strong>By default, CloudFront caches files in edge locations for 24 hours. So, it's not possible to remove the file before this time</strong> - It is true that CloudFront caches files in edge locations for 24 hours. But, it's possible to remove them by invalidating the files or versioning the files.</p>\n<p><strong>Use CloudFront policies to control what the users can see</strong> - With CloudFront policies, you can control the values that are included in the cache key for objects that are cached at CloudFront edge locations. These values can include HTTP request query strings, headers, and cookies.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/working-with-policies.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/working-with-policies.html</a></p>\n", "section": "DDomain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>A startup has reserved On-Demand Capacity Reservations for the Amazon EC2 instances they use for running analytics. Once the billing report was generated, the company was surprised to see that the costs were much higher than expected. The startup has hired you as a SysOps Administrator to bridge this knowledge gap.</p>\n<p>Can you identify the important points to remember when considering On-Demand Capacity Reservations? (Select two)</p>\n", "answers": ["On-Demand Capacity Reservations enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration", "On-Demand Capacity Reservations require a fixed one-year or three-year commitment", "Capacity Reservations can be used with Dedicated Hosts, however, they can't be used with placement groups", "Capacity Reservations are transferable from one AWS account to another", "Capacity Reservations do not offer any billing discounts"], "correct_answer": ["On-Demand Capacity Reservations enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration", "Capacity Reservations do not offer any billing discounts"], "explanation": "<p>Correct options:</p>\n<p><strong>On-Demand Capacity Reservations enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration</strong> - On-Demand Capacity Reservations enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration. This gives you the ability to create and manage Capacity Reservations independently from the billing discounts offered by Savings Plans or regional Reserved Instances.</p>\n<p><strong>Capacity Reservations do not offer any billing discounts</strong> - Capacity Reservations do not offer any billing discounts. You can combine Capacity Reservations with Savings Plans or Regional Reserved Instances to receive a discount.</p>\n<p>Incorrect options:</p>\n<p><strong>On-Demand Capacity Reservations require a fixed one-year or three-year commitment</strong> - No commitment is required for On-Demand Capacity Reservations. They can be created and canceled as needed.</p>\n<p><strong>Capacity Reservations can be used with Dedicated Hosts, however, they can't be used with placement groups</strong> - Capacity Reservations can be used with neither placement groups nor Dedicated Hosts.</p>\n<p><strong>Capacity Reservations are transferable from one AWS account to another</strong> - Capacity Reservations are not transferable from one AWS account to another. However, you can share Capacity Reservations with other AWS accounts.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html#capacity-reservations-differences\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html#capacity-reservations-differences</a></p>\n", "section": "DDomain 3: Deployment and Provisioning", "type": "checkbox"}, {"question": "<p>A junior developer created multiple stacks of resources in different AWS Regions per the CloudFormation template given to him. The development team soon started having issues with the created resources and their behavior. Initial checks have confirmed that some resources were created and some omitted, though the same template has been used. As a SysOps Administrator, you have been tasked to resolve these issues.</p>\n<p>Which of the following could be the possible reason for this unexpected behavior?</p>\n", "answers": ["There might have been dependency errors, that resulted in stack not being created completely", "Insufficient IAM permissions can lead to issues. When you work with an AWS CloudFormation stack, you not only need permissions to use AWS CloudFormation, you must also have permission to use the underlying services that are described in your template", "The CloudFormation template might have custom named IAM resources that are responsible for the unintended behavior", "The CloudFormation template was created using use-once only option and is not supposed to be reused for creating other stacks"], "correct_answer": "The CloudFormation template might have custom named IAM resources that are responsible for the unintended behavior", "explanation": "<p>Correct option:</p>\n<p><strong>The CloudFormation template might have custom named IAM resources that are responsible for the unintended behavior</strong> - If your template contains custom named IAM resources, don't create multiple stacks reusing the same template. IAM resources must be globally unique within your account. If you use the same template to create multiple stacks in different Regions, your stacks might share the same IAM resources, instead of each having a unique one. Shared resources among stacks can have unintended consequences from which you can't recover. For example, if you delete or update shared IAM resources in one stack, you will unintentionally modify the resources of other stacks.</p>\n<p>Incorrect options:</p>\n<p><strong>There might have been dependency errors, that resulted in stack not being created completely</strong> - Any error during stack creation will rollback the entire stack creation process and the result is, none of the mentioned resources are created.</p>\n<p><strong>Insufficient IAM permissions can lead to issues. When you work with an AWS CloudFormation stack, you not only need permissions to use AWS CloudFormation, you must also have permission to use the underlying services that are described in your template</strong> - If permissions were an issue, the stack wouldn't be created at all.</p>\n<p><strong>The CloudFormation template was created using <code>use-once only</code> option and is not supposed to be reused for creating other stacks</strong> - This is a made-up option and given only as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>Multiple teams of an e-commerce company use the same AWS CloudFormation template to create stacks of resources needed by them. For the next deployment, the teams need to update the stacks and have been testing the changes through change sets. However, the teams suddenly realized that all their change sets have been lost. Unable to figure out the error they have approached you.</p>\n<p>As a SysOps Administrator, how will you identify the error and suggest a way to fix the issue?</p>\n", "answers": ["An invalid change set was executed and this resulted in all stacks and change sets getting deleted", "The change set while being validated, surpassed the account limit of some AWS resource. Since the stacks cannot be updated when the account limit is reached, the change sets have been deleted by CloudFormation", "CloudFormation had issued a rollback on the change sets while validating them and deleted all the invalid sets", "A change set was successfully executed and this resulted in rest of the change sets being deleted by CloudFormation"], "correct_answer": "A change set was successfully executed and this resulted in rest of the change sets being deleted by CloudFormation", "explanation": "<p>Correct option:</p>\n<p><strong>A change set was successfully executed and this resulted in rest of the change sets being deleted by CloudFormation</strong></p>\n<p>Change sets allow you to preview how proposed changes to a stack might impact your existing resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. You can create and manage change sets using the AWS CloudFormation console, AWS CLI, or AWS CloudFormation API.</p>\n<p>After you execute a change, AWS CloudFormation removes all change sets that are associated with the stack because they aren't applicable to the updated stack.</p>\n<p>How to use change sets to update a stack:\n<img src=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/images/update-stack-changesets-diagram.png\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>An invalid change set was executed and this resulted in all stacks and change sets getting deleted</strong> - This option has been added as a distractor. An invalid change set won't result in any resource changes as it won't go through for provisioning.</p>\n<p><strong>The change set while being validated, surpassed the account limit of some AWS resource. Since the stacks cannot be updated when the account limit is reached, the change sets have been deleted by CloudFormation</strong> - Change sets don't indicate whether AWS CloudFormation will successfully update a stack. For example, a change set doesn't check if you will surpass an account limit, if you're updating a resource that doesn't support updates, or if you have insufficient permissions to modify a resource, all of which can cause a stack update to fail. If an update fails, AWS CloudFormation attempts to roll back your resources to their original state.</p>\n<p><strong>CloudFormation had issued a rollback on the change sets while validating them and deleted all the invalid sets</strong> - There is no rollback for change sets, since there is no real change. When they are applied on a stack and stack fails, the stack is rolled back to its previous state.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>An automobile company uses a hybrid environment to run its technology infrastructure using a mix of on-premises instances and AWS Cloud. The company has a few managed instances in Amazon VPC. The company wants to avoid using internet for accessing AWS Systems Manager APIs from this VPC.</p>\n<p>As a Systems Administrator, which of the following would you recommend to address this requirement?</p>\n", "answers": ["You can privately access AWS Systems Manager APIs from Amazon VPC by creating Internet Gateway", "You can privately access AWS Systems Manager APIs from Amazon VPC by creating VPC Endpoint", "You can privately access AWS Systems Manager APIs from Amazon VPC by creating NAT gateway", "You can privately access AWS Systems Manager APIs from Amazon VPC by creating VPN connection"], "correct_answer": "You can privately access AWS Systems Manager APIs from Amazon VPC by creating VPC Endpoint", "explanation": "<p>Correct option:</p>\n<p><strong>You can privately access AWS Systems Manager APIs from Amazon VPC by creating VPC Endpoint</strong> - A managed instance is any machine configured for AWS Systems Manager. You can configure EC2 instances or on-premises machines in a hybrid environment as managed instances.</p>\n<p>You can improve the security posture of your managed instances (including managed instances in your hybrid environment) by configuring AWS Systems Manager to use an interface VPC endpoint in Amazon Virtual Private Cloud (Amazon VPC). An interface VPC endpoint (interface endpoint) enables you to connect to services powered by AWS PrivateLink, a technology that enables you to privately access Amazon EC2 and Systems Manager APIs by using private IP addresses. PrivateLink restricts all network traffic between your managed instances, Systems Manager, and Amazon EC2 to the Amazon network. This means that your managed instances don't have access to the Internet. If you use PrivateLink, you don't need an Internet gateway, a NAT device, or a virtual private gateway.</p>\n<p>How to use AWS PrivateLink:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q42-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/how-it-works.html#what-is-privatelink\">https://docs.aws.amazon.com/vpc/latest/userguide/how-it-works.html#what-is-privatelink</a></p>\n<p>Incorrect options:</p>\n<p><strong>You can privately access AWS Systems Manager APIs from Amazon VPC by creating Internet Gateway</strong></p>\n<p>An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. Internet Gateways must be deployed in a public subnet and the corresponding entry should be added in the route table.</p>\n<p><strong>You can privately access AWS Systems Manager APIs from Amazon VPC by creating NAT gateway</strong></p>\n<p>You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances.</p>\n<p><strong>You can privately access AWS Systems Manager APIs from Amazon VPC by creating VPN connection</strong></p>\n<p>By default, instances that you launch into an Amazon VPC can't communicate with your own (remote) network. You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection.</p>\n<p>These three options contradict the explanation above, so these options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>A small financial services startup uses Amazon EC2 instance for their server infrastructure and Amazon S3 for storage. The files stored on S3 are critical for the business and the company wants to track access to the buckets for audit and security purposes. The startup is looking at a cost effective way of doing this without incurring extra costs.</p>\n<p>As a Systems Administrator, which of the following would you recommend to address this use-case?</p>\n", "answers": ["Use AWS X-Ray for tracing Amazon S3 requests from end-to-end", "Use Amazon Inspector, a security assessment service that helps track the calls made to AWS services configured with it", "Enable Amazon S3 Server Access Logging for all the buckets that the company deems important and store these logs in another S3 bucket for analysis", "Use AWS CloudTrail to identify the requests made to the Amazon S3 buckets"], "correct_answer": "Enable Amazon S3 Server Access Logging for all the buckets that the company deems important and store these logs in another S3 bucket for analysis", "explanation": "<p>Correct option:</p>\n<p><strong>Enable Amazon S3 Server Access Logging for all the buckets that the company deems important and store these logs in another S3 bucket for analysis</strong> - To track requests for access to your bucket, you can enable server access logging. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and an error code, if relevant.</p>\n<p>There is no extra charge for enabling server access logging on an Amazon S3 bucket, and you are not charged when the logs are PUT to your bucket. However, any log files that the system delivers to your bucket accrue the usual charges for storage. You can delete these log files at any time. Subsequent reads and other requests to these log files are charged normally, as for any other object, including data transfer charges.</p>\n<p>By default, logging is disabled. When logging is enabled, logs are saved to a bucket in the same AWS Region as the source bucket.</p>\n<p>This is the right choice, since it does not add any additional costs, while helping trace the user requests made on Amazon S3 buckets.</p>\n<p>Incorrect options:</p>\n<p><strong>Use AWS X-Ray for tracing Amazon S3 requests from end-to-end</strong> - AWS X-Ray collects data about requests that your application serves. You can then view and filter the data to identify and troubleshoot performance issues and errors in your distributed applications and micro-services architecture. For any traced request to your application, it shows you detailed information about the request, the response, and the calls that your application makes to downstream AWS resources, micro-services, databases, and HTTP web APIs.</p>\n<p>AWS X-Ray is overkill for the current requirement and also incurs extra costs, since X-Ray is a paid service.</p>\n<p><strong>Use Amazon Inspector, a security assessment service that helps track the calls made to AWS services configured with it</strong> - Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. This service is not for tracing Amazon S3 access requests.</p>\n<p><strong>Use AWS CloudTrail to identify the requests made to the Amazon S3 buckets</strong> - Amazon S3 lets you identify requests using an AWS CloudTrail event log. By default, CloudTrail logs S3 bucket-level API calls that were made in the last 90 days, but not log requests made to objects.</p>\n<p>AWS suggests use of CloudTrail to access information on Amazon S3 buckets. However, the data needed for current requirement is also provided by S3 server access logging, which is a free service.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/using-s3-access-logs-to-identify-requests.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/using-s3-access-logs-to-identify-requests.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-request-identification.html#cloudtrail-identification-object-access\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-request-identification.html#cloudtrail-identification-object-access</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/tracing_requests_using_xray.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/tracing_requests_using_xray.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>After configuring Amazon EC2 Auto Scaling, a systems administrator had tried to launch the Auto Scaling Group. But, the following launch failure message was displayed - <code>Client.InternalError: Client error on launch</code>.</p>\n<p>What is the cause of this error and how can it be fixed?</p>\n", "answers": ["The security group specified in your launch configuration might have been deleted", "The block device mappings in your launch configuration might contain block device names that are not available or currently not supported", "Your cluster placement group contains an invalid instance type", "This error can be caused when an Auto Scaling group attempts to launch an instance that has an encrypted EBS volume, but the service-linked role does not have access to the customer managed CMK used to encrypt it"], "correct_answer": "This error can be caused when an Auto Scaling group attempts to launch an instance that has an encrypted EBS volume, but the service-linked role does not have access to the customer managed CMK used to encrypt it", "explanation": "<p>Correct option:</p>\n<p><strong>This error can be caused when an Auto Scaling group attempts to launch an instance that has an encrypted EBS volume, but the service-linked role does not have access to the customer managed CMK used to encrypt it</strong></p>\n<p><code>Client.InternalError: Client error on launch</code> error is caused when an Auto Scaling group attempts to launch an instance that has an encrypted EBS volume, but the service-linked role does not have access to the customer managed CMK used to encrypt it. Additional setup is required to allow the Auto Scaling group to launch instances.</p>\n<p>There are two scenarios possible: 1)CMK and Auto Scaling group are in the same AWS account, 2)CMK and Auto Scaling group are in different AWS accounts.</p>\n<p>Full instructions for configuring the above two scenarios:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q44-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ts-as-instancelaunchfailure.html#ts-as-instancelaunchfailure-12\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ts-as-instancelaunchfailure.html#ts-as-instancelaunchfailure-12</a></p>\n<p>Incorrect options:</p>\n<p><strong>The security group specified in your launch configuration might have been deleted</strong> - This configuration will generate an error like so - \"The security group &lt;name of the security group&gt; does not exist. Launching EC2 instance failed.\"</p>\n<p><strong>The block device mappings in your launch configuration might contain block device names that are not available or currently not supported</strong> - This configuration will generate an error like so - \"Invalid device name upload. Launching EC2 instance failed.\"</p>\n<p><strong>Your cluster placement group contains an invalid instance type</strong> - This configuration will generate an error like so - \"Placement groups may not be used with instances of type 'm1.large'. Launching EC2 instance failed.\"</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ts-as-instancelaunchfailure.html#ts-as-instancelaunchfailure-12\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ts-as-instancelaunchfailure.html#ts-as-instancelaunchfailure-12</a></p>\n", "section": "DDomain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>As part of regular maintenance, a systems administrator was checking through the configured Auto Scaling groups (ASGs). An error was raised by an Auto Scaling group when attempting to launch an instance that has an encrypted EBS volume.  The service-linked role did not have access to the customer managed CMK used to encrypt the volume.</p>\n<p>Which of the following represents the best solution to fix this issue?</p>\n", "answers": ["Determine which service-linked role to use for this Auto Scaling group. Update the key policy on the CMK and allow the service-linked role to use the CMK. Update the Auto Scaling group to use the service-linked role", "Use a CMK in the same AWS account as the Auto Scaling group (ASG). Copy and re-encrypt the snapshot with another CMK that belongs to the same account as the Auto Scaling group. Allow the service-linked role to use the new CMK", "Export the CMK to the ASG account from the instance account. Then, define a role to access this CMK and attach the role to ASG", "It is not possible for ASGs to initiate EC2 instances that have encrypted volumes attached to them"], "correct_answer": "Use a CMK in the same AWS account as the Auto Scaling group (ASG). Copy and re-encrypt the snapshot with another CMK that belongs to the same account as the Auto Scaling group. Allow the service-linked role to use the new CMK", "explanation": "<p>Correct option:</p>\n<p><strong>Use a CMK in the same AWS account as the Auto Scaling group (ASG). Copy and re-encrypt the snapshot with another CMK that belongs to the same account as the Auto Scaling group. Allow the service-linked role to use the new CMK</strong></p>\n<p><code>Client.InternalError: Client error on launch</code> error is thrown when an Auto Scaling group attempts to launch an instance that has an encrypted EBS volume, but the service-linked role does not have access to the customer managed CMK used to encrypt it.</p>\n<p>There are two possible solutions:</p>\n<p>Solution 1: Use a CMK in the same AWS account as the Auto Scaling group. Copy and re-encrypt the snapshot with another CMK that belongs to the same account as the Auto Scaling group. Allow the service-linked role to use the new CMK.</p>\n<p>Solution 2: Continue to use the CMK in a different AWS account from the Auto Scaling group. Determine which service-linked role to use for this Auto Scaling group. Allow the Auto Scaling group account access to the CMK. Define an IAM user or role in the Auto Scaling group account that can create a grant. Create a grant to the CMK with the service-linked role as the grantee principal. Update the Auto Scaling group to use the service-linked role.</p>\n<p>Incorrect options:</p>\n<p><strong>Determine which service-linked role to use for this Auto Scaling group. Update the key policy on the CMK and allow the service-linked role to use the CMK. Update the Auto Scaling group to use the service-linked role</strong> - This is possible only when CMK and Auto Scaling group are in the same AWS account.</p>\n<p><strong>Export the CMK to the ASG account from the instance account. Then, define a role to access this CMK and attach the role to ASG</strong> - It is not possible to export CMKs.</p>\n<p><strong>It is not possible for ASGs to initiate EC2 instances that have encrypted volumes attached to them</strong> - This statement is incorrect and only given as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ts-as-instancelaunchfailure.html#ts-as-instancelaunchfailure-10\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ts-as-instancelaunchfailure.html#ts-as-instancelaunchfailure-10</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A retail company has realized that their Amazon EBS volume backed EC2 instance is consistently over-utilized and needs an upgrade. A developer has connected with you to understand the key parameters to be considered when changing the instance type.</p>\n<p>As a SysOps Administrator, which of the following would you identify as correct regarding the instance types for the given use-case? (Select three)</p>\n", "answers": ["Resizing of an instance is only possible if the root device for your instance is an EBS volume", "The new instance retains its pubic, private IPv4 addresses, any Elastic IP addresses, and any IPv6 addresses that were associated with the old instance", "You must stop your Amazon EBS–backed instance before you can change its instance type. AWS moves the instance to new hardware; however, the instance ID does not change", "If your instance is in an Auto Scaling group, the Amazon EC2 Auto Scaling service marks the stopped instance as unhealthy, and may terminate it and launch a replacement instance", "There is no downtime on the instance if you choose an instance of compatible type, since AWS starts the new instance and shifts the applications from current instance", "Resizing of an instance is possible if root device is either EBS volume or an instance store volume. However, instance store volumes taking longer to start on the new instance, since cache data is lost on these instances"], "correct_answer": ["Resizing of an instance is only possible if the root device for your instance is an EBS volume", "You must stop your Amazon EBS–backed instance before you can change its instance type. AWS moves the instance to new hardware; however, the instance ID does not change", "If your instance is in an Auto Scaling group, the Amazon EC2 Auto Scaling service marks the stopped instance as unhealthy, and may terminate it and launch a replacement instance"], "explanation": "<p>Correct options:</p>\n<p><strong>Resizing of an instance is only possible if the root device for your instance is an EBS volume</strong> - If the root device for your instance is an EBS volume, you can change the size of the instance simply by changing its instance type, which is known as resizing it. If the root device for your instance is an instance store volume, you must migrate your application to a new instance with the instance type that you need.</p>\n<p><strong>You must stop your Amazon EBS–backed instance before you can change its instance type. AWS moves the instance to new hardware; however, the instance ID does not change</strong> - You must stop your Amazon EBS–backed instance before you can change its instance type. When you stop and start an instance, AWS moves the instance to new hardware; however, the instance ID does not change.</p>\n<p><strong>If your instance is in an Auto Scaling group, the Amazon EC2 Auto Scaling service marks the stopped instance as unhealthy, and may terminate it and launch a replacement instance</strong> - If your instance is in an Auto Scaling group, the Amazon EC2 Auto Scaling service marks the stopped instance as unhealthy, and may terminate it and launch a replacement instance. To prevent this, you can suspend the scaling processes for the group while you're resizing your instance.</p>\n<p>Incorrect options:</p>\n<p><strong>The new instance retains its pubic, private IPv4 addresses, any Elastic IP addresses, and any IPv6 addresses that were associated with the old instance</strong> - If your instance has a public IPv4 address, AWS releases the address and gives it a new public IPv4 address. The instance retains its private IPv4 addresses, any Elastic IP addresses, and any IPv6 addresses.</p>\n<p><strong>There is no downtime on the instance if you choose an instance of compatible type, since AWS starts the new instance and shifts the applications from current instance</strong> - AWS suggests that you plan for downtime while your instance is stopped. Stopping and resizing an instance may take a few minutes, and restarting your instance may take a variable amount of time depending on your application's startup scripts.</p>\n<p><strong>Resizing of an instance is possible if root device is either EBS volume or an instance store volume. However, instance store volumes taking longer to start on the new instance, since cache data is lost on these instances</strong> - As discussed above, resizing of an instance is possible only if the root device for the instance is an EBS volume.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html</a></p>\n", "section": "DDomain 3: Deployment and Provisioning", "type": "checkbox"}, {"question": "<p>A large IT company manages several projects on AWS Cloud and has decided to use AWS X-Ray to trace application workflows. The company uses a plethora of AWS services like API Gateway, Amazon EC2 instances, Amazon S3 storage service, Elastic Load Balancers and AWS Lambda functions.</p>\n<p>Which of the following should the company keep in mind while using AWS X-Ray for the AWS services they use?</p>\n", "answers": ["Application Load balancers do not send data to X-Ray", "AWS X-Ray does not integrate with Amazon S3 and you need to use CloudTrail for tracking requests on S3", "AWS X-Ray cannot be used to trace your AWS Lambda functions, since they are not integrated", "You cannot use X-Ray to trace or analyze user requests to your Amazon API Gateway APIs"], "correct_answer": "Application Load balancers do not send data to X-Ray", "explanation": "<p>Correct option:</p>\n<p><strong>Application Load balancers do not send data to X-Ray</strong> - Elastic Load Balancing application load balancers add a trace ID to incoming HTTP requests in a header named X-Amzn-Trace-Id. Load balancers do not send data to X-Ray, and do not appear as a node on your service map.</p>\n<p>Incorrect options:</p>\n<p><strong>AWS X-Ray does not integrate with Amazon S3 and you need to use CloudTrail for tracking requests on S3</strong> - AWS X-Ray integrates with Amazon S3 to trace upstream requests to update your application's S3 buckets.</p>\n<p><strong>AWS X-Ray cannot be used to trace your AWS Lambda functions, since they are not integrated</strong> - You can use AWS X-Ray to trace your AWS Lambda functions. Lambda runs the X-Ray daemon and records a segment with details about the function invocation and execution.</p>\n<p><strong>You cannot use X-Ray to trace or analyze user requests to your Amazon API Gateway APIs</strong> - You can use X-Ray to trace and analyze user requests as they travel through your Amazon API Gateway APIs to the underlying services. API Gateway supports X-Ray tracing for all API Gateway endpoint types: Regional, edge-optimized, and private. You can use X-Ray with Amazon API Gateway in all AWS Regions where X-Ray is available.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/xray/latest/devguide/xray-services-elb.html\">https://docs.aws.amazon.com/xray/latest/devguide/xray-services-elb.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>A systems administrator has configured Amazon EC2 instances in an Auto Scaling Group (ASG) for two separate development teams. However, only one configuration has CloudWatch agent installed on the instances, whereas the other one does not have it. The administrator has not manually installed the agents on either group of instances.</p>\n<p>Which of the following would you identify as a root-cause behind this issue?</p>\n", "answers": ["CloudWatch agent can be configured to be loaded on the EC2 instances while configuring the ASG. The developer could have unintentionally checked this flag on one of the ASGs he created", "The architecture of the InstanceType mentioned in your launch configuration does not match the image architecture. So, the ASG was created with errors, resulting in skipping CloudWatch agent. A thorough check is needed for such ASGs, more services could have been skipped", "If your AMI contains a CloudWatch agent, it’s automatically installed on EC2 instances when you create an EC2 Auto Scaling group. The developer needs to chose the AMI that has CloudWatch agent pre-configured on it", "The instance architecture might not have been compatible with the AMI chosen. The incompatibility results in various errors, one of which is, some of the AWS services will not be installed as expected"], "correct_answer": "If your AMI contains a CloudWatch agent, it’s automatically installed on EC2 instances when you create an EC2 Auto Scaling group. The developer needs to chose the AMI that has CloudWatch agent pre-configured on it", "explanation": "<p>Correct option:</p>\n<p><strong>If your AMI contains a CloudWatch agent, it’s automatically installed on EC2 instances when you create an EC2 Auto Scaling group. The developer needs to chose the AMI that has CloudWatch agent pre-configured on it</strong></p>\n<p>If your AMI contains a CloudWatch agent, it’s automatically installed on EC2 instances when you create an EC2 Auto Scaling group. With the stock Amazon Linux AMI, you need to install it (AWS recommends to install via yum).</p>\n<p>Incorrect options:</p>\n<p><strong>CloudWatch agent can be configured to be loaded on the EC2 instances while configuring the ASG. The developer could have unintentionally checked this flag on one of the ASGs he created</strong> - This is incorrect and added only as a distractor.</p>\n<p><strong>The architecture of the <code>InstanceType</code> mentioned in your launch configuration does not match the image architecture. So, the ASG was created with errors, resulting in skipping CloudWatch agent. A thorough check is needed for such ASGs, more services could have been skipped</strong> - This is incorrect. Either the ASG is created successfully or fails completely. Partial installation of services will not take place.</p>\n<p><strong>The instance architecture might not have been compatible with the AMI chosen. The incompatibility results in various errors, one of which is, some of the AWS services will not be installed as expected</strong> - If there are compatibility issues, the ASG will not be able to spin up instances, and throws an error that explains the compatibility error.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/ec2/autoscaling/faqs/\">https://aws.amazon.com/ec2/autoscaling/faqs/</a></p>\n", "section": "DDomain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>An automobile company manages its AWS resource creation and maintenance process through AWS CloudFormation. The company has successfully used CloudFormation so far, and wishes to continue using the service. However, while moving to CloudFormation, the company only moved critical resources and left out the other resources to be managed manually. The ease of creation and maintenance that CloudFormation offers, the company wants to move rest of the resources to CloudFormation.</p>\n<p>Which of the following options is the recommended way to configure this requirement?</p>\n", "answers": ["Use Parameters section of CloudFormation template to input the required resources", "You can bring an existing resource into AWS CloudFormation management using resource import", "You can use Mappings part of CloudFormation template to input the needed resources", "Drift detection is the mechanism by which you add resources to the stack of Cloudformation resources already created"], "correct_answer": "You can bring an existing resource into AWS CloudFormation management using resource import", "explanation": "<p>Correct option:</p>\n<p><strong>You can bring an existing resource into AWS CloudFormation management using <code>resource import</code></strong></p>\n<p>If you created an AWS resource outside of AWS CloudFormation management, you can bring this existing resource into AWS CloudFormation management using <code>resource import</code>. You can manage your resources using AWS CloudFormation regardless of where they were created without having to delete and re-create them as part of a stack.</p>\n<p>During an import operation, you create a change set that imports your existing resources into a stack or creates a new stack from your existing resources. You provide the following during import.</p>\n<ol>\n<li><p>A template that describes the entire stack, including both the original stack resources and the resources you're importing. Each resource to import must have a DeletionPolicy attribute.</p></li>\n<li><p>Identifiers for the resources to import. You provide two values to identify each target resource.</p></li>\n</ol>\n<p>a) An identifier property. This is a resource property that can be used to identify each resource type. For example, an AWS::S3::Bucket resource can be identified using its BucketName.</p>\n<p>b) An identifier value. This is the target resource's actual property value. For example, the actual value for the BucketName property might be MyS3Bucket.</p>\n<p>Incorrect options:</p>\n<p><strong>Use <code>Parameters</code> section of CloudFormation template to input the required resources</strong> - Parameters are a way to provide inputs to your AWS CloudFormation template. They are useful when you want to reuse your templates. Some inputs can not be determined ahead of time. They aren't useful for importing resources into CloudFormation.</p>\n<p><strong>You can use <code>Mappings</code> part of CloudFormation template to input the needed resources</strong> - Mappings are fixed variables within your CloudFormation Template. They’re very handy to differentiate between different environments (dev vs prod), regions (AWS regions), AMI types, etc. They aren't useful for importing resources into CloudFormation.</p>\n<p><strong>Drift detection is the mechanism by which you add resources to the stack of Cloudformation resources already created</strong> - Performing a drift detection operation on a stack determines whether the stack has drifted from its expected template configuration, and returns detailed information about the drift status of each resource in the stack that supports drift detection. It is not useful for importing resources into CloudFormation.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>A SysOps Administrator has come across this CloudFormation template while doing the general maintenance work on the AWS resources used by his team.</p>\n<p>What does this template represent? (Select three)</p>\n<pre><code>AWSTemplateFormatVersion: 2010-09-09\nResources:\n  S3Bucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      AccessControl: PublicRead\n      WebsiteConfiguration:\n        IndexDocument: index.html\n        ErrorDocument: error.html\n    DeletionPolicy: Retain\n  BucketPolicy:\n    Type: AWS::S3::BucketPolicy\n    Properties:\n      PolicyDocument:\n        Id: MyPolicy\n        Version: 2012-10-17\n        Statement:\n          - Sid: PublicReadForGetBucketObjects\n            Effect: Allow\n            Principal: '*'\n            Action: 's3:GetObject'\n            Resource: !Join\n              - ''\n              - - 'arn:aws:s3:::'\n                - !Ref S3Bucket\n                - /*\n      Bucket: !Ref S3Bucket\nOutputs:\n  WebsiteURL:\n    Value: !GetAtt\n      - S3Bucket\n      - WebsiteURL\n    Description: URL for website hosted on S3\n  S3BucketSecureURL:\n    Value: !Join\n      - ''\n      - - 'https://'\n        - !GetAtt\n          - S3Bucket\n          - DomainName\n    Description: Name of S3 bucket to hold website content\n</code></pre>\n", "answers": ["AWS CloudFormation will not delete this bucket when it deletes the stack", "This template creates a bucket as a website", "AWS CloudFormation will delete this bucket when it deletes the stack", "When run from AWS CLI, URL of the website hosted on S3 will be displayed as output", "The output section takes the website URL and bucket URL for another stack, that is part of the nested stack configuration", "The S3 bucket created is configured to store objects from PublicRead API of Amazon RDS"], "correct_answer": ["AWS CloudFormation will not delete this bucket when it deletes the stack", "This template creates a bucket as a website", "When run from AWS CLI, URL of the website hosted on S3 will be displayed as output"], "explanation": "<p>Correct option:</p>\n<p><strong>AWS CloudFormation will not delete this bucket when it deletes the stack</strong></p>\n<p><strong>This template creates a bucket as a website</strong></p>\n<p><strong>When run from AWS CLI, URL of the website hosted on S3 will be displayed as output</strong></p>\n<p>The template above creates a bucket as a website. The AccessControl property is set to the canned ACL PublicRead (public read permissions are required for buckets set up for website hosting). Because this bucket resource has a DeletionPolicy attribute set to Retain, AWS CloudFormation will not delete this bucket when it deletes the stack. The Output section uses Fn::GetAtt to retrieve the WebsiteURL attribute and DomainName attribute of the S3Bucket resource.</p>\n<p>Incorrect options:</p>\n<p><strong>AWS CloudFormation will delete this bucket when it deletes the stack</strong></p>\n<p><strong>The <code>output</code> section takes the website URL and bucket URL for another stack, that is part of the nested stack configuration</strong></p>\n<p><strong>The S3 bucket created is configured to store objects from <code>PublicRead</code> API of Amazon RDS</strong></p>\n<p>These three options contradict the explanation above, so these options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/quickref-s3.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/quickref-s3.html</a></p>\n", "section": "DDomain 3: Deployment and Provisioning", "type": "checkbox"}, {"question": "<p>While checking different deployment options, a development team has realized that there is a significant increase in latency when data on a new EBS volume (created from a snapshot) is accessed for the first time. This seems to be the general behavior of all the EBS volumes used by Amazon EC2 instances for different applications.</p>\n<p>What should the team do to reduce the latency and increase the performance of EBS volumes before moving them to production?</p>\n", "answers": ["Increase read-ahead for high-throughput, read-heavy workloads on st1 and sc1", "Initialize the EBS volume or pre-warm it before moving the volumes to production", "Use RAID 0 to maximize utilization of instance resources", "Use an Amazon EBS–optimized instance"], "correct_answer": "Initialize the EBS volume or pre-warm it before moving the volumes to production", "explanation": "<p>Correct option:</p>\n<p><strong>Initialize the EBS volume or pre-warm it before moving the volumes to production</strong></p>\n<p>There is a significant increase in latency when you first access each block of data on a new EBS volume that was created from a snapshot. You can avoid this performance-lag by using one of the following options:</p>\n<p>Access each block prior to putting the volume into production. This process is called initialization (formerly known as pre-warming).</p>\n<p>Enable fast snapshot restore on a snapshot to ensure that the EBS volumes created from it are fully-initialized at creation and instantly deliver all of their provisioned performance.</p>\n<p>Incorrect options:</p>\n<p><strong>Increase read-ahead for high-throughput, read-heavy workloads on st1 and sc1</strong> - Some workloads are read-heavy and access the block device through the operating system page cache (for example, from a file system). In this case, to achieve the maximum throughput, AWS recommends that you configure the read-ahead setting to 1 MiB. This is a per-block-device setting that should only be applied to your HDD volumes.</p>\n<p><strong>Use RAID 0 to maximize utilization of instance resources</strong> - Some instance types can drive more I/O throughput than what you can provision for a single EBS volume. You can join multiple volumes together in a RAID 0 configuration to use the available bandwidth for these instances.</p>\n<p><strong>Use an Amazon EBS–optimized instance</strong> - An Amazon EBS–optimized instance uses an optimized configuration stack and provides additional, dedicated capacity for Amazon EBS I/O. This optimization provides the best performance for your EBS volumes by minimizing contention between Amazon EBS I/O and other traffic from your instance.</p>\n<p>All of the three options above are meant to improve performance of EBS volumes in general. But, the requirement for the given use case is best achieved by just pre-warming the volumes.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSPerformance.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSPerformance.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>A financial services company runs a flagship application that hosts critical data for several clients. The company uses AWS CloudTrail to track the user activities on various AWS resources. An audit firm has raised several security specific questions about the CloudTrail logs. The company is looking at ways to secure these logs from being tampered.</p>\n<p>What is the recommended way of implementing a solution for this requirement?</p>\n", "answers": ["Use KMS logfile security keys to keep the CloudTrail logs secure and tamper-proof", "Use Amazon S3 Versioning to keep all versions of the file created", "Use CloudTrail log file integrity to keep the logs tamper-proof", "Use Amazon S3 MFA Delete to know the delete operations performed by any user on the logs stored in S3 buckets"], "correct_answer": "Use CloudTrail log file integrity to keep the logs tamper-proof", "explanation": "<p>Correct option:</p>\n<p><strong>Use CloudTrail log file integrity to keep the logs tamper-proof</strong> - A trail is a configuration that enables delivery of events to an Amazon S3 bucket that you specify.</p>\n<p>To determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it, you can use CloudTrail log file integrity validation. This feature is built using industry standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing. This makes it computationally infeasible to modify, delete or forge CloudTrail log files without detection.</p>\n<p>When you enable log file integrity validation, CloudTrail creates a hash for every log file that it delivers. Every hour, CloudTrail also creates and delivers a file that references the log files for the last hour and contains a hash of each. This file is called a digest file. CloudTrail signs each digest file using the private key of a public and private key pair. After delivery, you can use the public key to validate the digest file. CloudTrail uses different key pairs for each AWS region.</p>\n<p>The digest files are delivered to the same Amazon S3 bucket associated with your trail as your CloudTrail log files. If your log files are delivered from all regions or from multiple accounts into a single Amazon S3 bucket, CloudTrail will deliver the digest files from those regions and accounts into the same bucket.</p>\n<p>The digest files are put into a folder separate from the log files. This separation of digest files and log files enables you to enforce granular security policies and permits existing log processing solutions to continue to operate without modification. Each digest file also contains the digital signature of the previous digest file if one exists. The signature for the current digest file is in the metadata properties of the digest file Amazon S3 object.</p>\n<p>Incorrect options:</p>\n<p><strong>Use KMS logfile security keys to keep the CloudTrail logs secure and tamper-proof</strong> - This is a made-up option and given only as a distractor.</p>\n<p><strong>Use Amazon S3 Versioning to keep all versions of the file created</strong> - Amazon S3 Versioning is helpful in retaining all the versions of a file created. CloudTrail log file integrity is a custom made solution for the given requirement.</p>\n<p><strong>Use Amazon S3 MFA Delete to know the delete operations performed by any user on the logs stored in S3 buckets</strong> - If a bucket's versioning configuration is MFA Delete–enabled, the bucket owner must include the x-amz-mfa request header in requests to permanently delete an object version or change the versioning state of the bucket. This option does not address the use-case.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>A media company uses Amazon EC2 instances with EBS volumes as the instance storage. The volumes have scheduled backups as part of the maintenance plans mandated by the company. One of the EBS volumes shows the status of <code>error</code>.</p>\n<p>As a SysOps Administrator, how will you restore the data and get the EBS volume working again?</p>\n", "answers": ["Restart the instance the EBS volume is connected to. In case the data doesn't show up, you can restore the data from the scheduled backups", "You can restore the EBS volume from Amazon Data Lifecycle Manager, by shifting the volume to another EC2 instance configured with the Data Lifecycle Manager", "The error status indicates that the communication channel between EBS volume and the instance has been disrupted. Restart the instance to fix the error", "The error status indicates that the underlying hardware related to the EBS volume has failed. The EBS volume and the data it holds cannot be restored"], "correct_answer": "The error status indicates that the underlying hardware related to the EBS volume has failed. The EBS volume and the data it holds cannot be restored", "explanation": "<p>Correct option:</p>\n<p><strong>The <code>error</code> status indicates that the underlying hardware related to the EBS volume has failed. The EBS volume and the data it holds cannot be restored</strong></p>\n<p>The <code>error</code> status indicates that the underlying hardware related to your EBS volume has failed. The data associated with the volume is unrecoverable and Amazon EBS processes the volume as lost. A notification appears on your account's Personal Health Dashboard when a volume enters an error state.</p>\n<p>You can't recover a volume in an <code>error</code> state, you can restore the lost data from your backup. It’s a best practice to keep backups of your EC2 resources, including EBS volumes. You can use Amazon Data Lifecycle Manager, AWS Backup, or regular EBS snapshots for maintaining regular backups of your critical volumes to avoid data loss.</p>\n<p>Restore the lost data from your backup using one of the following:</p>\n<ol>\n<li><p>If you have an EBS snapshot of the volume, you can restore that volume from your snapshot.</p></li>\n<li><p>If you don't have an EBS snapshot of the volume, create a new EBS volume, and then restore the data to it using the manual backup solution you prefer.</p></li>\n</ol>\n<p>Incorrect options:</p>\n<p><strong>You can restore the EBS volume from Amazon Data Lifecycle Manager, by shifting the volume to another EC2 instance configured with the Data Lifecycle Manager</strong> - You can use Amazon Data Lifecycle Manager to automate the creation, retention, and deletion of EBS snapshots and EBS-backed AMIs. You cannot, however, restore the actual volume that is in <code>error</code> state.</p>\n<p><strong>Restart the instance the EBS volume is connected to. In case the data doesn't show up, you can restore the data from the scheduled backups</strong> - As discussed above, it is a problem with the underlying hardware which will not be fixed by restarting the instance.</p>\n<p><strong>The <code>error</code> status indicates that the communication channel between EBS volume and the instance has been disrupted. Restart the instance to fix the error</strong> - This is an incorrect statement, given only as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ebs-error-status/\">https://aws.amazon.com/premiumsupport/knowledge-center/ebs-error-status/</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>The technology team at a startup is looking at moving their technology infrastructure to AWS Cloud. The team has hired you as a SysOps Administrator to help them understand the mechanics of the EC2 instance IP addressing in an Amazon Virtual Private Cloud (VPC).</p>\n<p>Which of the following would you identify as correct regarding the configuration of IP addresses for EC2 instances? (Select three) </p>\n", "answers": ["You cannot manually associate or disassociate a public IP address from your instance", "AWS releases your instance's public IP address when it is stopped or terminated. However, the IP address is retained if the instance is hibernated", "An instance can have both - a public IP address and an Elastic IP address with it", "By default, Amazon EC2 and Amazon VPC use the IPv4 addressing protocol; you can't disable this behavior", "By default, AWS assigns a public IP address to instances launched in both- default and nondefault VPCs", "If the public IP address of your instance in a VPC has been released, it will not receive a new one if there is more than one network interface attached to your instance"], "correct_answer": ["You cannot manually associate or disassociate a public IP address from your instance", "By default, Amazon EC2 and Amazon VPC use the IPv4 addressing protocol; you can't disable this behavior", "If the public IP address of your instance in a VPC has been released, it will not receive a new one if there is more than one network interface attached to your instance"], "explanation": "<p>Correct option:</p>\n<p><strong>You cannot manually associate or disassociate a public IP address from your instance</strong> - A public IP address is assigned to your instance from Amazon's pool of public IPv4 addresses, and is not associated with your AWS account. You cannot manually associate or disassociate a public IP address from your instance.</p>\n<p><strong>By default, Amazon EC2 and Amazon VPC use the IPv4 addressing protocol; you can't disable this behavior</strong> - Amazon EC2 and Amazon VPC support both the IPv4 and IPv6 addressing protocols. By default, Amazon EC2 and Amazon VPC use the IPv4 addressing protocol; you can't disable this behavior. When you create a VPC, you must specify an IPv4 CIDR block (a range of private IPv4 addresses). You can optionally assign an IPv6 CIDR block to your VPC and subnets, and assign IPv6 addresses from that block to instances in your subnet.</p>\n<p><strong>If the public IP address of your instance in a VPC has been released, it will not receive a new one if there is more than one network interface attached to your instance</strong> - If the public IP address of your instance in a VPC has been released, it will not receive a new one if there is more than one network interface attached to your instance. If your instance's public IP address is released while it has a secondary private IP address that is associated with an Elastic IP address, the instance does not receive a new public IP address.</p>\n<p>Incorrect options:</p>\n<p><strong>AWS releases your instance's public IP address when it is stopped or terminated. However, the IP address is retained if the instance is hibernated</strong> - AWS releases your instance's public IP address when it is stopped, hibernated, or terminated. Your stopped or hibernated instance receives a new public IP address when it is started.</p>\n<p><strong>An instance can have both - a public IP address and an Elastic IP address with it</strong> - AWS releases your instance's public IP address when you associate an Elastic IP address with it. When you disassociate the Elastic IP address from your instance, it receives a new public IP address.</p>\n<p><strong>By default, AWS assigns a public IP address to instances launched in both default and non-default VPCs</strong> - When you launch an instance in a default VPC, AWS assigns it a public IP address by default. When you launch an instance into a non-default VPC, the subnet has an attribute that determines whether instances launched into that subnet receive a public IP address from the public IPv4 address pool. By default, AWS does not assign a public IP address to instances launched in a non-default subnet.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html</a></p>\n", "section": "Domain 6: Networking", "type": "checkbox"}, {"question": "<p>A digital marketing company that manages customer data for its clients has seen a spike in traffic that seems to be malicious. The traffic is served via Amazon CloudFront service. The company wants to set up strong security to keep their server instances and databases secure and also be able to replicate the same security processes across multiple AWS accounts that the company holds.</p>\n<p>As a SysOps Administrator, which of these would you suggest as an optimal solution that can be quickly implemented and replicated?</p>\n", "answers": ["Configure Security Groups on CloudFront to deny access to IP addresses that seem to send the malicious traffic. The Security Group settings can be exported out to another AWS account for easy replication", "Configure AWS Web Application Firewall (WAF) on Amazon EC2 instances to keep the instances as well as the databases safe. WAF configured on CloudFront increases latency for users accessing the application. WAF configuration can be replicated using CloudFormation templates", "Configure AWS Firewall Manager to create a secure barrier on CloudFront. Settings can be replicated across accounts by manually exporting the Firewall Manager configuration", "Configure AWS Web Application Firewall (WAF) on CloudFront to keep the AWS infrastructure safe from malicious attacks. Use AWS Firewall Manager to replicate and manage the WAF configurations across AWS accounts"], "correct_answer": "Configure AWS Web Application Firewall (WAF) on CloudFront to keep the AWS infrastructure safe from malicious attacks. Use AWS Firewall Manager to replicate and manage the WAF configurations across AWS accounts", "explanation": "<p>Correct option:</p>\n<p><strong>Configure AWS Web Application Firewall (WAF) on CloudFront to keep the AWS infrastructure safe from malicious attacks. Use AWS Firewall Manager to replicate and manage the WAF configurations across AWS accounts</strong></p>\n<p>AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon CloudFront distribution, an Amazon API Gateway REST API, an Application Load Balancer, or an AWS AppSync GraphQL API.</p>\n<p>At the simplest level, AWS WAF lets you choose one of the following behaviors:</p>\n<ol>\n<li><p>Allow all requests except the ones that you specify – This is useful when you want Amazon CloudFront, Amazon API Gateway, Application Load Balancer, or AWS AppSync to serve content for a public website, but you also want to block requests from attackers.</p></li>\n<li><p>Block all requests except the ones that you specify – This is useful when you want to serve content for a restricted website whose users are readily identifiable by properties in web requests, such as the IP addresses that they use to browse to the website.</p></li>\n<li><p>Count the requests that match the properties that you specify – When you want to allow or block requests based on new properties in web requests, you first can configure AWS WAF to count the requests that match those properties without allowing or blocking those requests. This lets you confirm that you didn't accidentally configure AWS WAF to block all the traffic to your website. When you're confident that you specified the correct properties, you can change the behavior to allow or block requests.</p></li>\n</ol>\n<p>AWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure, from a central administrator account.</p>\n<p>Incorrect options:</p>\n<p><strong>Configure Security Groups on CloudFront to deny access to IP addresses that seem to send the malicious traffic. The Security Group settings can be exported out to another AWS account for easy replication</strong> - A Security Group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic. CloudFront does not support security groups.</p>\n<p><strong>Configure AWS Web Application Firewall (WAF) on Amazon EC2 instances to keep the instances as well as the databases safe. WAF configured on CloudFront increases latency for users accessing the application. WAF configuration can be replicated using CloudFormation templates</strong> - AWS WAF can only be configured with Amazon CloudFront distribution, an Amazon API Gateway REST API, an Application Load Balancer, or an AWS AppSync GraphQL API. Amazon EC2 instances cannot be directly configured with WAF, they need to be behind a CloudFront distribution or an Application Load Balancer.</p>\n<p><strong>Configure AWS Firewall Manager to create a secure barrier on CloudFront. Settings can be replicated across accounts by manually exporting the Firewall Manager configuration</strong> - It's AWS WAF (NOT Firewall Manager) that can create a secure barrier on CloudFront. AWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations.</p>\n<p>AWS Firewall Manager is integrated with AWS Organizations so you can enable AWS WAF rules, AWS Shield Advanced protections, security groups, and AWS Network Firewall rules for your Amazon VPC across multiple AWS accounts and resources from a single place. FireWall Manager is a management service to manage security resources under one umbrella. There is no need to manually export configurations.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\">https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html</a></p>\n<p><a href=\"https://aws.amazon.com/firewall-manager/\">https://aws.amazon.com/firewall-manager/</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A developer has configured inbound traffic for the relevant ports in both the Security Group of the EC2 instance as well as the Network Access Control List (NACL) of the subnet for the EC2 instance. The developer is, however, unable to connect to the service running on the Amazon EC2 instance.</p>\n<p>As a SysOps Administrator, how will you fix this issue?</p>\n", "answers": ["Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic", "IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs", "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic", "Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior"], "correct_answer": "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic", "explanation": "<p>Correct option:</p>\n<p><strong>Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic</strong> - Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic.</p>\n<p>To enable the connection to a service running on an instance, the associated network ACL must allow both inbound traffic on the port that the service is listening on as well as allow outbound traffic from ephemeral ports. When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port.</p>\n<p>The designated ephemeral port then becomes the destination port for return traffic from the service, so outbound traffic from the ephemeral port must be allowed in the network ACL.</p>\n<p>By default, network ACLs allow all inbound and outbound traffic. If your network ACL is more restrictive, then you need to explicitly allow traffic from the ephemeral port range.</p>\n<p>If you accept traffic from the internet, then you also must establish a route through an internet gateway. If you accept traffic over VPN or AWS Direct Connect, then you must establish a route through a virtual private gateway.</p>\n<p>Incorrect options:</p>\n<p><strong>Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound traffic</strong> - This is incorrect as already discussed.</p>\n<p><strong>IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs</strong> - This is a made-up option and just added as a distractor.</p>\n<p><strong>Rules associated with Network ACLs should never be modified from command line. An attempt to modify rules from command line blocks the rule and results in an erratic behavior</strong> - This option is a distractor. AWS does not support modifying rules of Network ACLs from the command line tool.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/\">https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>An organization has multiple AWS accounts to manage different lines of business. A user from the Finance account has to access reports stored in Amazon S3 buckets of two other AWS accounts (belonging to the HR and Audit departments) and copy these reports back to the S3 bucket in the Finance account. The user has requested necessary permissions from the systems administrator to perform this task.</p>\n<p>As a SysOps Administrator, how will you configure a solution for this requirement?</p>\n", "answers": ["Create resource-based policies in the HR, Audit accounts that will allow the requester from the Finance account to access the respective S3 buckets", "Create resource-level permissions in the HR, Audit accounts to allow access to respective S3 buckets, for user in the Finance account", "Create identity-based IAM policy in the Finance account that allows the user to make a request to the S3 buckets in the HR and Audit accounts. Also, create resource-based IAM policies in the HR, Audit accounts that will allow the requester from the Finance account to access the respective S3 buckets", "Create IAM roles in the HR, Audit accounts, which can be assumed by the user from the Finance account when the user needs to access the S3 buckets of the accounts"], "correct_answer": "Create identity-based IAM policy in the Finance account that allows the user to make a request to the S3 buckets in the HR and Audit accounts. Also, create resource-based IAM policies in the HR, Audit accounts that will allow the requester from the Finance account to access the respective S3 buckets", "explanation": "<p>Correct option:</p>\n<p><strong>Create identity-based IAM policy in the Finance account, that allow the user to make a request to the S3 buckets in the HR and Audit accounts. Also, create resource-based IAM policies in the HR, Audit accounts that will allow the requester from the Finance account to access the respective S3 buckets</strong></p>\n<p>Identity-based policies are attached to an IAM user, group, or role. These policies let you specify what that identity can do (its permissions).</p>\n<p>Resource-based policies are attached to a resource. For example, you can attach resource-based policies to Amazon S3 buckets, Amazon SQS queues, and AWS Key Management Service encryption keys.</p>\n<p>Identity-based policies and resource-based policies are both permissions policies and are evaluated together. For a request to which only permissions policies apply, AWS first checks all policies for a Deny. If one exists, then the request is denied. Then AWS checks for each Allow. If at least one policy statement allows the action in the request, the request is allowed. It doesn't matter whether the Allow is in the identity-based policy or the resource-based policy.</p>\n<p>For requests made from one account to another, the requester in Account A must have an identity-based policy that allows them to make a request to the resource in Account B. Also, the resource-based policy in Account B must allow the requester in Account A to access the resource. There must be policies in both accounts that allow the operation, otherwise the request fails.</p>\n<p>Comparing IAM policies:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q57-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Create resource-based policies in the HR, Audit accounts that will allow the requester from the Finance account to access the respective S3 buckets</strong> - Creating resource-based policy alone will be sufficient when the request is made within a single AWS account.</p>\n<p><strong>Create resource-level permissions in the HR, Audit accounts to allow access to respective S3 buckets, for user in the Finance account</strong> - Resource-based policies differ from resource-level permissions. You can attach resource-based policies directly to a resource, as described in this topic. Resource-level permissions refer to the ability to use ARNs to specify individual resources in a policy. Resource-based policies are supported only by some AWS services.</p>\n<p><strong>Create IAM roles in the HR, Audit accounts, which can be assumed by the user from the Finance account when the user needs to access the S3 buckets of the accounts</strong> - Cross-account access with a resource-based policy has some advantages over cross-account access with a role. With a resource that is accessed through a resource-based policy, the principal still works in the trusted account and does not have to give up his or her permissions to receive the role permissions. In other words, the principal continues to have access to resources in the trusted account at the same time as he or she has access to the resource in the trusting account. This is useful for tasks such as copying information to or from the shared resource in the other account.</p>\n<p>We chose resource-based policy, so the user from the Finance account will continue to have access to resources in his own account while also getting permissions on resources from other accounts.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A retail company wants to get out of the business of owning and maintaining its own IT infrastructure. As part of this digital transformation, the company wants to archive about 5PB of data in its on-premises data center to durable long term storage.</p>\n<p>As a SysOps Administrator, what is your recommendation to migrate this data in the MOST cost-optimal way?</p>\n", "answers": ["Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data into AWS Glacier", "Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into AWS Glacier", "Setup Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into AWS Glacier", "Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into AWS Glacier"], "correct_answer": "Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into AWS Glacier", "explanation": "<p>Correct option:</p>\n<p><strong>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data into Amazon S3 and create a lifecycle policy to transition the data into AWS Glacier</strong></p>\n<p>Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases.\nThe data stored on the Snowball Edge device can be copied into the S3 bucket and later transitioned into AWS Glacier via a lifecycle policy. You can't directly copy data from Snowball Edge devices into AWS Glacier.</p>\n<p>Incorrect options:</p>\n<p><strong>Transfer the on-premises data into multiple Snowball Edge Storage Optimized devices. Copy the Snowball Edge data into AWS Glacier</strong> - As mentioned earlier, you can't directly copy data from Snowball Edge devices into AWS Glacier. Hence, this option is incorrect.</p>\n<p><strong>Setup AWS direct connect between the on-premises data center and AWS Cloud. Use this connection to transfer the data into AWS Glacier</strong> - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. Direct Connect involves significant monetary investment and takes more than a month to set up, therefore it's not the correct fit for this use-case where just a one-time data transfer has to be done.</p>\n<p><strong>Setup Site-to-Site VPN connection between the on-premises data center and AWS Cloud. Use this connection to transfer the data into AWS Glacier</strong> - AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). VPN Connections are a good solution if you have an immediate need, and have low to modest bandwidth requirements. Because of the high data volume for the given use-case, Site-to-Site VPN is not the correct choice.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>The Chief Technology Officer (CTO) of a healthcare company realised that he does not have access to an Amazon S3 bucket present in the company's own AWS account. The CTO is the root user for the AWS account and has created other AWS users using the root user account.</p>\n<p>What is the reason for this behavior and how can you fix this?</p>\n", "answers": ["If an IAM user, with full access to IAM and Amazon S3, assigns a bucket policy to an Amazon S3 bucket and doesn't specify the AWS account root user as a principal, the root user is denied access to that bucket", "Root user always has access to all the resources of the account. The Amazon S3 bucket could be from another AWS account and the S3 bucket has been shared with the root user and hence appears in his list of S3 buckets", "An Amazon S3 bucket policy that specifies a wildcard (*) in the principal element, sometimes is declared void by AWS to avoid risk of complete public exposure. Such S3 buckets policies are in invalid status and have random behavior", "Root user has access to all the resources in his AWS account. Contact AWS support to resolve the access issue"], "correct_answer": "If an IAM user, with full access to IAM and Amazon S3, assigns a bucket policy to an Amazon S3 bucket and doesn't specify the AWS account root user as a principal, the root user is denied access to that bucket", "explanation": "<p>Correct option:</p>\n<p><strong>If an IAM user, with full access to IAM and Amazon S3, assigns a bucket policy to an Amazon S3 bucket and doesn't specify the AWS account root user as a principal, the root user is denied access to that bucket</strong></p>\n<p>Sometimes, you might have an IAM user with full access to IAM and Amazon S3. If the IAM user assigns a bucket policy to an Amazon S3 bucket and doesn't specify the AWS account root user as a principal, the root user is denied access to that bucket. However, as the root user, you can still access the bucket. To do that, modify the bucket policy to allow root user access from the Amazon S3 console or the AWS CLI. Use the following principal, replacing 123456789012 with the ID of the AWS account.</p>\n<p><code>\"Principal\": { \"AWS\": \"arn:aws:iam::123456789012:root\" }</code></p>\n<p>Incorrect options:</p>\n<p><strong>Root user always has access to all the resources of the account. The Amazon S3 bucket could be from another AWS account and the S3 bucket has been shared with the root user and hence appears in his list of S3 buckets</strong></p>\n<p><strong>An Amazon S3 bucket policy that specifies a wildcard (*) in the principal element, sometimes is declared void by AWS to avoid risk of complete public exposure. Such S3 buckets policies are in invalid status and have random behavior</strong></p>\n<p><strong>Root user has access to all the resources in his AWS account. Contact AWS support to resolve the access issue</strong></p>\n<p>These three options contradict the explanation above, so these options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_iam-s3.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_iam-s3.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>The development team at a retail company manages the deployment and scaling of their web application through AWS Elastic Beanstalk. After configuring the Elastic Beanstalk environment, the team has realized that Beanstalk is not handling the scaling activities the way they expected. This has impacted the application's ability to respond to the variations in traffic.</p>\n<p>How should the environment be configured to get the best of Beanstalk's auto scaling capabilities?</p>\n", "answers": ["The IAM Role attached to the Auto Scaling group might not have enough permissions to scale instances on-demand", "The Auto Scaling group in your Elastic Beanstalk environment uses the number of logged-in users, as the criteria to trigger auto scaling action. These alarms must be configured based on the parameters appropriate for your application", "By default, Auto Scaling group created from Beanstalk uses Elastic Load Balancing health checks. Configure the Beanstalk to use Amazon EC2 status checks", "The Auto Scaling group in your Elastic Beanstalk environment uses two default Amazon CloudWatch alarms to trigger scaling operations. These alarms must be configured based on the parameters appropriate for your application"], "correct_answer": "The Auto Scaling group in your Elastic Beanstalk environment uses two default Amazon CloudWatch alarms to trigger scaling operations. These alarms must be configured based on the parameters appropriate for your application", "explanation": "<p>Correct option:</p>\n<p><strong>The Auto Scaling group in your Elastic Beanstalk environment uses two default Amazon CloudWatch alarms to trigger scaling operations. These alarms must be configured based on the parameters appropriate for your application</strong></p>\n<p>The Auto Scaling group in your Elastic Beanstalk environment uses two Amazon CloudWatch alarms to trigger scaling operations. Default Auto Scaling triggers are configured to scale when the average outbound network traffic (NetworkOut) from each instance is higher than 6 MB or lower than 2 MB over a period of five minutes.</p>\n<p>For more efficient Amazon EC2 Auto Scaling, configure triggers that are appropriate for your application, instance type, and service requirements. You can scale based on several statistics including latency, disk I/O, CPU utilization, and request count.</p>\n<p>Incorrect options:</p>\n<p><strong>The IAM Role attached to the Auto Scaling group might not have enough permissions to scale instances on-demand</strong> - The Auto Scaling group will not be able to spin up Amazon EC2 instances if the IAM Role associated with Beanstalk does not have enough permissions. Since the current use case talks about scaling not happening at expected rate, this should not be the issue.</p>\n<p><strong>By default, Auto Scaling group created from Beanstalk uses Elastic Load Balancing health checks. Configure the Beanstalk to use Amazon EC2 status checks</strong> - This statement is incorrect. By default, Auto Scaling group created from Beanstalk uses Amazon EC2 status checks.</p>\n<p><strong>The Auto Scaling group in your Elastic Beanstalk environment uses the number of logged-in users, as the criteria to trigger auto scaling action. These alarms must be configured based on the parameters appropriate for your application</strong> - The default scaling criteria has already been discussed above (and it is not the number of logged-in users).</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.alarms.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.alarms.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>A company initially used a manual process to create and manage different IAM roles needed for the organization. As the company expanded and lines of business grew, different AWS accounts were created to manage the AWS resources as well as the users. The manual process has resulted in errors with IAM roles getting created with insufficient permissions. The company is looking at automating the process of creating and managing the necessary IAM roles for multiple AWS accounts. The company already uses AWS Organizations to manage multiple AWS accounts.</p>\n<p>As a SysOps Administrator, can you suggest an effective way to automate this process?</p>\n", "answers": ["Create CloudFormation templates and reuse them to create necessary IAM roles in each of the AWS accounts", "Use CloudFormation StackSets with AWS Organizations to deploy and manage IAM roles to multiple AWS accounts simultaneously", "Use AWS Directory Service with AWS Organizations to automatically associate necessary IAM roles with the Microsoft Active Directory users", "Use AWS Resource Access Manager that integrates with AWS Organizations to deploy and manage shared resources across AWS accounts"], "correct_answer": "Use CloudFormation StackSets with AWS Organizations to deploy and manage IAM roles to multiple AWS accounts simultaneously", "explanation": "<p>Correct option:</p>\n<p><strong>Use CloudFormation StackSets with AWS Organizations to deploy and manage IAM roles to multiple AWS accounts simultaneously</strong></p>\n<p>CloudFormation StackSets allow you to roll out CloudFormation stacks over multiple AWS accounts and in multiple Regions with just a couple of clicks. When AWS launched StackSets, grouping accounts was primarily for billing purposes. Since the launch of AWS Organizations, you can centrally manage multiple AWS accounts across diverse business needs including billing, access control, compliance, security and resource sharing.</p>\n<p>You can now centrally orchestrate any AWS CloudFormation enabled service across multiple AWS accounts and regions. For example, you can deploy your centralized AWS Identity and Access Management (IAM) roles, provision Amazon Elastic Compute Cloud (EC2) instances or AWS Lambda functions across AWS Regions and accounts in your organization. CloudFormation StackSets simplify the configuration of cross-accounts permissions and allow for automatic creation and deletion of resources when accounts are joining or are removed from your Organization.</p>\n<p>You can get started by enabling data sharing between CloudFormation and Organizations from the StackSets console. Once done, you will be able to use StackSets in the Organizations master account to deploy stacks to all accounts in your organization or in specific organizational units (OUs). A new service managed permission model is available with these StackSets. Choosing Service managed permissions allows StackSets to automatically configure the necessary IAM permissions required to deploy your stack to the accounts in your organization.</p>\n<p>How to use AWS CloudFormation StackSets for Multiple Accounts in an AWS Organization:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q61-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/\">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a></p>\n<p>Incorrect options:</p>\n<p><strong>Create CloudFormation templates and reuse them to create necessary IAM roles in each of the AWS accounts</strong> - CloudFormation templates can ease the current manual process that the company is using. However, it's not a completely automated process that the company needs.</p>\n<p><strong>Use AWS Directory Service with AWS Organizations to automatically associate necessary IAM roles with the Microsoft Active Directory users</strong> - AWS Directory Service for Microsoft Active Directory, or AWS Managed Microsoft AD, lets you run Microsoft Active Directory (AD) as a managed service. AWS Directory Service makes it easy to set up and run directories in the AWS Cloud or connect your AWS resources with an existing on-premises Microsoft Active Directory. It is not for automatic creation of IAM roles across AWS accounts.</p>\n<p><strong>Use AWS Resource Access Manager that integrates with AWS Organizations to deploy and manage shared resources across AWS accounts</strong> - AWS Resource Access Manager (AWS RAM) enables you to share specified AWS resources that you own with other AWS accounts. It's a centralized service that provides a consistent experience for sharing different types of AWS resources across multiple accounts. This service enables you to share resources across AWS accounts. It's not meant for re-creating the same resource definitions in different AWS accounts.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/\">https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multiple-accounts-in-an-aws-organization/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-ram.html\">https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-ram.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>An e-commerce company uses AWS Elastic Beanstalk to create test environments comprising of an Amazon EC2 instance and an RDS instance whenever a new product or line-of-service is launched. The company is currently testing one such environment but wants to decouple the database from the environment to run some analysis and reports later in another environment. Since testing is in progress for a high-stakes product, the company wants to avoid downtime and database sync issues.</p>\n<p>As a SysOps Administrator, which solution will you recommend to the company?</p>\n", "answers": ["Since it is a test environment, take a snapshot of the database and terminate the current environment. Create a new one without attaching an RDS instance directly to it (from the snapshot)", "Use an Elastic Beanstalk Immutable deployment to make the entire architecture completely reliable. You can terminate the first environment whenever you are confident of the second environment working correctly", "Use an Elastic Beanstalk blue (environment A)/green (environment B) deployment to decouple the RDS DB instance from environment A. Create a new Elastic Beanstalk environment (environment B) with the necessary information to connect to the decoupled RDS DB instance", "Decoupling an RDS instance that is part of a running Elastic Beanstalk environment is not currently supported by AWS. You will need to terminate the current environment after taking the snapshot of the database and create a new one with RDS configured outside the environment"], "correct_answer": "Use an Elastic Beanstalk blue (environment A)/green (environment B) deployment to decouple the RDS DB instance from environment A. Create a new Elastic Beanstalk environment (environment B) with the necessary information to connect to the decoupled RDS DB instance", "explanation": "<p>Correct option:</p>\n<p><strong>Use an Elastic Beanstalk blue (environment A)/green (environment B) deployment to decouple the RDS DB instance from environment A. Create a new Elastic Beanstalk environment (environment B) with the necessary information to connect to the decouple RDS DB instance</strong> - Attaching an RDS DB instance to an Elastic Beanstalk environment is ideal for development and testing environments. However, it's not recommended for production environments because the lifecycle of the database instance is tied to the lifecycle of your application environment. If you terminate the environment, then you lose your data because the RDS DB instance is deleted by the environment.</p>\n<p>Since, the current use case mentions not having a downtime on the database, we can follow these steps for resolution:\n1. Use an Elastic Beanstalk blue (environment A)/green (environment B) deployment to decouple an RDS DB instance from environment A. Create an RDS DB snapshot and enable <code>Deletion protection</code> on the DB instance to Safeguard your RDS DB instance from deletion.\n2. Create a new Elastic Beanstalk environment (environment B) with the necessary information to connect to the RDS DB instance. Your new Elastic Beanstalk environment (environment B) must not include an RDS DB instance in the same Elastic Beanstalk application.</p>\n<p>Step-by-step instructions to configure the above solution:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q62-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/\">https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/</a></p>\n<p>Incorrect options:</p>\n<p><strong>Since it is a test environment, take a snapshot of the database and terminate the current environment. Create a new one without attaching an RDS instance directly to it (from the snapshot)</strong> - It is clearly mentioned in the problem statement that the company is looking at a solution with no downtime. Hence, this is an incorrect option.</p>\n<p><strong>Use an Elastic Beanstalk Immutable deployment to make the entire architecture completely reliable. You can terminate the first environment whenever you are confident of the second environment working correctly</strong> - Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched. This solution is an over-kill for the test environment, even if the company is looking at a no-downtime option.</p>\n<p><strong>Decoupling an RDS instance that is part of a running Elastic Beanstalk environment is not currently supported by AWS. You will need to terminate the current environment after taking the snapshot of the database and create a new one with RDS configured outside the environment</strong> - This is a made-up option and given only as a distractor.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/\">https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p>\n", "section": "DDomain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>An IT company runs its server infrastructure on Amazon EC2 instances configured in an Auto Scaling Group (ASG) fronted by an Elastic Load Balancer (ELB). For ease of deployment and flexibility in scaling, this AWS architecture is maintained via an Elastic Beanstalk environment. The technology lead of a project has requested to automate the replacement of unhealthy Amazon EC2 instances in the Elastic Beanstalk environment.</p>\n<p>How will you configure a solution for this requirement?</p>\n", "answers": ["To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from EC2 to ELB by using a configuration file of your Beanstalk environment", "Modify the Auto Scaling Group from Amazon EC2 console directly to change the health check type to ELB", "To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from ELB to EC2 by using a configuration file of your Beanstalk environment", "Modify the Auto Scaling Group from Amazon EC2 console directly to change the health check type to EC2"], "correct_answer": "To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from EC2 to ELB by using a configuration file of your Beanstalk environment", "explanation": "<p>Correct option:</p>\n<p><strong>To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from EC2 to ELB by using a configuration file of your Beanstalk environment</strong></p>\n<p>By default, the health check configuration of your Auto Scaling group is set as an EC2 type that performs a status check of EC2 instances. To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from EC2 to ELB by using a configuration file.</p>\n<p>The following are some important points to remember:</p>\n<ol>\n<li><p>Status checks cover only an EC2 instance's health, and not the health of your application, server, or any Docker containers running on the instance.</p></li>\n<li><p>If your application crashes, the load balancer removes the unhealthy instances from its target. However, your Auto Scaling group doesn't automatically replace the unhealthy instances marked by the load balancer.</p></li>\n<li><p>By changing the health check type of your Auto Scaling group from EC2 to ELB, you enable the Auto Scaling group to automatically replace the unhealthy instances when the health check fails.</p></li>\n</ol>\n<p>Complete list of steps to configure the above:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt1-q63-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-instance-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-instance-automation/</a></p>\n<p>Incorrect options:</p>\n<p><strong>To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from ELB to EC2 by using a configuration file of your Beanstalk environment</strong> - As mentioned earlier, the health check type of your instance's Auto Scaling group should be changed from EC2 to ELB.</p>\n<p><strong>Modify the Auto Scaling Group from Amazon EC2 console directly to change the health check type to ELB</strong></p>\n<p><strong>Modify the Auto Scaling Group from Amazon EC2 console directly to change the health check type to EC2</strong></p>\n<p>You should configure your Amazon EC2 instances in an Elastic Beanstalk environment by using Elastic Beanstalk configuration files (.ebextensions). Configuration changes made to your Elastic Beanstalk environment won't persist if you use the following configuration methods:</p>\n<ol>\n<li><p>Configuring an Elastic Beanstalk resource directly from the console of a specific AWS service.</p></li>\n<li><p>Installing a package, creating a file, or running a command directly from your Amazon EC2 instance.</p></li>\n</ol>\n<p>Both these options contradict the above explanation and therefore these two options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-configuration-files/\">https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-configuration-files/</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>An e-commerce company runs its web application on Amazon EC2 instances backed by Amazon Elastic Block Store (Amazon EBS) volumes. An Amazon S3 bucket is used for storing sharable data. A developer has attached an Amazon EBS to an Amazon EC2 instance, but it’s still in the \"attaching\" state after 10-15 minutes.</p>\n<p>As a SysOps Administrator, what solution will you suggest to fix this issue with the EBS volume?</p>\n", "answers": ["Check that the device name you specified when you attempted to attach the EBS volume isn't already in use. Attempt to attach the volume to the instance, again, but use a different device name", "The EBS volume could be encrypted and the custom KMS key used to encrypt the snapshot is missing. The custom KMS key needs to be added to the volume configuration", "Each EBS volume receives an initial I/O credit balance, an error in accumulating the credit balance can stop the volume from attaching properly to the instance. Restart the instance to fix the error", "The attaching status indicates that the underlying hardware related to your EBS volume has failed. This issue cannot be fixed. Raise a service request on AWS and request for a new volume. You are not charged for volumes that are in error state"], "correct_answer": "Check that the device name you specified when you attempted to attach the EBS volume isn't already in use. Attempt to attach the volume to the instance, again, but use a different device name", "explanation": "<p>Correct option:</p>\n<p><strong>Check that the device name you specified when you attempted to attach the EBS volume isn't already in use. Attempt to attach the volume to the instance, again, but use a different device name</strong></p>\n<p>Check that the device name you specified when you attempted to attach the EBS volume isn't already in use. If the specified device name is already being used by the block device driver of the EC2 instance, the operation fails.</p>\n<p>When attaching an EBS volume to an Amazon EC2 instance, you can specify a device name for the volume (by default, one is filled in for you). The block device driver of the EC2 instance mounts the volume and assigns a name. The volume name can be different from the name that you assign.</p>\n<p>If you specify a device name that's not in use by Amazon EC2, but is used by the block device driver within the EC2 instance, the attachment of the Amazon EBS volume fails. Instead, the EBS volume is stuck in the attaching state. This is usually due to one of the following reasons:</p>\n<ol>\n<li><p>The block device driver is remapping the specified device name : On an HVM EC2 instance, /dev/sda1 remaps to /dev/xvda. If you attempt to attach a secondary Amazon EBS volume to /dev/xvda, the secondary EBS volume can't successfully attach to the instance. This can cause the EBS volume to be stuck in the attaching state.</p></li>\n<li><p>The block device driver didn't release the device name : If a user has initiated a forced detach of an Amazon EBS volume, the block device driver of the Amazon EC2 instance might not immediately release the device name for reuse. Attempting to use that device name when attaching a volume causes the volume to be stuck in the attaching state. You must either choose a different device name or reboot the instance.</p></li>\n</ol>\n<p>You can resolve most issues with volumes stuck in the attaching state by following these steps: Force detach the volume and attempt to attach the volume to the instance, again, but use a different device name. The instance must be in running state for this to work.</p>\n<p>If the above does not solve the problem, you can reboot the instance or stop and start the instance to migrate it to new underlying hardware. Keep in mind that instance store data is lost when you stop and start an instance.</p>\n<p>Incorrect options:</p>\n<p><strong>The EBS volume could be encrypted and the custom KMS key used to encrypt the snapshot is missing. The custom KMS key needs to be added to the volume configuration</strong> - A missing KMS key will not lead to <code>attaching</code> state of the volume.</p>\n<p><strong>Each EBS volume receives an initial I/O credit balance, an error in accumulating the credit balance can stop the volume from attaching properly to the instance. Restart the instance to fix the error</strong> - This is a made-up option and has been added as a distractor.</p>\n<p><strong>The <code>attaching</code> status indicates that the underlying hardware related to your EBS volume has failed. This issue cannot be fixed. Raise a service request on AWS and request for a new volume. You are not charged for volumes that are in error state</strong> - When the underlying hardware related to your EBS volume has failed, the EBS volume will have a status of <code>error</code>. The data associated with the volume is unrecoverable and Amazon EBS processes the volume as lost. AWS doesn't bill for volumes with a status of <code>error</code>.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ebs-stuck-attaching/\">https://aws.amazon.com/premiumsupport/knowledge-center/ebs-stuck-attaching/</a></p>\n<p><a href=\"https://docs.amazonaws.cn/en_us/AWSEC2/latest/WindowsGuide/ebs-volume-types.html\">https://docs.amazonaws.cn/en_us/AWSEC2/latest/WindowsGuide/ebs-volume-types.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>A hospitality company runs their applications on its on-premises infrastructure but stores the critical customer data on AWS Cloud using AWS Storage Gateway. At a recent audit, the company has been asked if the customer data is secure while in-transit and at rest in the Cloud.</p>\n<p>What is the correct answer to the auditor's question? And what should the company change to meet the security requirements?</p>\n", "answers": ["AWS Storage Gateway uses SSL/TLS (Secure Socket Layers/Transport Layer Security) to encrypt data that is transferred between your gateway appliance and AWS storage. File and Volume Gateway data stored on Amazon S3 is encrypted. Tape Gateway data cannot be encrypted at-rest", "AWS Storage Gateway uses IPsec to encrypt data that is transferred between your gateway appliance and AWS storage. File and Volume Gateway data stored on Amazon S3 is encrypted. Tape Gateway data cannot be encrypted at-rest", "AWS Storage Gateway uses SSL/TLS (Secure Socket Layers/Transport Layer Security) to encrypt data that is transferred between your gateway appliance and AWS storage. By default, Storage Gateway uses Amazon S3-Managed Encryption Keys to server-side encrypt all data it stores in Amazon S3", "AWS Storage Gateway uses IPsec to encrypt data that is transferred between your gateway appliance and AWS storage. All three Gateway types store data in encrypted form at-rest"], "correct_answer": "AWS Storage Gateway uses SSL/TLS (Secure Socket Layers/Transport Layer Security) to encrypt data that is transferred between your gateway appliance and AWS storage. By default, Storage Gateway uses Amazon S3-Managed Encryption Keys to server-side encrypt all data it stores in Amazon S3", "explanation": "<p>Correct option:</p>\n<p><strong>AWS Storage Gateway uses SSL/TLS (Secure Socket Layers/Transport Layer Security) to encrypt data that is transferred between your gateway appliance and AWS storage. By default, Storage Gateway uses Amazon S3-Managed Encryption Keys to server-side encrypt all data it stores in Amazon S3</strong></p>\n<p>AWS Storage Gateway uses SSL/TLS (Secure Socket Layers/Transport Layer Security) to encrypt data that is transferred between your gateway appliance and AWS storage. By default, Storage Gateway uses Amazon S3-Managed Encryption Keys (SSE-S3) to server-side encrypt all data it stores in Amazon S3. You have an option to use the Storage Gateway API to configure your gateway to encrypt data stored in the cloud using server-side encryption with AWS Key Management Service (SSE-KMS) customer master keys (CMKs).</p>\n<p>File, Volume and Tape Gateway data is stored in Amazon S3 buckets by AWS Storage Gateway. Tape Gateway supports backing data to Amazon S3 Glacier apart from the standard storage.</p>\n<p>Encrypting a file share: For a file share, you can configure your gateway to encrypt your objects with AWS KMS–managed keys by using SSE-KMS.</p>\n<p>Encrypting a volume: For cached and stored volumes, you can configure your gateway to encrypt volume data stored in the cloud with AWS KMS–managed keys by using the Storage Gateway API.</p>\n<p>Encrypting a tape: For a virtual tape, you can configure your gateway to encrypt tape data stored in the cloud with AWS KMS–managed keys by using the Storage Gateway API.</p>\n<p>Incorrect options:</p>\n<p><strong>AWS Storage Gateway uses IPsec to encrypt data that is transferred between your gateway appliance and AWS storage. File and Volume Gateway data stored on Amazon S3 is encrypted. Tape Gateway data cannot be encrypted at-rest</strong></p>\n<p><strong>AWS Storage Gateway uses IPsec to encrypt data that is transferred between your gateway appliance and AWS storage. All three Gateway types store data in encrypted form at-rest</strong></p>\n<p>There is no such thing as using IPSec for encrypting in-transit data between your gateway appliance and AWS storage. You need to use SSL/TLS for this. So both these options are incorrect.</p>\n<p><strong>AWS Storage Gateway uses SSL/TLS (Secure Socket Layers/Transport Layer Security) to encrypt data that is transferred between your gateway appliance and AWS storage. File and Volume Gateway data stored on Amazon S3 is encrypted. Tape Gateway data cannot be encrypted at-rest</strong> - For a virtual tape, you can configure your gateway to encrypt tape data stored in the cloud with AWS KMS–managed keys by using the Storage Gateway API. So this option is incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/encryption.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/encryption.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}]; 

    let correctCount = 0;
    let incorrectCount = 0;
    let totalQuestions = allQuizData.length;

    function shuffleArray(array) {
        for (let i = array.length - 1; i > 0; i--) {
            const j = Math.floor(Math.random() * (i + 1));
            [array[i], array[j]] = [array[j], array[i]];
        }
        return array;
    }

    function updateScoreDisplay() {
        document.getElementById('total-questions-display').textContent = totalQuestions;
        document.getElementById('correct-answers-display').textContent = correctCount;
        document.getElementById('wrong-answers-display').textContent = incorrectCount;
    }

    function renderQuiz() {
        const quizContainer = document.getElementById('quiz-container');
        quizContainer.innerHTML = '';
        correctCount = 0;
        incorrectCount = 0;

        const shuffledQuestions = shuffleArray([...allQuizData]);

        let questionCounter = 1;
        const sectionsMap = new Map();

        shuffledQuestions.forEach(qData => {
            const sectionName = qData.section || 'Uncategorized';
            if (!sectionsMap.has(sectionName)) {
                sectionsMap.set(sectionName, []);
            }
            sectionsMap.get(sectionName).push(qData);
        });

        sectionsMap.forEach((questionsInSection, sectionName) => {
            const sectionHeaderHtml = `<h2 class="section-header">${sectionName}</h2>`;
            quizContainer.insertAdjacentHTML('beforeend', sectionHeaderHtml);

            questionsInSection.forEach(qData => {
                const questionId = `q_${qData.id || questionCounter}`;
                const questionHtml = qData.question; // Giữ nguyên HTML
                const correctAnswers = qData.correct_answer;
                const explanation = qData.explanation;
                const questionType = qData.type;

                const shuffledAnswers = shuffleArray([...qData.answers]);

                let optionsListHtml = "";
                shuffledAnswers.forEach((ansText, i) => {
                    const inputId = `${questionId}_option_${i}`;
                    optionsListHtml += `
                    <label for="${inputId}" class="option-label">
                        <input type="${questionType}" id="${inputId}" name="${questionId}" value="${ansText.replace(/"/g, '&quot;')}" />
                        <span>${ansText}</span>
                    </label>
                    `;
                });
                
                const encodedCorrectAnswers = encodeURIComponent(JSON.stringify(correctAnswers));
                const formHtml = `
                <form id="form_${questionId}" class="question-container" data-question-id="${questionId}" data-correct-answer="${encodedCorrectAnswers}" data-question-type="${questionType}">
                    <p class="question-prompt">Câu hỏi ${questionCounter}:</p>
                    <div class="question-content">${questionHtml}</div> <div class="options-container">
                        ${optionsListHtml}
                    </div>
                    <div class="buttons-container">
                        <button type="button" class="submit-btn" onclick="checkAnswer('${questionId}')">Kiểm tra câu trả lời</button>
                        <button type="button" class="explanation-btn" onclick="showExplanation('${questionId}')">Xem giải thích</button>
                    </div>
                    <div id="feedback_${questionId}" class="feedback-message"></div>
                    <div id="explanation_content_${questionId}" style="display: none;">${explanation}</div>
                </form>
                `;
                quizContainer.insertAdjacentHTML('beforeend', formHtml);
                questionCounter++;
            });
        });
        updateScoreDisplay();
    }

    function checkAnswer(questionId) {
        const form = document.getElementById(`form_${questionId}`);
        const questionType = form.dataset.questionType;
        const correctAnswers = JSON.parse(decodeURIComponent(form.dataset.correctAnswer)); 
        const feedbackDiv = document.getElementById(`feedback_${questionId}`);
        const submitBtn = form.querySelector('.submit-btn');

        let selectedOptions = [];
        if (questionType === 'radio') {
            const selectedRadio = form.querySelector(`input[name="${questionId}"]:checked`);
            if (selectedRadio) {
                selectedOptions.push(selectedRadio.value);
            }
        } else { /* checkbox */
            form.querySelectorAll(`input[name="${questionId}"]:checked`).forEach(checkbox => {
                selectedOptions.push(checkbox.value);
            });
        }

        if (selectedOptions.length === 0) {
            feedbackDiv.className = 'incorrect-feedback';
            feedbackDiv.innerHTML = 'Vui lòng chọn ít nhất một câu trả lời!';
            return; 
        }

        if (form.dataset.answered === 'true') { 
             return;
        }

        form.querySelectorAll(`input[name="${questionId}"]`).forEach(input => {
            input.disabled = true;
        });
        submitBtn.disabled = true;
        submitBtn.style.opacity = '0.6';
        submitBtn.style.cursor = 'not-allowed';

        let isCorrect = false;
        if (questionType === 'radio') {
            isCorrect = (selectedOptions[0] === correctAnswers);
        } else { /* checkbox */
            const sortedSelected = selectedOptions.sort();
            const sortedCorrect = correctAnswers.sort();
            isCorrect = (sortedSelected.length === sortedCorrect.length &&
                         sortedSelected.every((val, index) => val === sortedCorrect[index]));
        }

        form.querySelectorAll('.option-label').forEach(label => {
            label.classList.remove('correct', 'incorrect');
        });

        if (isCorrect) {
            feedbackDiv.className = 'correct-feedback';
            feedbackDiv.innerHTML = 'Chính xác!';
            correctCount++;
        } else {
            feedbackDiv.className = 'incorrect-feedback';
            feedbackDiv.innerHTML = 'Sai rồi. Câu trả lời đúng là: ' + 
                                    (Array.isArray(correctAnswers) ? correctAnswers.join('; ') : correctAnswers);
            incorrectCount++;
        }
        
        form.dataset.answered = 'true';

        form.querySelectorAll(`input[name="${questionId}"]`).forEach(input => {
            const label = input.closest('.option-label');
            if (label) {
                if (questionType === 'radio') {
                    if (input.value === correctAnswers) {
                        label.classList.add('correct');
                    } else if (input.checked) { 
                        label.classList.add('incorrect');
                    }
                } else { // checkbox
                    if (correctAnswers.includes(input.value)) {
                        label.classList.add('correct');
                    } else if (input.checked) { 
                        label.classList.add('incorrect');
                    }
                }
            }
        });
        updateScoreDisplay();
    }

    const modal = document.getElementById('explanationModal');
    const modalContentBody = document.getElementById('modalExplanationContent');
    const closeBtn = document.querySelector('.close-button');

    function showExplanation(questionId) {
        const explanationText = document.getElementById(`explanation_content_${questionId}`).innerHTML;
        modalContentBody.innerHTML = explanationText; // Hiển thị HTML giải thích
        modal.style.display = 'block';
    }

    closeBtn.onclick = function() {
        modal.style.display = 'none';
    }

    window.onclick = function(event) {
        if (event.target == modal) {
            modal.style.display = 'none';
        }
    }
    // Chạy hàm renderQuiz khi trang được tải hoặc làm mới
    document.addEventListener('DOMContentLoaded', renderQuiz);
    
        </script>
    </body>
    </html>
    
