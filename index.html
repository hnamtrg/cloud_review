
    <!DOCTYPE html>
    <html lang="vi">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Practice Test #2 - AWS Certified SysOps Administrator Associate</title>
        <style>
            
    body {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        line-height: 1.6;
        margin: 0;
        padding-top: 60px; /* Space for score-stats-container */
        background-color: #020617;
        color: #c7d1dd;
        font-size: 16px;
    }
    main {
        max-width: 850px;
        margin: 0 auto;
        padding: 20px;
    }
    h1 {
        text-align: center;
        color: #e0e7ff;
        margin-bottom: 20px;
        font-size: 2em;
    }
    .quiz-description {
        background-color: #0f172a;
        padding: 15px;
        border-radius: 8px;
        margin-bottom: 30px;
        border: 1px solid #1e293b;
        color: #a0aec0;
    }
    .question-container {
        background-color: #0f172a;
        border: 1px solid #1e293b;
        border-radius: 8px;
        padding: 20px;
        margin-bottom: 25px;
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    }
    .question-prompt {
        font-weight: 600;
        margin-bottom: 15px;
        color: #e0e7ff;
        font-size: 1.1em;
    }
    .options-container {
        display: flex;
        flex-direction: column;
        gap: 10px;
        margin-bottom: 15px;
    }
    .option-label {
        display: flex;
        align-items: center;
        gap: 10px;
        padding: 10px 15px;
        border: 1px solid #334155;
        border-radius: 6px;
        cursor: pointer;
        transition: background-color 0.2s, border-color 0.2s;
    }
    .option-label:hover {
        background-color: #1e293b;
    }
    input[type="radio"],
    input[type="checkbox"] {
        appearance: none; /* Hide default radio/checkbox */
        width: 20px;
        height: 20px;
        border: 2px solid #64748b;
        border-radius: 50%; /* For radio */
        background-color: transparent;
        display: grid;
        place-content: center;
        flex-shrink: 0;
        cursor: pointer;
    }
    input[type="checkbox"] {
        border-radius: 4px; /* For checkbox */
    }
    input[type="radio"]::before,
    input[type="checkbox"]::before {
        content: '';
        width: 10px;
        height: 10px;
        border-radius: 50%; /* For radio */
        transform: scale(0);
        transition: transform 0.2s ease-in-out;
        background-color: #3b82f6; /* Active color */
    }
    input[type="checkbox"]::before {
        border-radius: 2px; /* For checkbox */
    }
    input[type="radio"]:checked::before,
    input[type="checkbox"]:checked::before {
        transform: scale(1);
    }
    input[type="radio"]:checked,
    input[type="checkbox"]:checked {
        border-color: #3b82f6;
    }

    .submit-btn, .explanation-btn {
        background-color: #22c55e; /* Success green */
        color: white;
        border: none;
        padding: 10px 20px;
        border-radius: 6px;
        cursor: pointer;
        font-size: 1em;
        transition: background-color 0.2s;
        margin-top: 15px;
        width: fit-content;
    }
    .submit-btn:hover {
        background-color: #16a34a;
    }
    .explanation-btn {
        background-color: #3b82f6; /* Info blue */
        margin-left: 10px;
    }
    .explanation-btn:hover {
        background-color: #2563eb;
    }

    .correct-feedback {
        border: 2px solid #22c55e; /* Green for correct */
        background-color: #dcfce7;
        color: #15803d;
        padding: 10px;
        border-radius: 6px;
        margin-top: 10px;
    }
    .incorrect-feedback {
        border: 2px solid #ef4444; /* Red for incorrect */
        background-color: #fee2e2;
        color: #b91c1c;
        padding: 10px;
        border-radius: 6px;
        margin-top: 10px;
    }
    .option-label.correct {
        border-color: #22c55e;
        background-color: #2f453a;
    }
    .option-label.incorrect {
        border-color: #ef4444;
        background-color: #3f2f31;
    }

    /* Modal styles */
    .modal {
        display: none; /* Hidden by default */
        position: fixed; /* Stay in place */
        z-index: 1; /* Sit on top */
        left: 0;
        top: 0;
        width: 100%; /* Full width */
        height: 100%; /* Full height */
        overflow: auto; /* Enable scroll if needed */
        background-color: rgba(0,0,0,0.7); /* Black w/ opacity */
    }
    .modal-content {
        background-color: #0f172a;
        margin: 15% auto; /* 15% from the top and centered */
        padding: 20px;
        border: 1px solid #888;
        width: 80%; /* Could be more or less, depending on screen size */
        border-radius: 10px;
        position: relative;
        color: #e0e7ff;
    }
    .close-button {
        color: #aaa;
        float: right;
        font-size: 28px;
        font-weight: bold;
    }
    .close-button:hover,
    .close-button:focus {
        color: #fff;
        text-decoration: none;
        cursor: pointer;
    }
    .modal-body {
        margin-top: 20px;
    }
    .section-header {
        font-size: 1.4em;
        font-weight: bold;
        margin-top: 30px;
        margin-bottom: 20px;
        padding-bottom: 10px;
        border-bottom: 2px solid #64748b;
        color: #e0e7ff;
    }
    #score-stats-container {
        position: fixed;
        z-index: 10;
        top: 0;
        height: auto; /* Allow height to adjust */
        width: 100%;
        background-color: #020617;
        padding: 8px 16px; /* Tăng padding trên dưới để có thêm không gian */
        color: #e0e7ff;
        font-weight: 600;
        display: flex; /* Dùng flexbox */
        flex-wrap: wrap; /* Cho phép các mục xuống dòng nếu không đủ chỗ */
        align-items: center;
        justify-content: space-around; /* Phân bổ không gian đều */
        box-shadow: 0 2px 5px rgba(0,0,0,0.3);
        gap: 15px; /* Khoảng cách giữa các mục */
    }
    #score-stats-container div {
        flex-shrink: 0; /* Ngăn các mục co lại */
        white-space: nowrap; /* Giữ các nhãn trên một dòng */
    }
    .question-prompt img {
        max-width: 100%; /* Đảm bảo hình ảnh không tràn ra ngoài */
        height: auto;
        display: block; /* Để kiểm soát margin dễ dàng hơn */
        margin-top: 10px; /* Khoảng cách giữa văn bản và hình ảnh */
        border-radius: 5px;
    }
    
        </style>
    </head>
    <body>
        <section id="score-stats-container">
            <div class="score-item">Tổng số câu hỏi: <span id="total-questions-display">0</span></div>
            <div class="score-item">Trả lời đúng: <span id="correct-answers-display">0</span></div>
            <div class="score-item">Trả lời sai: <span id="wrong-answers-display">0</span></div>
        </section>
        <main>
            <h1>Practice Test #2 - AWS Certified SysOps Administrator Associate</h1>
            <div class="quiz-description">
                <h3>Giới thiệu về bài kiểm tra này:</h3>
                <p>About this practice exam: - questions order and response orders are randomized - you can only review the answer after finishing the exam due to how Udemy works - it consists of 65 questions, the duration is 130 minutes, the passing score is 720 ====== In case of an issue with a question: - ask a question in the Q&A - please take a screenshot of the question (because they're randomized) and attach it - we will get back to you as soon as possible and fix the issue Good luck, and happy learning!</p>
                <p><strong>Điểm đậu:</strong> 72%</p>
            </div>
            <section id="quiz-container"></section> 

            <div id="explanationModal" class="modal">
                <div class="modal-content">
                    <span class="close-button">&times;</span>
                    <h2>Giải thích</h2>
                    <div class="modal-body" id="modalExplanationContent">
                        </div>
                </div>
            </div>
        </main>
        <script>
            
    const allQuizData = [{"question": "<p>A development team working for a gaming company has deployed an application on EC2 and needs CloudWatch monitoring for the relevant metrics with a resolution of 1 minute in order to set alarms that can rapidly react to changes.</p>\n<p>As a SysOps Administrator, which of the following would you suggest as the MOST optimal solution?</p>\n", "answers": ["The development team should create and send a high resolution custom metric", "Enable EC2 detailed monitoring", "Use AWS Lambda to retrieve metrics often using the application /health route", "Use Systems Manager"], "correct_answer": "Enable EC2 detailed monitoring", "explanation": "<p>Correct option:</p>\n<p><strong>Enable EC2 detailed monitoring</strong></p>\n<p>Metrics are the fundamental concept in CloudWatch. A metric represents a time-ordered set of data points that are published to CloudWatch. Think of a metric as a variable to monitor, and the data points as representing the values of that variable over time.</p>\n<p>By default, your instance is enabled for basic monitoring. You can optionally enable detailed monitoring. After you enable detailed monitoring, the Amazon EC2 console displays monitoring graphs with a 1-minute period for the instance. So you can use EC2 detailed monitoring for the given use-case.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q1-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>The development team should create and send a high resolution custom metric</strong> - You can publish your own metrics to CloudWatch using the AWS CLI or an API. You can view statistical graphs of your published metrics with the AWS Management Console. Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds. Custom metrics need extra effort to capture and push the custom metrics to CloudWatch via the API or CLI, so it's not the MOST optimal solution for the given use-case.</p>\n<p><strong>Use AWS Lambda to retrieve metrics often using the application <code>/health</code> route</strong> - This option has been added as a distractor\nas you cannot retrieve performance metrics using the <code>/health</code> route via Lambda or otherwise.</p>\n<p><strong>Use Systems Manager</strong> - Using AWS Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. You cannot use Systems Manager to capture metrics for monitoring on CloudWatch.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>An AWS Lambda function written in Python shuts down all instances at night for cost savings purposes. Some of these instances should actually not be shut down, as the underlying applications transition to an unstable state afterward.</p>\n<p>How could you efficiently prevent the shut down of the critical instances?</p>\n", "answers": ["Store all the instance ids you should not shut down in SSM Parameter Store", "Use an environment variable for your AWS Lambda with a list of instances not to shut down", "Tag your EC2 instances and make the AWS Lambda script skip the shutdown if the tag is found", "Change the shutdown behavior of the EC2 instances and enable termination protection as well"], "correct_answer": "Tag your EC2 instances and make the AWS Lambda script skip the shutdown if the tag is found", "explanation": "<p>Correct option:</p>\n<p><strong>Tag your EC2 instances and make the AWS Lambda script skip the shutdown if the tag is found</strong></p>\n<p>A tag is a label that you assign to an AWS resource. Each tag consists of a key and an optional value, both of which you define.</p>\n<p>Tags enable you to categorize your AWS resources in different ways, for example, by purpose, owner, or environment. For example, you could define a set of tags for your account's Amazon EC2 instances that helps you track each instance's owner and stack level.</p>\n<p>AWS Resource Tags:\n<img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/Tag_Example.png\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html</a></p>\n<p>Tags allow for the cleanest solution here, as you can create instances in the future and tag them accordingly at the time of creation, without the need to modify your AWS Lambda function.</p>\n<p>Incorrect options:</p>\n<p><strong>Store all the instance ids you should not shut down in SSM Parameter Store</strong> - AWS Systems Manager Parameter Store (aka SSM Parameter Store) provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, EC2 instance IDs, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter.</p>\n<p>You would need to modify the parameter store values everytime you need to add/delete/modify the instances, so this option is not the right fit for the given use-case.</p>\n<p><strong>Use an environment variable for your AWS Lambda with a list of instances not to shut down</strong> - An environment variable is a pair of strings that are stored in a function's version-specific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request.</p>\n<p>You would need to modify the environment variables everytime you need to add/delete/modify the instances, so this option is not the right fit for the given use-case.</p>\n<p><strong>Change the shutdown behavior of the EC2 instances and enable termination protection as well</strong> - By default, you can terminate your instance using the Amazon EC2 console, command line interface, or API. To prevent your instance from being accidentally terminated using Amazon EC2, you can enable termination protection for the instance.</p>\n<p>You would need to modify the termination behaviour everytime you need to add/delete/modify an instance running the application, so this option is not the right fit for the given use-case.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#Using_ChangingDisableAPITermination\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#Using_ChangingDisableAPITermination</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>Your accounting application on an EC2 instance has the tendency to sometimes go into a panic and then the CPU Utilization of your EC2 instance runs at 100% for a long period of time. When this happens, someone has to manually intervene and restart your application for it to work properly again.</p>\n<p>How can you automate this in the most efficient way?</p>\n", "answers": ["Invoke an AWS Lambda function via a cron job that checks for the metric every minute and restarts the instance if a problem is found", "Create a CloudWatch Event when CPU Utilization reaches 100% and trigger an EC2 reboot action", "Create a CloudWatch Alarm when CPU Utilization reaches 100% for 3 periods of 5 minutes and trigger an EC2 reboot action", "Put your instance in an ASG and behind an ELB and enable ELB health check, so that the instance gets terminated upon problems and a new one gets created"], "correct_answer": "Create a CloudWatch Alarm when CPU Utilization reaches 100% for 3 periods of 5 minutes and trigger an EC2 reboot action", "explanation": "<p>Correct option:</p>\n<p><strong>Create a CloudWatch Alarm when CPU Utilization reaches 100% for 3 periods of 5 minutes and trigger an EC2 reboot action</strong></p>\n<p>Using Amazon CloudWatch alarm actions, you can create alarms that automatically stop, terminate, reboot, or recover your EC2 instances. You can use the stop or terminate actions to help you save money when you no longer need an instance to be running. You can use the reboot and recover actions to automatically reboot those instances or recover them onto new hardware if a system impairment occurs.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q3-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html</a></p>\n<p>Exam Alert:</p>\n<p>Please note that the CloudWatch Alarm action can only have the following targets and the exam may trick you into choosing other options:</p>\n<p>A valid CloudWatch action can be sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Auto Scaling action, or creating a Systems Manager OpsItem.</p>\n<p>Incorrect options:</p>\n<p><strong>Invoke an AWS Lambda function via a cron job that checks for the metric every minute and restarts the instance if a problem is found</strong> - Invoking an AWS Lambda function via a cron job to check the metric every minute represents a wasteful and inelegant solution. Using the CloudWatch Alarms action is a better solution for the given use-case.</p>\n<p><strong>Create a CloudWatch Event when CPU Utilization reaches 100% and trigger an EC2 reboot action</strong> - The CloudWatch metric - CPU Utilization - for an EC2 instance can be directly consumed to set up a CloudWatch Alarm and it cannot be used to invoke a CloudWatch Event.</p>\n<p><strong>Put your instance in an ASG and behind an ELB and enable ELB health check, so that the instance gets terminated upon problems and a new one gets created</strong> - Using an ELB incurs additional costs and the combination of ELB with ASG adds more complexity to the solution, so it's not an efficient solution for the given use-case.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>You are launching an EC2 instance and it fails with an <code>InsufficientInstanceCapacity</code> error. What should you do?</p>\n", "answers": ["Request for a service limit increase in AWS support console", "Try to launch the instance in another AZ", "Run Amazon Inspector on your EC2 instances to find out what's consuming the capacity", "Use AWS Trusted Advisor to understand the root cause of this issue"], "correct_answer": "Try to launch the instance in another AZ", "explanation": "<p>Correct option:</p>\n<p><strong>Try to launch the instance in another AZ</strong></p>\n<p>You get the InsufficientInstanceCapacity error when you try to launch a new instance or restart a stopped instance. <code>InsufficientInstanceCapacity</code> error implies that AWS does not have the capacity to serve your request.</p>\n<p>Exam Alert:</p>\n<p>Please make sure that you understand the differences between the <code>InsufficientInstanceCapacity</code> error and the <code>InstanceLimitExceeded</code> error as these are commonly probed in the exam.</p>\n<p>Troubleshooting instance launch issues:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q4-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Run Amazon Inspector on your EC2 instances to find out what's consuming the capacity</strong></p>\n<p><strong>Use AWS Trusted Advisor to understand the root cause of this issue</strong></p>\n<p>Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances.</p>\n<p>AWS Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices. Whether establishing new workflows, developing applications, or as part of ongoing improvement, you can take advantage of the recommendations provided by Trusted Advisor regularly to help keep your solutions provisioned optimally.</p>\n<p>Both these options have been added as distractors as neither Amazon Inspector nor AWS Trusted Advisor can help in solving the <code>InsufficientInstanceCapacity</code> error.</p>\n<p><strong>Request for a service limit increase in AWS support console</strong> - There is no need to request for a service limit increase to handle the <code>InsufficientInstanceCapacity</code> error. Please refer to the solutions described in the explanation above.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>You have set up a security group for your bastion host that only allows SSH from your IP:</p>\n<p><img src='\"https://udemy-images.s3.amazonaws.com:443/redactor/raw/2019-01-14_16-16-21-fd44fa7ccf07b7e4dc324bad5c220ee3.png\"'/></p>\n<p>Yet when looking at the VPC Flow Logs with AWS Athena, you see a lot of instances with the IP starting with <code>172.XXX.XXX.XXX</code> also being able to issue SSH commands.</p>\n<p>Why is that so?</p>\n", "answers": ["The IP rule should be 109.190.217.138/0", "The NACL rules are too open", "Someone is attacking your EC2 instance, use AWS Inspector to verify that", "The second rule allows EC2 instances from an entire security group to SSH into your bastion host"], "correct_answer": "The second rule allows EC2 instances from an entire security group to SSH into your bastion host", "explanation": "<p>Correct option:</p>\n<p><strong>The second rule allows EC2 instances from an entire security group to SSH into your bastion host</strong></p>\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance.</p>\n<p>The following are the characteristics of security group rules:</p>\n<p>By default, security groups allow all outbound traffic.</p>\n<p>Security group rules are always permissive; you can't create rules that deny access.</p>\n<p>Security groups are stateful</p>\n<p>As all the rules in the Security Group are aggregated together, therefore, the second rule allows all the instances in the mentioned Security Group to SSH into the bastion host.</p>\n<p>Incorrect options:</p>\n<p><strong>The IP rule should be <code>109.190.217.138/0</code></strong> - A CIDR block of /0 would allow access to any IP address between 0.0.0.0 and 255.255.255.255, while a CIDR block of /32 would only allow access to the IP address that precedes it. Therefore this option is not correct.</p>\n<p><strong>The NACL rules are too open</strong> - A Network Access Control List (NACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups to add an additional layer of security to your VPC.</p>\n<p>This option is a distractor. Even if the NACL rules are too open, the Security Group can be used to allow only the desired traffic.</p>\n<p><strong>Someone is attacking your EC2 instance, use AWS Inspector to verify that</strong> - Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances. This option has been added as a distractor.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>You are setting up a distributed in-memory database and you would like to auto-scale your Auto Scaling Group based on the average RAM usage of your EC2 instances.</p>\n<p>How can you achieve this?</p>\n", "answers": ["Enable EC2 detailed monitoring and use the CloudWatch metric RAMUtilization to setup scaling policies", "Auto Scale your ASG based on the CPUUtilization metric", "Push the RAMUtilization as a custom metric using custom scripts in EC2 and setup scaling policies using this metric", "Place the instances behind a load balancer, which will have the capability of monitoring the RAM of the EC2 instances with the smart balancing feature"], "correct_answer": "Push the RAMUtilization as a custom metric using custom scripts in EC2 and setup scaling policies using this metric", "explanation": "<p>Correct option:</p>\n<p><strong>Push the RAMUtilization as a custom metric using custom scripts in EC2 and setup scaling policies using this metric</strong></p>\n<p>You can publish your own metrics to CloudWatch using the AWS CLI or an API. You can view statistical graphs of your published metrics with the AWS Management Console. Metrics produced by AWS services are standard resolution by default.</p>\n<p>Each metric is one of the following:</p>\n<p>Standard resolution, with data having a one-minute granularity</p>\n<p>High resolution, with data at a granularity of one second</p>\n<p>When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p>\n<p>For example, the following command publishes a Buffers metric with two dimensions named InstanceId and InstanceType:\n<code>aws cloudwatch put-metric-data --metric-name Buffers --namespace MyNameSpace --unit Bytes --value 231434333 --dimensions InstanceId=1-23456789,InstanceType=m1.small</code></p>\n<p>For the given use-case, you can set up RAMUtilization as a custom metric and push it to CloudWatch to be further used in the scaling policy.</p>\n<p>Incorrect options:</p>\n<p><strong>Enable EC2 detailed monitoring and use the CloudWatch metric RAMUtilization to setup scaling policies</strong> - RAMUtilization is not available as an EC2 metric out-of-the-box, so this option is incorrect.</p>\n<p><strong>Auto Scale your ASG based on the CPUUtilization metric</strong> - The use-case refers to RAM usage as the relevant metric, so this option is not correct.</p>\n<p><strong>Place the instances behind a load balancer, which will have the capability of monitoring the RAM of the EC2 instances with the smart balancing feature</strong> - This option has been added as a distractor. There is no such thing as monitoring the RAM of the EC2 instance via the smart balancing feature.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>You work for a blockchain company and you have a ledger application that is memory intensive. It is exposed in an auto scaling group behind a load balancer. You would like to auto scale your application based on the number of users that you have.</p>\n<p>As a SysOps Administrator, which of the following would you recommend to meet this requirement?</p>\n", "answers": ["Push the RAM usage as a custom metric for the Load Balancer and auto scale based on that", "Use the RAM usage CloudWatch metric directly from the Load Balancer and auto scale based on that", "Auto Scale based on the number of connections CloudWatch metric for the Load Balancer", "Deploy a script on the Load Balancer to expose the number of users that are connected to your application as a custom CloudWatch metric"], "correct_answer": "Auto Scale based on the number of connections CloudWatch metric for the Load Balancer", "explanation": "<p>Correct option:</p>\n<p><strong>Auto Scale based on the number of connections CloudWatch metric for the Load Balancer</strong></p>\n<p>Elastic Load Balancing publishes metrics to Amazon CloudWatch for your load balancers and your targets. Elastic Load Balancing reports metrics to CloudWatch only when requests are flowing through the load balancer. If there are requests flowing through the load balancer, Elastic Load Balancing measures and sends its metrics in 60-second intervals. If there are no requests flowing through the load balancer or no data for a metric, the metric is not reported.</p>\n<p>For the given use-case, you can use the <code>ActiveConnectionCount</code> metric to auto scale. This metric represents the total number of concurrent TCP connections active from clients to the load balancer and from the load balancer to targets.</p>\n<p>Incorrect options:</p>\n<p><strong>Push the RAM usage as a custom metric for the Load Balancer and auto scale based on that</strong></p>\n<p><strong>Use the RAM usage CloudWatch metric directly from the Load Balancer and auto scale based on that</strong></p>\n<p>Both these options advocating RAM usage as a metric are incorrect since the use-case refers to the auto scaling criteria based on the number of users which is better represented by the <code>ActiveConnectionCount</code> metric.</p>\n<p><strong>Deploy a script on the Load Balancer to expose the number of users that are connected to your application as a custom CloudWatch metric</strong> - This option has been added as a distractor as you cannot deploy a script on a Load Balancer.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.amazonaws.cn/en_us/elasticloadbalancing/latest/application/load-balancer-cloudwatch-metrics.html\">https://docs.amazonaws.cn/en_us/elasticloadbalancing/latest/application/load-balancer-cloudwatch-metrics.html</a></p>\n", "section": "Domain 2: High Availability", "type": "radio"}, {"question": "<p>Your infrastructure runs a daily job to compute different metrics based on all the resources that are running in your account. The goal of this job is to provide you with metrics that will be pushed into a reporting Tableau dashboard and allow your SysOps Administrator to make good decisions to bring cost down. That job is fault tolerant and can be resumed at any time.</p>\n<p>Which EC2 instance type would you choose to keep costs low?</p>\n", "answers": ["EC2 On Demand", "EC2 Reserved Instances", "EC2 Spot Instances", "EC2 Placement Groups - Cluster"], "correct_answer": "EC2 Spot Instances", "explanation": "<p>Correct option:</p>\n<p><strong>EC2 Spot Instances</strong></p>\n<p>A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price (up to 90% off the On-Demand price). Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone is set by Amazon EC2 and adjusted gradually based on the long-term supply of and demand for Spot Instances. Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price.</p>\n<p>Since the job is fault-tolerant and can be resumed at any time, therefore Spot Instances are a good fit for the given use-case.</p>\n<p>Please see this detailed overview of various types of EC2 instances from a pricing perspective:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q8-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n<p>Incorrect options:</p>\n<p><strong>EC2 On-Demand</strong> - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. There is no long-term commitment required when you purchase On-Demand Instances. You pay only for the seconds that your On-Demand Instances are running. AWS recommends that you use On-Demand Instances for applications with short-term, irregular workloads that cannot be interrupted. On-Demand Instances would be costlier compared to Spot Instances for the given use-case.</p>\n<p><strong>EC2 Reserved Instances</strong> - Reserved instances reduce your Amazon EC2 costs by making a commitment to a consistent instance configuration, including instance type and Region, for a term of 1 or 3 years. For the given use case, this kind of annual commitment might not be a desirable option, as the daily job runs just for some given duration in a day which is better served via the Spot Instances.</p>\n<p><strong>EC2 Placement Groups - Cluster</strong> - When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies:</p>\n<p>Cluster – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.</p>\n<p>Partition – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka.</p>\n<p>Spread – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.</p>\n<p>This option has been added as a distractor as the cluster placement group would have no bearing on reducing the cost of the solution.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>When your baby products website started, it was running at low volume so your instances of type T2.micro were doing a fine job. After a while, your website exploded in popularity and now your ELB is seeing greater traffic. You had planned for the scaling events and your T2.micro instances are running in an auto scaling group. You also noticed that the EC2 instances are experiencing high CPU utilization because the CPU is being throttled and have very poor performance and your users are complaining. Hence the ASG is not scaling.</p>\n<p>What can you do to improve the performance of your application? (Select two)</p>\n", "answers": ["Your Load Balancer needs to be pre-warmed, and then your users will be happy", "Your T2.micro instances have run out of burst credit. Switch to a T2.large or m4.large instance type for greater stability", "You need to disable ELB stickiness", "You should enable T2 unlimited", "Change the ELB from an Application Load Balancer type to a Network Load Balancer type"], "correct_answer": ["Your T2.micro instances have run out of burst credit. Switch to a T2.large or m4.large instance type for greater stability", "You should enable T2 unlimited"], "explanation": "<p>Correct options:</p>\n<p>Traditional Amazon EC2 instance types provide fixed CPU utilization, while burstable performance instances provide a baseline level of CPU utilization with the ability to burst CPU utilization above the baseline level. The baseline utilization and ability to burst are governed by CPU credits.</p>\n<p>The CPU credits used depends on CPU utilization. The following scenarios all use one CPU credit:</p>\n<p>One vCPU at 100% utilization for one minute</p>\n<p>One vCPU at 50% utilization for two minutes</p>\n<p>Two vCPUs at 25% utilization for two minutes</p>\n<p>Burstable performance instances are designed to provide a baseline level of CPU performance with the ability to burst to a higher level when required by your workload. Burstable performance instances are well suited for a wide range of general-purpose applications. Examples include microservices, low-latency interactive applications, small and medium databases, virtual desktops, development, build, and stage environments, code repositories, and product prototypes.</p>\n<p>The given use-case highlights the fact that the instances are being throttled for CPU usage. To resolve CPU throttling, you can either enable T2/T3 Unlimited, or change the instance type.</p>\n<p><strong>Your T2.micro instances have run out of burst credit. Switch to a T2.large or m4.large instance type for greater stability</strong></p>\n<p>You can change to one of the following instance types:</p>\n<p>A T2 or T3 instance type with a higher CPU credit limit.</p>\n<p>An instance type that doesn't use a CPU credit bucket model.</p>\n<p><strong>You should enable T2 unlimited</strong></p>\n<p>T2 Unlimited instances can sustain high CPU performance for as long as a workload needs it. For most general-purpose workloads, T2 Unlimited instances will provide ample performance without any additional charges. If the instance needs to run at higher CPU utilization for a prolonged period, it can also do so at a flat additional rate of 5 cents per vCPU-hour.</p>\n<p>Incorrect options:</p>\n<p><strong>Your Load Balancer needs to be pre-warmed, and then your users will be happy</strong> - ELB is able to handle the vast majority of use cases for the customers without requiring \"pre-warming\" (configuring the load balancer to have the appropriate level of capacity based on expected traffic). In certain scenarios, such as when flash traffic is expected, or in the case where a load test cannot be configured to gradually increase traffic, AWS recommends that you contact AWS to have your load balancer \"pre-warmed\". AWS will then configure the load balancer to have the appropriate level of capacity based on the traffic that you expect. AWS will need to know the start and end dates of your tests or expected flash traffic, the expected request rate per second and the total size of the typical request/response that you will be testing.</p>\n<p><strong>You need to disable ELB stickiness</strong> - You can use the sticky session feature (also known as session affinity), which enables the load balancer to bind a user's session to a specific instance.</p>\n<p><strong>Change the ELB from an Application Load Balancer type to a Network Load Balancer type</strong></p>\n<p>These three options have been added as distractors since the constraint is with CPU throttling for the EC2 instances and not at the ELB level.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ec2-cpu-utilization-throttled/\">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-cpu-utilization-throttled/</a></p>\n<p><a href=\"https://aws.amazon.com/ec2/instance-types/t2/\">https://aws.amazon.com/ec2/instance-types/t2/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-credits-baseline-concepts.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-credits-baseline-concepts.html</a></p>\n<p><a href=\"https://aws.amazon.com/articles/best-practices-in-evaluating-elastic-load-balancing/#pre-warming\">https://aws.amazon.com/articles/best-practices-in-evaluating-elastic-load-balancing/#pre-warming</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "checkbox"}, {"question": "<p>You have designed an AMI in an account that is optimizing the legacy database technology your gambling company has developed. You wish to share that AMI with other AWS accounts that belong to the same organization.</p>\n<p>How do you do it?</p>\n", "answers": ["The AMI can be shared without doing anything special. Just provide the target account with your secret AMI id and they can start using it", "Edit the account list that can see the AMI from the AMI Console UI and the other accounts can start using it", "Edit the account list that can see the AMI from the AMI Console UI, and create an IAM role to be assumed by the other account using STS and the other accounts can start using it", "Create an IAM role to be assumed by the other account using STS and they can start accessing your AMI"], "correct_answer": "Edit the account list that can see the AMI from the AMI Console UI and the other accounts can start using it", "explanation": "<p>Correct option:</p>\n<p><strong>Edit the account list that can see the AMI from the AMI Console UI and the other accounts can start using it</strong></p>\n<p>An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance. You can launch multiple instances from a single AMI when you need multiple instances with the same configuration. You can use different AMIs to launch instances when you need instances with different configurations. A shared AMI is an AMI that a developer created and made available for other developers to use.</p>\n<p>You can share an AMI with specific AWS accounts without making the AMI public. All you need is the AWS account IDs. You can only share AMIs that have unencrypted volumes and volumes that are encrypted with a customer managed CMK. If you share an AMI with encrypted volumes, you must also share any CMKs used to encrypt them.</p>\n<p>For the given use-case, you can modify image permissions for the AMI and then specify the AWS account number of the user with whom you want to share the AMI.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q10-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>The AMI can be shared without doing anything special. Just provide the target account with your secret AMI id and they can start using it</strong></p>\n<p><strong>Edit the account list that can see the AMI from the AMI Console UI, and create an IAM role to be assumed by the other account using STS and the other accounts can start using it</strong></p>\n<p><strong>Create an IAM role to be assumed by the other account using STS and they can start accessing your AMI</strong></p>\n<p>The correct process is described in the explanation above. These three options contradict the given explanation, therefore these are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>You have developed a script that checks if all the instances that were launched in your AWS region are using an AMI ID that is authorized by your financial company standards. After creating and testing this script in your region, eu-west-1, you share it with your colleagues in New York and ask them to run the script. Upon running it, they come back to you and say it's not working, as all the instances are declared non compliant. Auditors manually checked the instances and they are indeed compliant.</p>\n<p>What did you do wrong?</p>\n", "answers": ["AMI IDs are region specific and a different list of compliant AMI ID should be provided based on the region of where the script is executed", "The script is missing IAM permissions. Edit the script to include the IAM policy from within and run it again", "The API call limit has been reached and the script did not handle that error case", "Your colleagues did not run the script properly. You write detailed documentation on what they did wrong"], "correct_answer": "AMI IDs are region specific and a different list of compliant AMI ID should be provided based on the region of where the script is executed", "explanation": "<p>Correct option:\n<strong>AMI IDs are region specific and a different list of compliant AMI ID should be provided based on the region of where the script is executed</strong></p>\n<p>An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an AMI when you launch an instance. You can launch multiple instances from a single AMI when you need multiple instances with the same configuration. You can use different AMIs to launch instances when you need instances with different configurations.</p>\n<p>As the AMI is tied to a specific AWS Region, you need to copy the AMI to other AWS Regions if required. You can copy an Amazon Machine Image (AMI) within or across AWS Regions using the AWS Management Console, the AWS Command Line Interface or SDKs, or the Amazon EC2 API, all of which support the CopyImage action. You can copy both Amazon EBS-backed AMIs and instance-store-backed AMIs. You can copy AMIs with encrypted snapshots and also change encryption status during the copy process.</p>\n<p>Copying a source AMI results in an identical but distinct target AMI with its own unique identifier. In the case of an Amazon EBS-backed AMI, each of its backing snapshots is, by default, copied to an identical but distinct target snapshot. (The sole exceptions are when you choose to encrypt or re-encrypt the snapshot.) You can change or deregister the source AMI with no effect on the target AMI.</p>\n<p>For the given use-case, you need to provide the unique identifiers for the AMIs created in other AWS regions so that those can be used in the validation script.</p>\n<p>Incorrect options:</p>\n<p><strong>The script is missing IAM permissions. Edit the script to include the IAM policy from within and run it again</strong></p>\n<p><strong>The API call limit has been reached and the script did not handle that error case</strong></p>\n<p><strong>Your colleagues did not run the script properly. You write detailed documentation on what they did wrong</strong></p>\n<p>These three options contradict the explanation given above, therefore these are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>Your company is experiencing an unusually high cost of Elastic IPs (EIPs) as most of them sit unassigned. Management would like to see a report showing the allocation of costs for these EIPs by department.</p>\n<p>What do you advise on doing?</p>\n", "answers": ["Define an AWS Config Rule per department and track cost", "Define Cost Allocation Tags and generate a report using Cost Explorer", "Create an AWS Lambda function which checks on an hourly basis the status of the EIPs and tracks using CloudTrail who is the last person who accessed them", "Use AWS Artifact to forbid people from leaving Elastic IPs unassigned for more than 20 minutes"], "correct_answer": "Define Cost Allocation Tags and generate a report using Cost Explorer", "explanation": "<p>Correct option:</p>\n<p><strong>Define Cost Allocation Tags and generate a report using Cost Explorer</strong></p>\n<p>An Elastic IP address is a static, public, IPv4 address allocated to your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. Elastic IPs do not change and remain allocated to your account until you delete them.</p>\n<p>To ensure efficient use of Elastic IP addresses, AWS imposes a small hourly charge if an Elastic IP address is not associated with a running instance, or if it is associated with a stopped instance or an unattached network interface. While your instance is running, you are not charged for one Elastic IP address associated with the instance, but you are charged for any additional Elastic IP addresses associated with the instance.</p>\n<p>A tag is a label that you or AWS assigns to an AWS resource. Each tag consists of a key and a value. For each resource, each tag key must be unique, and each tag key can have only one value. You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level. After you activate cost allocation tags, AWS uses the cost allocation tags to organize your resource costs on your cost allocation report, to make it easier for you to categorize and track your AWS costs.</p>\n<p><img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/Tag_Example.png\"/>\nvia - <a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html</a></p>\n<p>After you or AWS applies tags to your AWS resources (such as Amazon EC2 instances or Amazon S3 buckets) and you activate the tags in the Billing and Cost Management console, AWS generates a cost allocation report as a comma-separated value (CSV file) with your usage and costs grouped by your active tags. You can apply tags that represent business categories (such as cost centers, application names, or owners) to organize your costs across multiple services.</p>\n<p>For the given use-case, you can define department-wise cost allocation tags for EC2 instances with EIPs and then generate a report using Cost Explorer.</p>\n<p>Incorrect options:</p>\n<p><strong>Define an AWS Config Rule per department and track cost</strong> - AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - \"What did my AWS resource look like at xyz point in time?\". You cannot use AWS Config to track the cost of EC2 instances with EIPs.</p>\n<p><strong>Create an AWS Lambda function which checks on an hourly basis the status of the EIPs and tracks using CloudTrail who is the last person who accessed them</strong> - AWS Lambda lets you run code without provisioning or managing servers. This option has been added as a distractor, you cannot use AWS Config to track the cost of EC2 instances with EIPs.</p>\n<p><strong>Use AWS Artifact to forbid people from leaving Elastic IPs unassigned for more than 20 minutes</strong> - AWS Artifact is a self-service audit artifact retrieval portal that provides our customers with on-demand access to AWS’ compliance documentation and AWS agreements. You cannot use AWS Artifact to track usage for unassigned Elastic IPs.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>Your company has recently been attacked by a team of hackers, exploiting a vulnerability in your Windows OS. A new Windows patch has been released and it needs to be applied as soon as possible to all your instances.</p>\n<p>How can you do it?</p>\n", "answers": ["Use Artifact", "Deploy it using Amazon Inspector", "Deploy the patch using Systems Manager", "Patch the instances directly from the AWS Config interface"], "correct_answer": "Deploy the patch using Systems Manager", "explanation": "<p>Correct option:</p>\n<p><strong>Deploy the patch using Systems Manager</strong></p>\n<p>AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks such as running commands, managing patches, and configuring servers across AWS Cloud as well as on-premises infrastructure.</p>\n<p>With Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources.</p>\n<p>How Systems Manager Works:\n<img src=\"https://d1.awsstatic.com/AWS%20Systems%20Manager/product-page-diagram-AWS-Systems-Manager_how-it-works.2e7c5d550e833eed0f49fb8dc1872de23b09d183.png\"/>\nvia - <a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n<p>AWS Systems Manager patch manager helps you select and deploy operating system and software patches automatically across large groups of Amazon EC2 or on-premises instances. Through patch baselines, you can set rules to auto-approve select categories of patches to be installed, such as operating system or high severity patches, and you can specify a list of patches that override these rules and are automatically approved or rejected.</p>\n<p>For the given use-case, you can deploy the patches using Systems Manager which supports the patching of both Windows-based and Linux-based instances.</p>\n<p>Incorrect options:</p>\n<p><strong>Use Artifact</strong> - AWS Artifact is a self-service audit artifact retrieval portal that provides our customers with on-demand access to AWS’ compliance documentation and AWS agreements. You cannot use Artifact to deploy the patches on the instance.</p>\n<p><strong>Deploy it using Amazon Inspector</strong> - Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances. You cannot use Inspector to deploy the patches on the instance.</p>\n<p><strong>Patch the instances directly from the AWS Config interface</strong> - AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - \"What did my AWS resource look like at xyz point in time?\". You cannot use Config to deploy the patches on the instance.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/systems-manager/faq/\">https://aws.amazon.com/systems-manager/faq/</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>A healthcare company has machines both on their own data center for HIPAA compliance reasons, as well as on the AWS cloud to perform their big data analysis. All the instances must be managed using the same Puppet modules, as per the CTO decision.</p>\n<p>Which AWS service helps you in achieving that?</p>\n", "answers": ["Artifact", "GuardDuty", "Ansible", "OpsWorks"], "correct_answer": "OpsWorks", "explanation": "<p>Correct option:</p>\n<p><strong>OpsWorks</strong></p>\n<p>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments.</p>\n<p>AWS OpsWorks for Puppet Enterprise provides a managed Puppet Enterprise server and suite of automation tools that give you workflow automation for orchestration, automated provisioning, and visualization for traceability. The Puppet Enterprise server gives you full stack automation by handling operational tasks such as software and operating system configurations, package installations, database setups, and more. The Puppet Master centrally stores your configuration tasks and provides them to each node in your compute environment at any scale, from a few nodes to thousands of nodes.</p>\n<p>Incorrect options:</p>\n<p><strong>Artifact</strong> - AWS Artifact is a self-service audit artifact retrieval portal that provides our customers with on-demand access to AWS’ compliance documentation and AWS agreements. You cannot use Artifact to deploy the patches on the instance. You cannot use Artifact to manage the instances using Puppet modules.</p>\n<p><strong>GuardDuty</strong> - Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately. You cannot use GuardDuty to manage the instances using Puppet modules.</p>\n<p><strong>Ansible</strong> - Ansible is an open-source software provisioning, configuration management, and application-deployment tool enabling infrastructure as code. You cannot use Ansible to manage the instances using Puppet modules.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/opsworks/\">https://aws.amazon.com/opsworks/</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>You sell beauty products and have spent thousands of dollars on a new marketing campaign that declares that the 22nd of February is \"national beauty day\". The marketing campaign is showing very early signs of success and on the 22nd of February, you expect traffic to increase by 10x on your website. Your CEO wants to make sure your entire infrastructure is ready for the big day. Your website runs on Elastic Beanstalk, which deployed an ASG and an ELB.</p>\n<p>What should you do to ensure you can handle the traffic? (Select two)</p>\n", "answers": ["Open a support request with AWS to pre-warm the load balancer", "Open a support request with AWS to request a penetration testing authorization", "Open a support request to increase the upper limit on the number of the EC2 instance types you're using", "Enable Blue/Green Beanstalk Deployment", "Use a weighted policy record in Route 53"], "correct_answer": ["Open a support request with AWS to pre-warm the load balancer", "Open a support request to increase the upper limit on the number of the EC2 instance types you're using"], "explanation": "<p>Correct options:</p>\n<p><strong>Open a support request with AWS to pre-warm the load balancer</strong></p>\n<p>ELB is able to handle the vast majority of use cases for the customers without requiring \"pre-warming\" (configuring the load balancer to have the appropriate level of capacity based on expected traffic). In certain scenarios, such as when flash traffic is expected, or in the case where a load test cannot be configured to gradually increase traffic, AWS recommends that you contact AWS to have your load balancer \"pre-warmed\". AWS will then configure the load balancer to have the appropriate level of capacity based on the traffic that you expect. AWS will need to know the start and end dates of your tests or expected flash traffic, the expected request rate per second and the total size of the typical request/response that you will be testing.</p>\n<p><strong>Open a support request to increase the upper limit on the number of the EC2 instance types you're using</strong></p>\n<p>When you create your AWS account, AWS sets default quotas (also referred to as limits) on these resources on a per-Region basis. For example, there is a maximum number of instances that you can launch in a Region. So if you were to launch an instance in the US West (Oregon) Region, for example, the request must not cause your usage to exceed your maximum number of instances in that Region. If you get an <code>InstanceLimitExceeded</code> error when you try to launch a new instance or restart a stopped instance, you have reached the limit on the number of instances that you can launch in a Region.</p>\n<p>For the given use-case, you need to create a support request to raise the upper limit on the number of allowed EC2 instances from 20 to an appropriate number that allows the ASG to scale-out to meet the usage demand.</p>\n<p>Incorrect options:</p>\n<p><strong>Open a support request with AWS to request a penetration testing authorization</strong> - AWS allows its customers to carry out security assessments or penetration tests against their AWS infrastructure without prior approval for the permitted services. However, penetration testing cannot help meet the traffic demand for the given use-case.</p>\n<p><strong>Enable Blue/Green Beanstalk Deployment</strong> - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.</p>\n<p>You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time.</p>\n<p>Because AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly.</p>\n<p>Blue/Green Beanstalk Deployment will not help meet the traffic demand for the given use-case.</p>\n<p><strong>Use a weighted policy record in Route 53</strong> - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software.</p>\n<p>Weighted routing will not help meet the traffic demand for the given use-case.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/articles/best-practices-in-evaluating-elastic-load-balancing/#pre-warming\">https://aws.amazon.com/articles/best-practices-in-evaluating-elastic-load-balancing/#pre-warming</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n<p><a href=\"https://aws.amazon.com/security/penetration-testing/\">https://aws.amazon.com/security/penetration-testing/</a></p>\n", "section": "Domain 2: High Availability", "type": "checkbox"}, {"question": "<p>You operate a technology company which implements the Netflix chaos testing in production. This means that your EC2 instances in production can be terminated at any time, to test the resiliency of your applications. You have been experiencing a lot of 4XXs errors lately on your website that is exposed by a load balancer, and you realize you cannot SSH into the instances that were producing these errors as they have been terminated.</p>\n<p>How can you gain access to logs files that describe the list of HTTP requests that were inducing these problems?</p>\n", "answers": ["Use EC2 Rescue and bring back the log files from the wiped EBS volumes", "Contact AWS Support to recover the instances", "Enable the ELB access logs and query them using Athena", "Look at the EC2 default logs in CloudWatch Logs"], "correct_answer": "Enable the ELB access logs and query them using Athena", "explanation": "<p>Correct option:</p>\n<p><strong>Enable the ELB access logs and query them using Athena</strong></p>\n<p>ELB access logs is an optional feature of Elastic Load Balancing that is disabled by default. The access logs capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues. Each access log file is automatically encrypted using SSE-S3 before it is stored in your S3 bucket and decrypted when you access it. You do not need to take any action; the encryption and decryption is performed transparently.</p>\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n<p>For the given use-case, you can enable ELB access logs and then use Athena to analyze the 4XX errors from the log files stored in S3.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q16-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Use EC2 Rescue and bring back the log files from the wiped EBS volumes</strong> - EC2Rescue for Linux is an easy-to-use, open-source tool that can be run on an Amazon EC2 Linux instance to diagnose and troubleshoot common issues using its library of over 100 modules. A few generalized use cases for EC2Rescue for Linux include gathering syslog and package manager logs, collecting resource utilization data, and diagnosing/remediating known problematic kernel parameters and common OpenSSH issues.</p>\n<p>Since the use-case mentions that the instances have been terminated, so this tool cannot be used for such analysis.</p>\n<p><strong>Contact AWS Support to recover the instances</strong> - You cannot recover terminated EC2 instances.</p>\n<p><strong>Look at the EC2 default logs in CloudWatch Logs</strong> There are no default logs for EC2 in CloudWatch Logs. You need to set up the CloudWatch Agent to collect logs from Amazon EC2 instances and on-premises servers, running either Linux or Windows Server.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Linux-Server-EC2Rescue.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Linux-Server-EC2Rescue.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>You want a small website on EC2 instances under an ASG that has a target size varying between 2 and 10 instances. Your ASG has a policy to scale out when your target CPU Utilization is above 75%. It has been over 3 hours that the CPU Utilization of your ASG is 90% and still, no scaling out actions have taken place.</p>\n<p>What are the most likely reasons for this? (Select two)</p>\n", "answers": ["Your ASG is at maximum capacity already", "Your ASG Launch process has been suspended", "Your ASG AZRebalance process has been suspended", "AWS does not have the capacity for more of the requested EC2 instance types", "The warmup period of the EC2 instances has not elapsed yet"], "correct_answer": ["Your ASG is at maximum capacity already", "Your ASG Launch process has been suspended"], "explanation": "<p>Correct options:</p>\n<p>A scaling policy instructs Amazon EC2 Auto Scaling to track a specific CloudWatch metric, and it defines what action to take when the associated CloudWatch alarm is in ALARM. The metrics that are used to trigger an alarm are an aggregation of metrics coming from all of the instances in the Auto Scaling group. (For example, let's say you have an Auto Scaling group with two instances where one instance is at 60 percent CPU and the other is at 40 percent CPU. On average, they are at 50 percent CPU.) When the policy is in effect, Amazon EC2 Auto Scaling adjusts the group's desired capacity up or down when the alarm is triggered.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q18-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html</a></p>\n<p><strong>Your ASG is at maximum capacity already</strong></p>\n<p>You can configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity up front, it defaults to your minimum capacity.</p>\n<p>If your ASG is already at the maximum capacity, then it will not lead to a scale out action.</p>\n<p><strong>Your ASG Launch process has been suspended</strong></p>\n<p>For Amazon EC2 Auto Scaling, there are two primary process types: Launch and Terminate. The Launch process adds a new Amazon EC2 instance to an Auto Scaling group, increasing its capacity. The Terminate process removes an Amazon EC2 instance from the group, decreasing its capacity.</p>\n<p>If the Launch process is suspended, then your Auto Scaling group does not scale out for alarms or any scheduled actions that occur.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q18-i2.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Your ASG AZRebalance process has been suspended</strong> - For the AZRebalance process type, your Auto Scaling group does not attempt to redistribute instances after certain events. However, if a scale-out or scale-in event occurs, the scaling process still tries to balance the Availability Zones. Suspending the AZRebalance process will not stop the scale out from happening.</p>\n<p><strong>AWS does not have the capacity for more of the requested EC2 instance types</strong> - If this is the case, you will get an error message that says \"Your requested instance type (&lt;instance type&gt;) is not supported in your requested Availability Zone (&lt;instance Availability Zone&gt;). Please retry your request by not specifying an Availability Zone or choosing &lt;list of Availability Zones that supports the instance type&gt;. Launching EC2 instance failed.\".</p>\n<p><strong>The warmup period of the EC2 instances has not elapsed yet</strong> - The warmup period specifies the number of seconds that it takes for a newly launched instance to warm up. Until its specified warm-up time has expired, an instance is not counted toward the aggregated metrics of the Auto Scaling group. If the warmup period was set to a really high value, then new instances should still have been launched as the existing instance under warmup would not have contributed to the aggregated metrics yet. So this option is incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/ts-as-instancelaunchfailure.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/ts-as-instancelaunchfailure.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "checkbox"}, {"question": "<p>Some of your users' requests are completely being lost due to the metric <strong>SpilloverCount</strong> being greater than 0. This is now happening on a daily basis. Your application is running on EC2 instances managed by an ASG running behind a load balancer.</p>\n<p>What should you do to prevent this issue from happening?</p>\n", "answers": ["Pre-warm your load balancer", "Monitor for SurgeQueueLength and scale the ASG based on that metric", "Monitor for BackendConnectionErrors and scale the ASG based on that metric", "Enable ALB access logs and scale based on CloudWatch Logs"], "correct_answer": "Monitor for SurgeQueueLength and scale the ASG based on that metric", "explanation": "<p>Correct options:</p>\n<p><strong>Monitor for SurgeQueueLength and scale the ASG based on that metric</strong></p>\n<p>SpilloverCount represents the total number of requests that were rejected because the surge queue is full.</p>\n<p>The Classic Load Balancer metric SurgeQueueLength measures the total number of requests queued by your Classic Load Balancer. An increased maximum statistic for SurgeQueueLength indicates that backend systems aren't able to process incoming requests as fast as the requests are received. Possible reasons for a high SurgeQueueLength metric include:</p>\n<p>Overloaded Amazon Elastic Compute Cloud (Amazon EC2) instances behind the Classic Load Balancer that are unable to process all incoming requests</p>\n<p>Application dependency issues due to external resource performance issues</p>\n<p>Maximum allowable connection limits for instances</p>\n<p>To solve this use-case, you need to configure the Auto Scaling groups to scale your instances based on the SurgeQueueLength metric.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q17-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Pre-warm your load balancer</strong> - ELB is able to handle the vast majority of use cases for the customers without requiring \"pre-warming\" (configuring the load balancer to have the appropriate level of capacity based on expected traffic). In certain scenarios, such as when flash traffic is expected, or in the case where a load test cannot be configured to gradually increase traffic, AWS recommends that you contact AWS to have your load balancer \"pre-warmed\". AWS will then configure the load balancer to have the appropriate level of capacity based on the traffic that you expect. AWS will need to know the start and end dates of your tests or expected flash traffic, the expected request rate per second and the total size of the typical request/response that you will be testing.</p>\n<p>Here the backend systems aren't able to process incoming requests as fast as the requests are received, so pre-warming your load balancer will not help.</p>\n<p><strong>Monitor for BackendConnectionErrors and scale the ASG based on that metric</strong> - BackendConnectionErrors represents the number of connections that were not successfully established between the load balancer and the registered instances. Because the load balancer retries the connection when there are errors, this count can exceed the request rate. Note that this count also includes any connection errors related to health checks. Scaling the ASG based on BackendConnectionErrors will not solve the use-case.</p>\n<p><strong>Enable ALB access logs and scale based on CloudWatch Logs</strong> - This option has been added as a distractor as you cannot scale based on CloudWatch Logs.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elb-capacity-troubleshooting/\">https://aws.amazon.com/premiumsupport/knowledge-center/elb-capacity-troubleshooting/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>You have an ASG in which the Terminate process is suspended. Your ASG goes into a rebalance, what will happen?</p>\n", "answers": ["The rebalance will not start, as the terminate process is suspended", "The rebalance will start and the EC2 instances will fail to get launched", "The rebalance will start and the EC2 instances will launch, the ASG will grow up to 10% of its size. After a bit, the instances will get terminated as the ASG is at over capacity", "The rebalance will start and the EC2 instances will launch, the ASG will grow up to 10% of its size. The instances will not get terminated"], "correct_answer": "The rebalance will start and the EC2 instances will launch, the ASG will grow up to 10% of its size. The instances will not get terminated", "explanation": "<p>Correct option:</p>\n<p><strong>The rebalance will start and the EC2 instances will launch, the ASG will grow up to 10% of its size. The instances will not get terminated</strong></p>\n<p>If the Terminate process is suspended, your Auto Scaling group does not scale in for alarms or scheduled actions that occur. While the Terminate process is suspended and the AZRebalance process is still active then AZRebalance will not function properly. AZRebalance will be able to launch new instances without terminating the old ones. This could cause your Auto Scaling group to grow up to 10 percent larger than its maximum size, because this is allowed temporarily during rebalancing activities.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q19-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>The rebalance will not start, as the terminate process is suspended</strong></p>\n<p><strong>The rebalance will start and the EC2 instances will fail to get launched</strong></p>\n<p><strong>The rebalance will start and the EC2 instances will launch, the ASG will grow up to 10% of its size. After a bit, the instances will get terminated as the ASG is at over capacity</strong></p>\n<p>These three options contradict the explanation provided above, so all these options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>You run a full e-commerce website on Elastic Beanstalk, which provisions an Application Load Balancer in a public subnet, an Auto Scaling Group that spans 3 private subnets, and an RDS database in Multi-AZ mode in two private subnets. The Load Balancer is able to access your application, and your application can access the database.</p>\n<p>Yet, you have trouble patching your EC2 instances using SSM as these instances cannot access the internet. What's the issue?</p>\n", "answers": ["Deploy the instances in the public subnet instead. Private subnets cannot access the internet", "Deploy a NAT Gateway in the public subnet and add entries to your route table", "Open up security groups on the EC2 instances", "Deploy an Internet Gateway in the public subnet and add entries to your route table"], "correct_answer": "Deploy a NAT Gateway in the public subnet and add entries to your route table", "explanation": "<p>Correct option:</p>\n<p><strong>Deploy a NAT Gateway in the public subnet and add entries to your route table</strong></p>\n<p>You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. A NAT gateway has the following characteristics and limitations:</p>\n<ol>\n<li><p>A NAT gateway supports 5 Gbps of bandwidth and automatically scales up to 45 Gbps.</p></li>\n<li><p>You can associate exactly one Elastic IP address with a NAT gateway.</p></li>\n<li><p>A NAT gateway supports the following protocols: TCP, UDP, and ICMP.</p></li>\n<li><p>You cannot associate a security group with a NAT gateway.</p></li>\n<li><p>You can use a network ACL to control the traffic to and from the subnet in which the NAT gateway is located.</p></li>\n<li><p>A NAT gateway can support up to 55,000 simultaneous connections to each unique destination.</p></li>\n</ol>\n<p>Therefore you must use a NAT Gateway in your public subnet in order to provide internet access to your instances in your private subnets. You also need to set up the appropriate entries in the route table of the private subnets. You are charged for creating and using a NAT gateway in your account. NAT gateway hourly usage and data processing rates apply.</p>\n<p>Comparison of NAT instances and NAT gateways:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q20-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Deploy the instances in the public subnet instead. Private subnets cannot access the internet</strong> - Deploying the instances in private subnet would jeopardize the security of the EC2 instances, so this option is not correct.</p>\n<p><strong>Open up security groups on the EC2 instances</strong> - A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance.</p>\n<p>Opening up the security groups would jeopardize the security of the EC2 instances, so this option is not correct.</p>\n<p><strong>Deploy an Internet Gateway in the public subnet and add entries to your route table</strong> - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. Internet Gateways must be deployed in a public subnet and the corresponding entry should be added in the route table.</p>\n<p>This option has been added as a distractor as it just states the necessary conditions for a public subnet to have internet access.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>Your application has complex runtime and OS dependencies and is taking a really long time to be deployed on Elastic Beanstalk. You cannot sacrifice application availability.</p>\n<p>What should you do to improve the deployment time? (Select two)</p>\n", "answers": ["Create a Golden AMI with your application", "Create a new beanstalk environment for each application and apply blue / green deployment patterns", "Use rolling with additional batch", "Upgrade the EC2 instance type", "Use all at once deployment pattern"], "correct_answer": ["Create a Golden AMI with your application", "Create a new beanstalk environment for each application and apply blue / green deployment patterns"], "explanation": "<p>Correct options:</p>\n<p><strong>Create a Golden AMI with your application</strong></p>\n<p>A Golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc. For the given use-case, you can have the complex runtime and OS dependencies already setup via the golden AMI.</p>\n<p>Golden AMI Pipeline:\n<img src=\"https://d2908q01vomqb2.cloudfront.net/761f22b2c1593d0bb87e0b606f990ba4974706de/2018/05/16/GAP-1.png\"/>\nvia - <a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n<p><strong>Create a new beanstalk environment for each application and apply blue / green deployment patterns</strong></p>\n<p>Elastic Beanstalk provides several deployment policies and settings.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q21-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n<p>Since AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly.</p>\n<p><img src=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/aeb-env-swap-url.png\"/>\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p>\n<p>A blue/green deployment is also required when you want to update an environment to an incompatible platform version.</p>\n<p>Incorrect options:</p>\n<p><strong>Use rolling with additional batch</strong> - With rolling update, your application is deployed to your environment one batch of instances at a time. Most bandwidth is retained throughout the deployment. This also avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. Since the use-case mandates a short deployment time, this option is ruled out.</p>\n<p><strong>Upgrade the EC2 instance type</strong> - An upgraded instance type may only marginally improve the deployment time.</p>\n<p><strong>Use all at once deployment pattern</strong> - With all at once deployment, Elastic Beanstalk deploys the new application version to each instance. Then, the web proxy or application server might need to restart. As a result, your application might be unavailable to users (or have low availability) for a short time. Since the use-case mandates high availability, this option is ruled out.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/\">https://aws.amazon.com/blogs/awsmarketplace/announcing-the-golden-ami-pipeline/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "checkbox"}, {"question": "<p>You are deploying an application and use the cfn-init and cfn-signal script to ensure the application is properly deployed before signaling to CloudFormation the success of your stack deployment. Right now, every time you deploy, CloudFormation completes successfully, even though the instance is still executing the cfn-init script.</p>\n<p>As a SysOps Administrator, which of the following would you identify as the root cause behind the issue?</p>\n", "answers": ["You forgot the Wait Condition", "You did not disable Rollbacks", "You forgot to include the cfn-signal command in your user data", "You forgot to include a deletion policy"], "correct_answer": "You forgot the Wait Condition", "explanation": "<p>Correct option:</p>\n<p><strong>You forgot the Wait Condition</strong></p>\n<p>The cfn-init helper script reads template metadata from the AWS::CloudFormation::Init key and acts accordingly to:</p>\n<p>Fetch and parse metadata from AWS CloudFormation</p>\n<p>Install packages</p>\n<p>Write files to disk</p>\n<p>Enable/disable and start/stop services</p>\n<p>The cfn-signal helper script signals AWS CloudFormation to indicate whether Amazon EC2 instances have been successfully created or updated. If you install and configure software applications on instances, you can signal AWS CloudFormation when those software applications are ready.</p>\n<p>You can use the wait condition and wait condition handle to make AWS CloudFormation pause the creation of a stack and wait for a signal before it continues to create the stack. For example, you might want to download and configure applications on an Amazon EC2 instance before considering the creation of that Amazon EC2 instance complete.</p>\n<p>AWS CloudFormation creates a wait condition just like any other resource. When AWS CloudFormation creates a wait condition, it reports the wait condition’s status as CREATE_IN_PROGRESS and waits until it receives the requisite number of success signals or the wait condition’s timeout period has expired. If AWS CloudFormation receives the requisite number of success signals before the time out period expires, it continues creating the stack; otherwise, it sets the wait condition’s status to CREATE_FAILED and rolls the stack back.</p>\n<p>Incorrect options:</p>\n<p><strong>You did not disable Rollbacks</strong> - Enabling/disabling rollbacks has no impact on the ability to track the status of the cfn-init script.</p>\n<p><strong>You forgot to include the cfn-signal command in your user data</strong> - This is a distractor as the cfn-signal command is managed via CloudFormation and not via the user data.</p>\n<p><strong>You forgot to include a deletion policy</strong> - This is again a distractor as a deletion policy has nothing to do with tracking the status of the cfn-init script.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-helper-scripts-reference.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-helper-scripts-reference.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-waitcondition.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-waitcondition.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>As part of the best practices for DevOps, all your infrastructure is deployed using CloudFormation. This includes EBS volumes. When the CloudFormation stacks are deleted, it is mandatory to keep a snapshot of the EBS volumes for backup and compliance purposes.</p>\n<p>How can you achieve this using CloudFormation?</p>\n", "answers": ["Enable termination protection", "Use cfn helper scripts and Wait Conditions upon stack deletion", "Use DeletionPolicy=Snapshot", "Reference the EBS volume as a stack output"], "correct_answer": "Use DeletionPolicy=Snapshot", "explanation": "<p>Correct option:</p>\n<p><strong>Use DeletionPolicy=Snapshot</strong></p>\n<p>To control how AWS CloudFormation handles the EBS volume when the stack is deleted, set a deletion policy for your volume. You can choose to retain the volume, to delete the volume, or to create a snapshot of the volume.</p>\n<p>Here is the sample YAML:</p>\n<pre><code>NewVolume:\n  Type: AWS::EC2::Volume\n  Properties:\n    Size: 100\n    Encrypted: true\n    AvailabilityZone: !GetAtt Ec2Instance.AvailabilityZone\n    Tags:\n      - Key: MyTag\n        Value: TagValue\n  DeletionPolicy: Snapshot\n</code></pre>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q23-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-ebs-volume.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-ebs-volume.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Enable termination protection</strong> - You can prevent a stack from being accidentally deleted by enabling termination protection on the stack. If a user attempts to delete a stack with termination protection enabled, the deletion fails and the stack--including its status--remains unchanged. You can enable termination protection on a stack when you create it. Termination protection on stacks is disabled by default. You can set termination protection on a stack with any status except DELETE_IN_PROGRESS or DELETE_COMPLETE.</p>\n<p><strong>Use cfn helper scripts and Wait Conditions upon stack deletion</strong> - The cfn helper scripts such as cfn-init, cfn-signal etc help in installing packages or to indicate whether Amazon EC2 instances have been successfully created or updated. You cannot use these scripts to mandatorily keep a snapshot of the EBS volume.</p>\n<p><strong>Reference the EBS volume as a stack output</strong> - The optional Outputs section for a CloudFormation stack declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. You should note that a stack that is referenced by another stack cannot be deleted and it cannot modify or remove the exported value. Just by referencing the EBS volume as a stack output, you will not be able to enforce the snapshot of the EBS volume.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-ebs-volume.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-ebs-volume.html</a></p>\n<p><a href=\"https://aws.amazon.com/blogs/aws/aws-cloudformation-update-yaml-cross-stack-references-simplified-substitution/\">https://aws.amazon.com/blogs/aws/aws-cloudformation-update-yaml-cross-stack-references-simplified-substitution/</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>You are provisioning an internal full LAMP stack using CloudFormation, and the EC2 instance gets configured automatically using the cfn helper scripts, such as cfn-init and cfn-signal. The stack creation fails as CloudFormation fails to receive a signal from your EC2 instance.</p>\n<p>What are the possible reasons for this? (Select two)</p>\n", "answers": ["The subnet where the application is deployed does not have a network route to the CloudFormation service through a NAT Gateway or Internet Gateway", "The EC2 instance does not have a proper IAM role allowing to signal the success to CloudFormation", "The cfn-signal script does not get executed before the timeout of the wait condition", "AWS is experiencing an Insufficient Capacity for the instance type you requested", "The cfn-init script failed"], "correct_answer": ["The subnet where the application is deployed does not have a network route to the CloudFormation service through a NAT Gateway or Internet Gateway", "The cfn-signal script does not get executed before the timeout of the wait condition"], "explanation": "<p>Correct options:</p>\n<p><strong>The subnet where the application is deployed does not have a network route to the CloudFormation service through a NAT Gateway or Internet Gateway</strong> - As the use-case mentions an internal full LAMP stack, this implies that the stack is to be deployed in a private subnet. Now this private subnet must have a network route to the CloudFormation service through a NAT Gateway or Internet Gateway.</p>\n<p>You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances.</p>\n<p>An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. Internet Gateways must be deployed in a public subnet and the corresponding entry should be added in the route table.</p>\n<p><strong>The cfn-signal script does not get executed before the timeout of the wait condition</strong></p>\n<p>The Timeout property determines how long AWS CloudFormation waits for the requisite number of success signals. Timeout is a minimum-bound property, meaning the timeout occurs no sooner than the time you specify, but can occur shortly thereafter. The maximum time that you can specify is 43200 seconds (12 hours ). For the given scenario, the stack creation can fail as CloudFormation may fail to receive a signal from your EC2 instance if the Timeout property is set to a low value.</p>\n<p>Incorrect options:</p>\n<p><strong>The EC2 instance does not have a proper IAM role allowing to signal the success to CloudFormation</strong> - You do not need an IAM role to use cfn-signal.</p>\n<p><strong>AWS is experiencing an Insufficient Capacity for the instance type you requested</strong> - In case of Insufficient Capacity, the instance would have not been created and the CloudFormation stack would have failed altogether.</p>\n<p><strong>The cfn-init script failed</strong> - The cfn-init script failure should still be followed by the cfn-signal script, which would have sent a signal to CloudFormation nonetheless.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-waitcondition.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-waitcondition.html</a></p>\n", "section": "Domain 6: Networking", "type": "checkbox"}, {"question": "<p>You are developing a new CloudFormation stack and writing some very complex cfn-init code. The code fails and you would like to debug why. When reading the documentation, you see all the logs are in the file <code>/var/cfn/cfn-init-output.log</code> and will give you more information as to why the instance provisioning is failing. But you realize that it is impossible for you to gain access to this file as the CloudFormation stack always terminates the EC2 instance when the creation fails.</p>\n<p>What can you do to access these logs files, while not changing the way your EC2 instance works, and ensuring you can debug your instance over a course of 24 hours?</p>\n", "answers": ["Install the CloudWatch logs agent, create a new IAM role and assign it to the EC2 instance, and send the logs directly to CloudWatch Logs", "Set OnFailure=DO_NOTHING", "Increase the Wait Timeout to 2 hours", "Enable VPC Flow Logs and intercept the cfn-init log file"], "correct_answer": "Set OnFailure=DO_NOTHING", "explanation": "<p>Correct option:</p>\n<p><strong>Set OnFailure=DO_NOTHING</strong></p>\n<p>You can use the OnFailure property of the CloudFormation CreateStack call for this use-case. The OnFailure property determines what action will be taken if stack creation fails. This must be one of: DO_NOTHING, ROLLBACK, or DELETE. You can specify either OnFailure or DisableRollback, but not both.</p>\n<p>Using the OnFailure property, you can prevent the termination of the EC2 instances created by the CloudFormation stack.</p>\n<p>Incorrect options:</p>\n<p><strong>Install the CloudWatch logs agent, create a new IAM role and assign it to the EC2 instance, and send the logs directly to CloudWatch Logs</strong></p>\n<p><strong>Enable VPC Flow Logs and intercept the cfn-init log file</strong></p>\n<p>As the use-case mentions that there should be no changes done to the EC2 instance, so both these options are ruled out since these involve installing or configuring additional software.</p>\n<p><strong>Increase the Wait Timeout to 2 hours</strong> - The wait timeout works with cfn-signal, however, the given issue is related to cfn-init wherein some underlying code is failing. Therefore increasing wait timeout is not a valid solution for this scenario.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_CreateStack.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_CreateStack.html</a></p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudformation-prevent-rollback-failure/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudformation-prevent-rollback-failure/</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>Your gp2 drive of 8TB is reaching its peak performance of 10,000 IOPS, while being almost fully utilized.</p>\n<p>How can you increase the performance while keeping the costs at the same level?</p>\n", "answers": ["Convert the gp2 drive to io1 and increase the PIOPS", "Create two 4 TB gp2 drives and mount them in RAID 0 on the EC2 instance", "Create two 4 TB gp2 drives and mount them in RAID 1 on the EC2 instance", "Enable burst mode on the gp2 drive"], "correct_answer": "Create two 4 TB gp2 drives and mount them in RAID 0 on the EC2 instance", "explanation": "<p>Correct option:</p>\n<p><strong>Create two 4 TB gp2 drives and mount them in RAID 0 on the EC2 instance</strong></p>\n<p>With Amazon EBS, you can use any of the standard RAID configurations that you can use with a traditional bare metal server, as long as that particular RAID configuration is supported by the operating system for your instance. This is because all RAID is accomplished at the software level.</p>\n<p>For greater I/O performance than you can achieve with a single volume, RAID 0 can stripe multiple volumes together; for on-instance redundancy, RAID 1 can mirror two volumes together. So for the given use-case, to increase the performance, you should use RAID 0.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q26-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Convert the gp2 drive to io1 and increase the PIOPS</strong> - Changing the gp2 drive to io1 entails more costs as the pricing is $0.10 per GB-month of provisioned storage for gp2 and $0.125 per GB-month of provisioned storage for io1. So this option is ruled out.</p>\n<p><strong>Create two 4 TB gp2 drives and mount them in RAID 1 on the EC2 instance</strong> - You should use RAID 1 when fault tolerance is more important than I/O performance.</p>\n<p><strong>Enable burst mode on the gp2 drive</strong> - gp2 volumes can burst to 3,000 IOPS for extended periods of time. This option is a distractor as you do not need to enable the burst mode for gp2 volumes as it's available by default.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q26-i2.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/ebs/pricing/\">https://aws.amazon.com/ebs/pricing/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p>\n<p><a href=\"https://aws.amazon.com/blogs/database/understanding-burst-vs-baseline-performance-with-amazon-rds-and-gp2/\">https://aws.amazon.com/blogs/database/understanding-burst-vs-baseline-performance-with-amazon-rds-and-gp2/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>Which of the following services allows for an in-place switch from unencrypted to encrypted without impacting existing operations?</p>\n", "answers": ["S3", "RDS", "EBS", "EFS"], "correct_answer": "S3", "explanation": "<p>Correct options:</p>\n<p><strong>S3</strong></p>\n<p>Amazon S3 default encryption provides a way to set the default encryption behavior for an S3 bucket. You can set default encryption on a bucket so that all new objects are encrypted when they are stored in the bucket. The objects are encrypted using server-side encryption with either Amazon S3-managed keys (SSE-S3) or customer master keys (CMKs) stored in AWS Key Management Service (AWS KMS). When you use server-side encryption, Amazon S3 encrypts an object before saving it to disk and decrypts it when you download the objects.</p>\n<p>There is no change to the encryption of the objects that existed in the bucket before default encryption was enabled.</p>\n<p>So for the given use-case, you can continue to use the same S3 buckets without impacting operations.</p>\n<p>Incorrect options:</p>\n<p><strong>RDS</strong> - You can only enable encryption for an Amazon RDS DB instance when you create it, not after the DB instance is created.</p>\n<p>However, because you can encrypt a copy of an unencrypted snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot, and thus you have an encrypted copy of your original DB instance.</p>\n<p><strong>EBS</strong> - There is no direct way to encrypt an existing unencrypted volume or snapshot, you can encrypt them by creating either a volume or a snapshot. If you enabled encryption by default, Amazon EBS encrypts the resulting new volume or snapshot using your default key for EBS encryption.</p>\n<p><strong>EFS</strong> - You can enable encryption of data at rest when creating an Amazon EFS file system. Once the file system is created, you cannot modify the file system to be unencrypted or vice-versa.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>How can you enforce SSE-S3 encryption on all the files uploaded into your example S3 bucket?</p>\n", "answers": ["Using the \"Default Encryption\" setting in AWS S3", "Use the following S3 bucket policy: {\n    \"Statement\":[\n        {\n            \"Action\": \"s3:*\",\n            \"Effect\":\"Deny\",\n            \"Principal\": \"*\",\n            \"Resource\":\"arn:aws:s3:::bucketname/*\",\n            \"Condition\":{\n                \"Bool\":\n                { \"aws:SecureTransport\": false }\n            }\n        }\n    ]\n}", "Use the following S3 bucket policy: {\n    \"Statement\":[\n        {\n            \"Action\": \"s3:*\",\n            \"Effect\":\"Deny\",\n            \"Principal\": \"*\",\n            \"Resource\":\"arn:aws:s3:::bucketname/*\",\n            \"Condition\":{\n                \"Bool\":\n                { \"aws:SecureTransport\": true }\n            }\n        }\n    ]\n}", "Use an encrypted CloudFront distribution in front of your S3 bucket"], "correct_answer": "Using the \"Default Encryption\" setting in AWS S3", "explanation": "<p>Correct option:</p>\n<p><strong>Using the \"Default Encryption\" setting in AWS S3</strong></p>\n<p>Amazon S3 default encryption provides a way to set the default encryption behavior for an S3 bucket. You can set default encryption on a bucket so that all new objects are encrypted when they are stored in the bucket. The objects are encrypted using server-side encryption with either Amazon S3-managed keys (SSE-S3) or customer master keys (CMKs) stored in AWS Key Management Service (AWS KMS).</p>\n<p>When you use server-side encryption, Amazon S3 encrypts an object before saving it to disk and decrypts it when you download the objects.</p>\n<p>Incorrect options:</p>\n<p><strong>Use the following S3 bucket policy</strong>:</p>\n<pre><code>{\n    \"Statement\":[\n        {\n            \"Action\": \"s3:*\",\n            \"Effect\":\"Deny\",\n            \"Principal\": \"*\",\n            \"Resource\":\"arn:aws:s3:::bucketname/*\",\n            \"Condition\":{\n                \"Bool\":\n                { \"aws:SecureTransport\": false }\n            }\n        }\n    ]\n}\n</code></pre>\n<p>The above bucket policy only denies access to HTTP requests for any action on the S3 bucket <code>bucketname</code>. It cannot help enforce SSE-S3 encryption on S3. So it's not the right fit for the given use-case.</p>\n<p><strong>Use the following S3 bucket policy</strong>:</p>\n<pre><code>{\n    \"Statement\":[\n        {\n            \"Action\": \"s3:*\",\n            \"Effect\":\"Deny\",\n            \"Principal\": \"*\",\n            \"Resource\":\"arn:aws:s3:::bucketname/*\",\n            \"Condition\":{\n                \"Bool\":\n                { \"aws:SecureTransport\": true }\n            }\n        }\n    ]\n}\n</code></pre>\n<p>The above bucket policy only denies access to HTTPS requests for any action on the S3 bucket <code>bucketname</code>. It cannot help enforce SSE-S3 encryption on S3. So it's not the right fit for the given use-case.</p>\n<p><strong>Use an encrypted CloudFront distribution in front of your S3 bucket</strong> - This option is a distractor as you cannot enforce SSE-S3 encryption on S3 by using in-transit or at-rest encryption for CloudFront.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html</a></p>\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/\">https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/data-protection-summary.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/data-protection-summary.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>You are in S3 and have deleted all the files in it. As you can see, the bucket is empty:</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q29-i1.jpg\"/></p>\n<p>You have tried to delete the bucket afterwards and it fails with an error saying the bucket is not empty.</p>\n<p>What's the issue?</p>\n", "answers": ["S3 is eventually consistent. Wait two minutes and retry, it will work then", "S3 versioning is enabled and delete markers are still present in the bucket", "An S3 bucket policy is set up and it prevents bucket deletion", "Some files are in Glacier"], "correct_answer": "S3 versioning is enabled and delete markers are still present in the bucket", "explanation": "<p>Correct option:</p>\n<p><strong>S3 versioning is enabled and delete markers are still present in the bucket</strong></p>\n<p>S3 Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures. When you enable versioning for a bucket, if Amazon S3 receives multiple write requests for the same object simultaneously, it stores all of the objects.</p>\n<p>If you overwrite an object, it results in a new object version in the bucket. You can always restore the previous version.</p>\n<p>If you delete an object, instead of removing it permanently, Amazon S3 inserts a delete marker, which becomes the current object version.</p>\n<p>So for the given use-case, as delete markers are still present in the bucket, therefore you get the error saying the bucket is not empty.</p>\n<p>Incorrect options:</p>\n<p><strong>S3 is eventually consistent. Wait two minutes and retry, it will work then</strong> - This is a made-up option. Amazon S3 provides strong read-after-write consistency for PUTs and DELETEs of objects in your Amazon S3 bucket in all AWS Regions.</p>\n<p><strong>An S3 bucket policy is set up and it prevents bucket deletion</strong> - You could set up a bucket policy to prevent bucket deletion, but it would not present an error that says the bucket is not empty.</p>\n<p><strong>Some files are in Glacier</strong> - You store your data in Amazon S3 Glacier as archives. Archives may be further grouped into vaults. So files are not stored in buckets for Glacier. So while deleting, you will not get an error that says the bucket is not empty.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel</a></p>\n<p><a href=\"https://aws.amazon.com/glacier/faqs/\">https://aws.amazon.com/glacier/faqs/</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>After enabling S3 MFA-Delete, for which actions do you need MFA? (Select two)</p>\n", "answers": ["Permanently delete an object version", "Suspending versioning", "Enabling Versioning", "Listing deleted versions", "Uploading a new object version"], "correct_answer": ["Permanently delete an object version", "Suspending versioning"], "explanation": "<p>Correct options:</p>\n<p><strong>Permanently delete an object version</strong></p>\n<p><strong>Suspending versioning</strong></p>\n<p>You may add another layer of security by configuring a bucket to enable MFA (multi-factor authentication) Delete, which requires additional authentication for either of the following operations:</p>\n<p>Change the versioning state of your bucket</p>\n<p>Permanently delete an object version</p>\n<p>MFA Delete requires two forms of authentication together:</p>\n<p>Your security credentials</p>\n<p>The concatenation of a valid serial number, a space, and the six-digit code displayed on an approved authentication device</p>\n<p>If a bucket's versioning configuration is MFA Delete–enabled, the bucket owner must include the x-amz-mfa request header in requests to permanently delete an object version or change the versioning state of the bucket. Requests that include x-amz-mfa must use HTTPS.</p>\n<p>Incorrect options:</p>\n<p><strong>Enabling Versioning</strong> - You do not need MFA to enable versioning for a bucket.</p>\n<p><strong>Listing deleted versions</strong> - You do not need MFA to list deleted versions.</p>\n<p><strong>Uploading a new object version</strong> - You do not need MFA to upload a new object version.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html#MultiFactorAuthenticationDelete\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html#MultiFactorAuthenticationDelete</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "checkbox"}, {"question": "<p>How should MFA-Delete be enabled on an S3 bucket?</p>\n", "answers": ["Using the root account and the AWS Console", "Using the root account and the AWS CLI", "Using an admin IAM user and the AWS Console", "Using an admin IAM user and the AWS CLI"], "correct_answer": "Using the root account and the AWS CLI", "explanation": "<p>Correct option:</p>\n<p><strong>Using the root account and the AWS CLI</strong></p>\n<p>MFA Delete represents another layer of security wherein you can configure a bucket to enable MFA (multi-factor authentication) Delete, which requires additional authentication for either of the following operations:</p>\n<p>Change the versioning state of your bucket</p>\n<p>Permanently delete an object version</p>\n<p>You should note that only the bucket owner (root account) can enable MFA Delete only via the AWS CLI. However, the bucket owner, the AWS account that created the bucket (root account), and all authorized IAM users can enable versioning.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q31-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html#MultiFactorAuthenticationDelete\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html#MultiFactorAuthenticationDelete</a></p>\n<p>Incorrect options:</p>\n<p><strong>Using the root account and the AWS Console</strong></p>\n<p><strong>Using an admin IAM user and the AWS Console</strong></p>\n<p><strong>Using an admin IAM user and the AWS CLI</strong></p>\n<p>These three options contradict the explanation above, so these options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html#MultiFactorAuthenticationDelete\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html#MultiFactorAuthenticationDelete</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>In order to make your website hosted in S3 available globally, you have decided to deploy it using CloudFront. As part of this deployment, you would like to ensure that only CloudFront is allowed to access the S3 bucket files.</p>\n<p>How can you achieve that?</p>\n", "answers": ["Using an Origin Access Identity and a bucket policy", "Attaching an IAM role to CloudFront and defining a bucket policy to only allow this role", "Encrypt all your files using a KMS key that only CloudFront can access", "Attaching a security group to S3 and CloudFront and only allow incoming traffic from CloudFront using the security group rules"], "correct_answer": "Using an Origin Access Identity and a bucket policy", "explanation": "<p>Correct option:</p>\n<p><strong>Using an Origin Access Identity and a bucket policy</strong></p>\n<p>To restrict access to content that you serve from Amazon S3 buckets, you need to follow these steps:</p>\n<p>Create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution.</p>\n<p>Configure your S3 bucket permissions so that CloudFront can use the OAI to access the files in your bucket and serve them to your users. Make sure that users can’t use a direct URL to the S3 bucket to access a file there.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q34-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>\n<p>After you take these steps, users can only access your files through CloudFront, not directly from the S3 bucket.</p>\n<p>Incorrect options:</p>\n<p><strong>Attaching an IAM role to CloudFront and defining a bucket policy to only allow this role</strong> - This is a distractor as you cannot associate an IAM role to CloudFront.</p>\n<p><strong>Encrypt all your files using a KMS key that only CloudFront can access</strong> - Although you could enable SSE-KMS on S3 and serve content using CloudFront by using Lambda@Edge, but this solution does not address the given use-case. You can ensure that only CloudFront is allowed to access the S3 bucket files by using Origin Access Identity and a bucket policy.</p>\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/5b384ce32d8cdef02bc3a139d4cac0a22bb029e8/2020/05/15/architecture-lambdedge.png\"/>\nvia - <a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/serving-sse-kms-encrypted-content-from-s3-using-cloudfront/\">https://aws.amazon.com/blogs/networking-and-content-delivery/serving-sse-kms-encrypted-content-from-s3-using-cloudfront/</a></p>\n<p><strong>Attaching a security group to S3 and CloudFront and only allow incoming traffic from CloudFront using the security group rules</strong> - This is a distractor as you cannot attach a security group to S3 or CloudFront.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/serving-sse-kms-encrypted-content-from-s3-using-cloudfront/\">https://aws.amazon.com/blogs/networking-and-content-delivery/serving-sse-kms-encrypted-content-from-s3-using-cloudfront/</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>As a service provider, you generate daily report that you need to share with your dynamically changing list of over 10,000 customers. These reports sit in S3, and you would like to automate sharing the reports with them so they can have on-demand access upon their identity being proven.</p>\n<p>You plan to use Cognito, API Gateway and AWS Lambda to address this use-case. On the S3 side, what should you do?</p>\n", "answers": ["Provide each of your customers an AWS user and tell them to use the CLI", "Generate pre-signed URLs for your reports", "Create a bucket policy so that the S3 files are only accessible from CloudFront and force SSL mutual authentication there", "Make the S3 bucket public and password protect each S3 file. Share the password with each customer"], "correct_answer": "Generate pre-signed URLs for your reports", "explanation": "<p>Correct option:</p>\n<p><strong>Generate pre-signed URLs for your reports</strong></p>\n<p>A presigned URL gives you access to the object identified in the URL, provided that the creator of the presigned URL has permissions to access that object.</p>\n<p>All objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant time-limited permission to download the objects.</p>\n<p>When you create a presigned URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The presigned URLs are valid only for the specified duration.</p>\n<p>Anyone who receives the presigned URL can then access the object. For example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a presigned URL.</p>\n<p>Incorrect options:</p>\n<p><strong>Provide each of your customers an AWS user and tell them to use the CLI</strong> - This is not practicable considering that there are 10,000 customers.</p>\n<p><strong>Create a bucket policy so that the S3 files are only accessible from CloudFront and force SSL mutual authentication there</strong> - Mutual Transport Layer Security (TLS) authentication is supported for Amazon API Gateway and not for CloudFront. This is a new method for client-to-server authentication that can be used with API Gateway’s existing authorization options.</p>\n<p><strong>Make the S3 bucket public and password protect each S3 file. Share the password with each customer</strong> - This is a distractor as there is no way to password protect files on S3.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>You suspect some of your employees try to access files in S3 that they don't have access to.</p>\n<p>How can you verify this is indeed the case without them noticing?</p>\n", "answers": ["Restrict their IAM policies and look at CloudTrail logs", "Enable S3 Access Logs and analyze them using Athena", "Use a bucket policy", "Use AWS Config to define compliance rules on these users"], "correct_answer": "Enable S3 Access Logs and analyze them using Athena", "explanation": "<p>Correct option:</p>\n<p><strong>Enable S3 Access Logs and analyze them using Athena</strong></p>\n<p>By default, Amazon Simple Storage Service (Amazon S3) doesn't collect server access logs. When you enable logging, Amazon S3 delivers access logs for a source bucket to a target bucket that you choose. The target bucket must be in the same AWS Region as the source bucket and must not have a default retention period configuration.</p>\n<p>Server access logging provides detailed records for the requests that are made to an S3 bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and an error code, if relevant.</p>\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n<p>For the given use-case, you can enable S3 access logs and then use Athena to analyze the access patterns for specific employees.</p>\n<p>Incorrect options:</p>\n<p><strong>Restrict their IAM policies and look at CloudTrail logs</strong> - Restricting their IAM policies would deny access to S3 which is to be avoided per the use-case.</p>\n<p><strong>Use a bucket policy</strong> - You cannot use a bucket policy to log S3 access information</p>\n<p><strong>Use AWS Config to define compliance rules on these users</strong> - AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - \"What did my AWS resource look like at xyz point in time?\". You cannot use AWS Config to log S3 access information.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>You distribute a monthly raw data extract of your public forum's discussions that is about 10TB each month. Currently, the archive is distributed through an EFS drive, that is mounted on all your EC2 instances. Customers retrieve the file through the load balancer you have. This solution is costing you a lot of money and forces you to tremendously scale on the 1st of each month as people all try to retrieve the file at the same time.</p>\n<p>What can you do to improve the situation?</p>\n", "answers": ["Enable static file caching on the ALB", "Store the files in S3 and distribute them using a CloudFront distribution instead", "Store the files on instance stores instead, so you don't need to use EFS anymore", "Enable enhanced networking between EC2 and ALB"], "correct_answer": "Store the files in S3 and distribute them using a CloudFront distribution instead", "explanation": "<p>Correct option:</p>\n<p><strong>Store the files in S3 and distribute them using a CloudFront distribution instead</strong></p>\n<p>S3 is more cost effective than EFS. For example, per GB storage cost for S3 is $0.023/month whereas per GB storage cost for EFS is $0.3/month. Further, storing your static content with S3 provides a lot of advantages. But to help optimize your application’s performance and security while effectively managing cost, AWS recommends that you also set up Amazon CloudFront to work with your S3 bucket to serve and protect the content. CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost effective than delivering it from S3 directly to your users.</p>\n<p>Incorrect options:</p>\n<p><strong>Enable static file caching on the ALB</strong> - This is a distractor as there is no such thing as static file caching on the ALB.</p>\n<p><strong>Store the files on instance stores instead, so you don't need to use EFS anymore</strong> - You cannot use Instance Stores since they are physically attached to their own EC2 instances. Instance Store is not a shared storage like EFS, so this option is ruled out.</p>\n<p><strong>Enable enhanced networking between EC2 and ALB</strong> - Enhanced networking uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported EC2 instance types. SR-IOV is a method of device virtualization that provides higher I/O performance and lower CPU utilization when compared to traditional virtualized network interfaces. There is no such thing as enhanced networking between EC2 and ALB.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>Your website is hosted on S3 and exposed through a CloudFront distribution and some users are said to experience a lot of 501 errors.</p>\n<p>How can you analyze these errors and come up with a solution?</p>\n", "answers": ["Analyze the CloudFront access logs using Athena", "Analyze the CloudFront access logs using Inspector", "Enable S3 access logs and analyze using Athena", "Enable S3 access logs and analyze using Inspector"], "correct_answer": "Analyze the CloudFront access logs using Athena", "explanation": "<p>Correct option:</p>\n<p><strong>Analyze the CloudFront access logs using Athena</strong></p>\n<p>You can configure CloudFront to create log files that contain detailed information about every user request that CloudFront receives. These are called standard logs, also known as access logs. These standard logs are available for both web and RTMP distributions. If you enable standard logs, you can also specify the Amazon S3 bucket that you want CloudFront to save files in.</p>\n<p><img src=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/images/Logging.png\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html</a></p>\n<p>AWS recommends that you use the logs to understand the nature of the requests for your content, not as a complete accounting of all requests. CloudFront delivers access logs on a best-effort basis. The log entry for a particular request might be delivered long after the request was actually processed and, in rare cases, a log entry might not be delivered at all. When a log entry is omitted from access logs, the number of entries in the access logs won't match the usage that appears in the AWS usage and billing reports.</p>\n<p>Incorrect options:</p>\n<p><strong>Analyze the CloudFront access logs using Inspector</strong></p>\n<p><strong>Enable S3 access logs and analyze using Inspector</strong></p>\n<p>Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances.</p>\n<p>Inspector cannot be used to analyse CloudFront access logs or S3 access logs, so both these options are incorrect.</p>\n<p><strong>Enable S3 access logs and analyze using Athena</strong> - The S3 access logs will not provide details about the user IP and other crucial information, as the requests are proxied through CloudFront. Additionally, results are cached in CloudFront and the S3 access logs won't contain a lot of information, so this option is incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>You host a forum for law questions and per your country's law, you must store all the archives of conversations (about 1 TB) every week for 7 years. These archives must not be tampered with in any way, and you must prove you have set enough controls around your data protection.</p>\n<p>What should you do?</p>\n", "answers": ["Store the archives in S3 and set up a bucket policy, enable versioning and MFA-Delete", "Store the archives in Glacier and set up a Vault Lock Policy for WORM access", "Store the archives in EBS and use Linux file system protection on the files", "Store the archives in AWS Artifact and enable compliance monitoring"], "correct_answer": "Store the archives in Glacier and set up a Vault Lock Policy for WORM access", "explanation": "<p>Correct option:</p>\n<p><strong>Store the archives in Glacier and set up a Vault Lock Policy for WORM access</strong></p>\n<p>You store your data in Amazon S3 Glacier as archives. Archives may be further grouped into vaults.</p>\n<p>S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy. You can specify controls such as “write once read many” (WORM) in a vault lock policy and lock the policy from future edits. Once locked, the policy can no longer be changed.</p>\n<p>For the given use-case, as you need to ensure that the archives are not tampered, so you need to store the archives in Glacier and set up a Vault Lock Policy for WORM access.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q37-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Store the archives in S3 and set up a bucket policy, enable versioning and MFA-Delete</strong> - Even if you store the archives in a versioned S3 bucket, someone could overwrite the archive and create a new version of it so it does not strictly meet the requirements of the given use-case wherein an existing object mutates into a new version. MFA-Delete would only protect against permanent delete of any object.</p>\n<p><strong>Store the archives in EBS and use Linux file system protection on the files</strong> Linux file system protection would not be able to enforce compliance controls for the archives in EFS.</p>\n<p><strong>Store the archives in AWS Artifact and enable compliance monitoring</strong> - AWS Artifact is a self-service audit artifact retrieval portal that provides our customers with on-demand access to AWS’ compliance documentation and AWS agreements. You cannot use AWS Artifact to enforce compliance controls for the archives.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>Your data center generates tens of terabytes of data on a daily basis and has a cumulative historic data volume of 5PB. The data center is running short of storage as well as bandwidth infrastructure to store or transfer this data. Later you would like to analyze this data using Redshift or Athena, but first you must clean it using a proprietary process running on EC2.</p>\n<p>What's the optimal way of moving this data to the cloud?</p>\n", "answers": ["Use S3 transfer acceleration", "Use Snowball Edge", "Use Volume Gateway", "Use AWS Data Migration"], "correct_answer": "Use Snowball Edge", "explanation": "<p>Correct option:</p>\n<p><strong>Use Snowball Edge</strong></p>\n<p>AWS Snowball, a part of the AWS Snow Family, is a data migration and edge computing device that comes in two options. Snowball Edge Storage Optimized devices provide both block storage and Amazon S3-compatible object storage, and 40 vCPUs. They are well suited for local storage and large scale data transfer. Snowball Edge Compute Optimized devices provide 52 vCPUs, block and object storage, and an optional GPU for use cases like advanced machine learning and full-motion video analysis in disconnected environments.</p>\n<p>Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases.</p>\n<p>For the given use-case, you can use multiple Snowball Edge devices to migrate the entire data to AWS Cloud.</p>\n<p>Exam Alert:</p>\n<p>The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space.</p>\n<p>Incorrect options:</p>\n<p><strong>Use S3 transfer acceleration</strong> - Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. As the data center does not have sufficient bandwidth infrastructure, so this option is ruled out.</p>\n<p><strong>Use Volume Gateway</strong> - You can configure the AWS Storage Gateway service as a Volume Gateway to present cloud-based iSCSI block storage volumes to your on-premises applications. The Volume Gateway provides either a local cache or full volumes on-premises while also storing full copies of your volumes in the AWS cloud. Volume Gateway also provides Amazon EBS Snapshots of your data for backup, disaster recovery, and migration. It's easy to get started with the Volume Gateway: Deploy it as a virtual machine or hardware appliance, give it local disk resources, connect it to your applications, and start using your hybrid cloud storage for block data. As the data center does not have sufficient bandwidth infrastructure, so this option is ruled out.</p>\n<p>How Storage Gateway Works:\n<img src=\"https://d1.awsstatic.com/cloud-storage/AWS-Storage-Gateway-How-it-Works-Diagram.3d8305b8c1e9c46e4579fbc341209d6d3c5d2eb4.png\"/>\nvia - <a href=\"https://aws.amazon.com/storagegateway/\">https://aws.amazon.com/storagegateway/</a></p>\n<p><strong>Use AWS Data Migration</strong> - AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. The AWS Database Migration Service can migrate your data to and from most widely used commercial and open-source databases. As the data center does not have sufficient bandwidth infrastructure, so this option is ruled out.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p>\n<p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p>\n<p><a href=\"https://aws.amazon.com/storagegateway/\">https://aws.amazon.com/storagegateway/</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>You have tape backup processes and you would like to start migrating to the cloud to leverage the S3 storage capacity, while keeping the same processes and iSCSI-compatible backup software you purchased a 10-year license for.</p>\n<p>What do you recommend your company should be using?</p>\n", "answers": ["File Gateway", "Volume Gateway", "Tape Gateway", "Snowball"], "correct_answer": "Tape Gateway", "explanation": "<p>Correct option:</p>\n<p><strong>Tape Gateway</strong></p>\n<p>Tape Gateway enables you to replace using physical tapes on-premises with virtual tapes in AWS without changing existing backup workflows. Tape Gateway supports all leading backup applications and caches virtual tapes on-premises for low-latency data access. Tape Gateway encrypts data between the gateway and AWS for secure data transfer and compresses data and transitions virtual tapes between Amazon S3 and Amazon S3 Glacier, or Amazon S3 Glacier Deep Archive, to minimize storage costs.</p>\n<p>How Storage Gateway Works:\n<img src=\"https://d1.awsstatic.com/cloud-storage/AWS-Storage-Gateway-How-it-Works-Diagram.3d8305b8c1e9c46e4579fbc341209d6d3c5d2eb4.png\"/>\nvia - <a href=\"https://aws.amazon.com/storagegateway/\">https://aws.amazon.com/storagegateway/</a></p>\n<p>How Tape Gateway Works:\n<img src=\"https://d1.awsstatic.com/cloud-storage/tape-gateway-diagram.4b6ca2b4e3f97d4df7988544400ae91424503248.png\"/>\nvia - <a href=\"https://aws.amazon.com/storagegateway/vtl/\">https://aws.amazon.com/storagegateway/vtl/</a></p>\n<p>Incorrect options:</p>\n<p><strong>File Gateway</strong> - File Gateway provides a seamless way to connect to the cloud in order to store application data files and backup images as durable objects in Amazon S3 cloud storage. File Gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. It can be used for on-premises applications, and for Amazon EC2-based applications that need file protocol access to S3 object storage.</p>\n<p>File Gateway cannot be used to facilitate tape backup processes.</p>\n<p><strong>Volume Gateway</strong> - You can configure the AWS Storage Gateway service as a Volume Gateway to present cloud-based iSCSI block storage volumes to your on-premises applications. The Volume Gateway provides either a local cache or full volumes on-premises while also storing full copies of your volumes in the AWS cloud. Volume Gateway also provides Amazon EBS Snapshots of your data for backup, disaster recovery, and migration. It's easy to get started with the Volume Gateway: Deploy it as a virtual machine or hardware appliance, give it local disk resources, connect it to your applications, and start using your hybrid cloud storage for block data.</p>\n<p>Volume Gateway cannot be used to facilitate tape backup processes.</p>\n<p><strong>Snowball</strong> - AWS Snowball, a part of the AWS Snow Family, is a data migration and edge computing device that comes in two options. Snowball Edge Storage Optimized devices provide both block storage and Amazon S3-compatible object storage, and 40 vCPUs. They are well suited for local storage and large scale data transfer. Snowball Edge Compute Optimized devices provide 52 vCPUs, block and object storage, and an optional GPU for use cases like advanced machine learning and full-motion video analysis in disconnected environments.</p>\n<p>Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases.</p>\n<p>Snowball cannot be used to facilitate tape backup processes.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/storagegateway/vtl/\">https://aws.amazon.com/storagegateway/vtl/</a></p>\n<p><a href=\"https://aws.amazon.com/storagegateway/\">https://aws.amazon.com/storagegateway/</a></p>\n<p><a href=\"https://aws.amazon.com/snowball/\">https://aws.amazon.com/snowball/</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>You would like to replace your on-premise NFS v3 drive with something that will leverage the huge capacity of Amazon S3. You would like to ensure files that are commonly used are locally cached on premise.</p>\n<p>What should you use?</p>\n", "answers": ["EFS", "File Gateway", "EBS Drives", "Volume Gateway"], "correct_answer": "File Gateway", "explanation": "<p>Correct option:</p>\n<p><strong>File Gateway</strong> - File Gateway provides a seamless way to connect to the cloud in order to store application data files and backup images as durable objects in Amazon S3 cloud storage. File Gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. It can be used for on-premises applications, and for Amazon EC2-based applications that need file protocol access to S3 object storage.</p>\n<p>How Storage Gateway Works:\n<img src=\"https://d1.awsstatic.com/cloud-storage/AWS-Storage-Gateway-How-it-Works-Diagram.3d8305b8c1e9c46e4579fbc341209d6d3c5d2eb4.png\"/>\nvia - <a href=\"https://aws.amazon.com/storagegateway/\">https://aws.amazon.com/storagegateway/</a></p>\n<p>How File Gateway Works:\n<img src=\"https://d1.awsstatic.com/cloud-storage/File-Gateway-How-it-Works.6a5ce3c54688864e5b951df9cb8732fc4f2926b4.png\"/>\nvia - <a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p>\n<p>Incorrect options:</p>\n<p><strong>EFS</strong> - Amazon EFS is a file storage service for use with Amazon EC2. Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently-accessible storage for up to thousands of Amazon EC2 instances. Amazon S3 is an object storage service. EFS cannot leverage S3 for storage.</p>\n<p><strong>EBS Drives</strong> - Amazon Elastic Block Store (EBS) is an easy to use, high-performance, block-storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction intensive workloads at any scale. A broad range of workloads, such as relational and non-relational databases, enterprise applications, containerized applications, big data analytics engines, file systems, and media workflows are widely deployed on Amazon EBS. EBS cannot leverage S3 for storage.</p>\n<p><strong>Volume Gateway</strong> - You can configure the AWS Storage Gateway service as a Volume Gateway to present cloud-based iSCSI block storage volumes to your on-premises applications. The Volume Gateway provides either a local cache or full volumes on-premises while also storing full copies of your volumes in the AWS cloud. Volume Gateway also provides Amazon EBS Snapshots of your data for backup, disaster recovery, and migration. It's easy to get started with the Volume Gateway: Deploy it as a virtual machine or hardware appliance, give it local disk resources, connect it to your applications, and start using your hybrid cloud storage for block data. Since Volume Gateway represents cloud-backed iSCSI block storage, so it cannot be used to replace an on-premise NFS v3 drive, so this option is incorrect.</p>\n<p>How Volume Gateway Works:\n<img src=\"https://d1.awsstatic.com/cloud-storage/volume-gateway-diagram.eedd58ab3fb8a5dcae088622b5c1595dac21a04b.png\"/>\nvia - <a href=\"https://aws.amazon.com/storagegateway/volume/\">https://aws.amazon.com/storagegateway/volume/</a></p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p>\n<p><a href=\"https://aws.amazon.com/storagegateway/volume/\">https://aws.amazon.com/storagegateway/volume/</a></p>\n<p><a href=\"https://aws.amazon.com/storagegateway/\">https://aws.amazon.com/storagegateway/</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>You just released a new mobile game and users have the chance to interact with each other. In order to publish a profile picture, your company has made the architectural decision to have users directly upload their images into a designated S3 bucket.</p>\n<p>How can you provide write access to the mobile application users effectively?</p>\n", "answers": ["Create an AWS Lambda function that will create an IAM User for each new user, and store their API keys in the mobile app database", "Create one IAM user and publish the access keys as part of the mobile application", "Federate the users with SAML so they can use Single Sign-On (SSO) to access S3", "Federate the users with Cognito so they can assume a role to access S3"], "correct_answer": "Federate the users with Cognito so they can assume a role to access S3", "explanation": "<p>Correct option:</p>\n<p><strong>Federate the users with Cognito so they can assume a role to access S3</strong></p>\n<p>Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0. Application-specific user authentication can be provided via a Cognito User Pool and then users can access AWS services such as S3 using a Cognito Identity Pool. Here Cognito is the best technology choice for managing mobile user accounts.</p>\n<p><img src=\"https://docs.aws.amazon.com/cognito/latest/developerguide/images/scenario-cup-cib.png\"/>\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-integrating-user-pools-with-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-integrating-user-pools-with-identity-pools.html</a></p>\n<p>Amazon Cognito Features:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q41-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/cognito/details/\">https://aws.amazon.com/cognito/details/</a></p>\n<p>Exam Alert:</p>\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q41-i2.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Create an AWS Lambda function that will create an IAM User for each new user, and store their API keys in the mobile app database</strong> - Creating an IAM user for each new user of the mobile phone is not practicable, so this option is ruled out.</p>\n<p><strong>Create one IAM user and publish the access keys as part of the mobile application</strong> - This is a security bad-practice. You should not expose the IAM user access keys via a third-party application. The best solution is to use Cognito user pool for authentication and then access AWS services using an identity pool.</p>\n<p><strong>Federate the users with SAML so they can use Single Sign-On (SSO) to access S3</strong> - The scenario does not mention that the users belong to a specific organization, therefore you cannot use SAML to facilitate SSO to access S3.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q41-i3.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html</a></p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/cognito/details/\">https://aws.amazon.com/cognito/details/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-integrating-user-pools-with-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-integrating-user-pools-with-identity-pools.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>Your home-cooking website stores its recipes and comments from users in a Multi-AZ RDS database, which is located in a private subnet. As of yesterday, it seems that your users are unable to access the website, and see an error message \"512 - Cannot connect to the database\".</p>\n<p>What could be the reason why the website cannot connect to the database anymore? (Select three)</p>\n", "answers": ["DB Security Group inbound rules have changed", "Network ACL inbound rules have changed", "Security Group outbound rules have changed", "Network ACL outbound rules have changed", "The database DNS name has changed", "A read replica has been created recently"], "correct_answer": ["DB Security Group inbound rules have changed", "Network ACL inbound rules have changed", "Network ACL outbound rules have changed"], "explanation": "<p>Correct options:</p>\n<p><strong>DB Security Group inbound rules have changed</strong></p>\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance.</p>\n<p>The following are the characteristics of security group rules:</p>\n<p>By default, security groups allow all outbound traffic.</p>\n<p>Security group rules are always permissive; you can't create rules that deny access.</p>\n<p>Security groups are stateful</p>\n<p>For the given use-case, if the DB Security Group inbound rules have changed, then the website may not be able to connect to the database.</p>\n<p><strong>Network ACL inbound rules have changed</strong></p>\n<p><strong>Network ACL outbound rules have changed</strong></p>\n<p>A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.</p>\n<p>The following are the basic things that you need to know about network ACLs:</p>\n<p>The default NACL allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic.</p>\n<p>You can create a custom network ACL and associate it with a subnet. By default, each custom network ACL denies all inbound and outbound traffic until you add rules.</p>\n<p>Each subnet in your VPC must be associated with a network ACL. If you don't explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL.</p>\n<p>You can associate a network ACL with multiple subnets. However, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed.</p>\n<p>A network ACL contains a numbered list of rules. AWS evaluates the rules in order, starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL. The highest number that you can use for a rule is 32766. AWS recommends that you start by creating rules in increments (for example, increments of 10 or 100) so that you can insert new rules where you need to later on.</p>\n<p>A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic.</p>\n<p>Network ACLs are stateless, which means that responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa).</p>\n<p>For the given use-case, if the inbound or the outbound NACL rules have changed, then the website may not be able to connect to the database.</p>\n<p>Incorrect options:</p>\n<p><strong>Security Group outbound rules have changed</strong> - Since security groups are stateful so incoming connections can send a reply back to, regardless of any outbound rules, so this option is incorrect.</p>\n<p><strong>The database DNS name has changed</strong> Multi-AZ databases have a unique DNS name that points to the primary database's DNS name (and it does not change unless you do so manually), so this option is ruled out.</p>\n<p><strong>A read replica has been created recently</strong> - Adding a read replica does not change the primary database's DNS name, so this option is ruled out.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p>\n", "section": "Domain 6: Networking", "type": "checkbox"}, {"question": "<p>The Big Data team at an insurance company is performing a nightly ETL on top of your production RDS database to compute a view and then extract it into their data lake in Amazon S3. This query has been performing reasonably well in your website's infancy but now that it has grown in popularity, the query is running for a much longer period and affects the user experience while they browse your website.</p>\n<p>How can you improve the situation in the short and long term?</p>\n", "answers": ["Enable RDS Multi-AZ", "Create an RDS Read Replica for the ETL team", "Upgrade the RDS instance type", "Use Athena to query RDS"], "correct_answer": "Create an RDS Read Replica for the ETL team", "explanation": "<p>Correct option:</p>\n<p><strong>Create an RDS Read Replica for the ETL team</strong></p>\n<p>Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance.</p>\n<p>For the given use-case, you can use one or more read replicas for the given source DB instance as the source for the ETL process to populate the data lake on S3.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q43-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Enable RDS Multi-AZ</strong> - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). You cannot use Multi-AZ to improve the ETL process as it cannot use the standby instance as a source for the ETL process.</p>\n<p>Exam Alert:</p>\n<p>Please review the key differences between Read Replicas and Multi-AZ:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q43-i2.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n<p><strong>Upgrade the RDS instance type</strong> - Upgrade the RDS instance type may help a little bit, but the problem will resurface as traffic increases further. A better solution is to use the Read Replica as the source for the ETL process to populate the data lake on S3.</p>\n<p><strong>Use Athena to query RDS</strong> - Although Athena can query data from RDS by using its federated query feature, however, the problem would persist as the entire ETL load will fall on the main database. A better solution is to use the Read Replica as the source for the ETL process to populate the data lake on S3.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n<p><a href=\"https://aws.amazon.com/blogs/big-data/query-any-data-source-with-amazon-athenas-new-federated-query/\">https://aws.amazon.com/blogs/big-data/query-any-data-source-with-amazon-athenas-new-federated-query/</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>Your RDS database sometimes can become unresponsive, failing health checks and you need your application to fail-over automatically and safely without losing any committed transactions.</p>\n<p>Which options would you choose?</p>\n", "answers": ["Create an RDS read replica in the same region and an AWS lambda function to promote that replica as a main database when the main RDS database is down", "Enable RDS Multi-AZ", "Create an RDS read replica in a different region and an AWS lambda function to promote that replica as a main database when the main RDS database is down", "Setup a CloudWatch alarm for DB RAM going over 90% and reboot the database then"], "correct_answer": "Enable RDS Multi-AZ", "explanation": "<p>Correct option:</p>\n<p><strong>Enable RDS Multi-AZ</strong></p>\n<p>RDS provides high availability and failover support for DB instances using Multi-AZ deployments. In a Multi-AZ deployment, RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone.</p>\n<p>The failover happens only in the following conditions:</p>\n<p>The primary DB instance fails</p>\n<p>An Availability Zone outage</p>\n<p>The DB instance server type is changed</p>\n<p>The operating system of the DB instance is undergoing software patching.</p>\n<p>A manual failover of the DB instance can be initiated using Reboot with failover.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q44-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Create an RDS read replica in the same region and an AWS lambda function to promote that replica as a main database when the main RDS database is down</strong></p>\n<p><strong>Create an RDS read replica in a different region and an AWS lambda function to promote that replica as a main database when the main RDS database is down</strong></p>\n<p>RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Whether you create the Read Replica in the same AWS Region or a different Region from the primary DB, you cannot use it for building a High Availability solution that also transparently switches to the standby by maintaining the same DB endpoint. Using AWS Lambda to promote the Read Replica as the main database when the primary RDS database is down, would cause the DB endpoint to change.</p>\n<p>Therefore, both these options are incorrect.</p>\n<p>Exam Alert:</p>\n<p>Please review the key differences between Read Replicas and Multi-AZ:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q44-i2.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n<p><strong>Setup a CloudWatch alarm for DB RAM going over 90% and reboot the database then</strong> - This is akin to applying a band-aid. As the root cause persists, as soon as the DB instance is up and running, you will face the DB performance issues again.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p>\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n", "section": "Domain 4: Storage and Data Management", "type": "radio"}, {"question": "<p>You have a production Postgres RDS database and a custom rule in AWS Config has been setup and shows that some connections established to your database are not encrypted.</p>\n<p>How can you ensure all connections to RDS are encrypted?</p>\n", "answers": ["Edit the security group rules", "Review the DB parameter groups", "Enable SSL connections from the RDS Console", "Patch the database with the SSL/TLS Postgres Addon"], "correct_answer": "Review the DB parameter groups", "explanation": "<p>Correct option:</p>\n<p><strong>Review the DB parameter groups</strong></p>\n<p>You can allow only SSL connections to your RDS for PostgreSQL database instance by enabling the rds.force_ssl parameter (\"0\" by default) through the parameter groups page on the RDS Console, or through the CLI.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q45-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.SSL\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.SSL</a></p>\n<p>Incorrect options:</p>\n<p><strong>Edit the security group rules</strong> - Security group can be used to allow connections based on certain inbound rules from selected sources. However, you need to use parameter groups to enforce SSL.</p>\n<p><strong>Enable SSL connections from the RDS Console</strong> - This is a made-up option and has been added as a distractor.</p>\n<p><strong>Patch the database with the SSL/TLS Postgres Addon</strong> - You do not have the ability to install patches on RDS databases as these are completely managed by AWS, so this option is incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.SSL\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.SSL</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>One of your web applications runs behind a load balancer and an auto scaling group, which has a scaling policy based on the backend Aurora database requests. On top of scaling the ASG, the CloudWatch alarms auto scale the Aurora database. After a few scale out and scale in events, your application has completely lost connectivity to the database. You check the database URL referenced in the SSM parameter store and it turns out that it does not correspond to any of the Aurora read replicas, although it used to.</p>\n<p>How can you fix that problem easily in the long term while allowing your application to remain elastic?</p>\n", "answers": ["Disable Aurora Auto Scaling", "Use the Aurora Reader Endpoint", "Create an AWS Lambda function CRON job that updates SSM with the latest connection string from all the alive Aurora Read Replicas", "Create a target group made up of the Aurora Read Replicas and set up a Network Load Balancer"], "correct_answer": "Use the Aurora Reader Endpoint", "explanation": "<p>Correct option:</p>\n<p><strong>Use the Aurora Reader Endpoint</strong></p>\n<p>To perform queries, you can connect to the reader endpoint, with Aurora automatically performing load-balancing among all the Aurora Replicas.</p>\n<p>A reader endpoint for an Aurora DB cluster provides load-balancing support for read-only connections to the DB cluster. Use the reader endpoint for read operations, such as queries. By processing those statements on the read-only Aurora Replicas, this endpoint reduces the overhead on the primary instance. It also helps the cluster to scale the capacity to handle simultaneous SELECT queries, proportional to the number of Aurora Replicas in the cluster. Each Aurora DB cluster has one reader endpoint.</p>\n<p>If the cluster contains one or more Aurora Replicas, the reader endpoint load-balances each connection request among the Aurora Replicas. In that case, you can only perform read-only statements such as SELECT in that session.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q46-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Disable Aurora Auto Scaling</strong> - This option has been added as a distractor as you cannot disable Aurora Auto Scaling.</p>\n<p><strong>Create an AWS Lambda function CRON job that updates SSM with the latest connection string from all the alive Aurora Read Replicas</strong> - The Lambda CRON job can update the SSM with the connection string for Read Replicas but it is neither a simple nor an efficient solution.</p>\n<p><strong>Create a target group made up of the Aurora Read Replicas and set up a Network Load Balancer</strong> - Amazon Aurora typically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the host name and port that you specify point to an intermediate handler called an endpoint. Aurora uses the endpoint mechanism to abstract these connections. Thus, you don't have to hardcode all the hostnames or write your own logic for load-balancing and rerouting connections when some DB instances aren't available. You should note that Elastic Load Balancer cannot be used to load balance against RDS databases.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html</a></p>\n", "section": "Domain 7: Automation and Optimization", "type": "radio"}, {"question": "<p>Your e-commerce website has a few really popular items that constitute 10% of your portfolio items in your RDS database but represent 90% of your traffic. Your database is starting to struggle with the read demand and your CTO tasked you with designing a solution to improve the read scalability on the database side.</p>\n<p>What do you recommend? (Select two)</p>\n", "answers": ["Setup an ElastiCache cluster", "Setup a Multi-AZ RDS database", "Setup Read Replicas", "Setup a DAX cluster", "Setup an API gateway with cache enabled in front of your database"], "correct_answer": ["Setup an ElastiCache cluster", "Setup Read Replicas"], "explanation": "<p>Correct options:</p>\n<p><strong>Setup an ElastiCache cluster</strong></p>\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.</p>\n<p>How ElastiCache for Redis works:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\"/>\nvia - <a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n<p>Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database. Session stores are easy to create with Amazon ElastiCache for Memcached.</p>\n<p>How ElastiCache for Memcached works:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_memcached_how-it-works.cd490b0e711a3019e7bfc230408b95e97e221fa7.png\"/>\nvia - <a href=\"https://aws.amazon.com/elasticache/memcached/\">https://aws.amazon.com/elasticache/memcached/</a></p>\n<p>For the given use-case, you can use ElastiCache in front of the RDS database to improve the read scalability.</p>\n<p>Exam Alert:</p>\n<p>Please review this comparison sheet for Redis vs Memcached features:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q47-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n<p><strong>Setup Read Replicas</strong></p>\n<p>Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q47-i2.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Setup a Multi-AZ RDS database</strong> - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). You cannot use Multi-AZ to enhance the read performance for the RDS databases.</p>\n<p><strong>Setup a DAX cluster</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. DAX cannot be used as a caching layer for a relational database.</p>\n<p><strong>Setup an API gateway with cache enabled in front of your database</strong> - This is a distractor as an API Gateway with the cache enabled is typically used to front the web servers based on EC2 instances or even Lambda based serverless solutions.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n<p><a href=\"https://aws.amazon.com/elasticache/memcached/\">https://aws.amazon.com/elasticache/memcached/</a></p>\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "checkbox"}, {"question": "<p>As part of monitoring your global e-learning website, you have decided to implement a CloudWatch dashboard. The most important metric to monitor is the number of users that are connected over time, in each region.</p>\n<p>Which option should you opt for?</p>\n", "answers": ["Create one CloudWatch dashboard per region", "Create one CloudWatch dashboard and add a graph per region using the region selector in the top right corner of the AWS Console", "Create one CloudWatch dashboard and add a graph per region using the CloudWatch metric \"region\" filter", "Create one CloudWatch dashboard of the metric, and tick the option \"global metric\". Use the CloudWatch Dashboard region dropdown to change the graph on demand"], "correct_answer": "Create one CloudWatch dashboard and add a graph per region using the region selector in the top right corner of the AWS Console", "explanation": "<p>Correct option:</p>\n<p><strong>Create one CloudWatch dashboard and add a graph per region using the region selector in the top right corner of the AWS Console</strong></p>\n<p>Amazon CloudWatch dashboards are customizable home pages in the CloudWatch console that you can use to monitor your resources in a single view, even those resources that are spread across different Regions. You can use CloudWatch dashboards to create customized views of the metrics and alarms for your AWS resources. To address the given use-case, you can create a graph per region using the region selector.</p>\n<p>Incorrect options:</p>\n<p><strong>Create one CloudWatch dashboard per region</strong> - Since each CloudWatch dashboard supports resources from multiple regions, so this option is incorrect.</p>\n<p><strong>Create one CloudWatch dashboard and add a graph per region using the CloudWatch metric \"region\" filter</strong> - You can add a graph per region using the region selector in the top right corner of the AWS Console. This option has been added as a distractor.</p>\n<p><strong>Create one CloudWatch dashboard of the metric, and tick the option \"global metric\". Use the CloudWatch Dashboard region dropdown to change the graph on demand</strong> There is no such option as a \"global metric\". This option has been added as a distractor.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Dashboards.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Dashboards.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_xaxr_dashboard.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_xaxr_dashboard.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>Amazon DynamoDB is experiencing a high level of rejected requests and this outage is directly impacting your applications. Your CTO would like to know all the resources that are affected in your AWS Account and how to mitigate them.</p>\n<p>Where should you look first?</p>\n", "answers": ["In DynamoDB", "In AWS Service Health Dashboard", "In AWS Personal Health Dashboard", "In AWS Organizations"], "correct_answer": "In AWS Personal Health Dashboard", "explanation": "<p>Correct option:</p>\n<p><strong>In AWS Personal Health Dashboard</strong></p>\n<p>AWS Personal Health Dashboard provides alerts and remediation guidance when AWS is experiencing events that may impact you.\nWhile the Service Health Dashboard displays the general status of AWS services, Personal Health Dashboard gives you a personalized view into the performance and availability of the AWS services underlying your AWS resources.</p>\n<p>What’s more, Personal Health Dashboard proactively notifies you when AWS experiences any events that may affect you, helping provide quick visibility and guidance to help you minimize the impact of events in progress, and plan for any scheduled changes, such as AWS hardware maintenance.</p>\n<p>AWS Personal Health Dashboard Overview:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q49-i1.jpg\"/></p>\n<p>Incorrect options:</p>\n<p><strong>In DynamoDB</strong> - You cannot use DynamoDB to know about issues impacting all the resources in your AWS account.</p>\n<p><strong>In AWS Service Health Dashboard</strong> - AWS Service Health Dashboard publishes most up-to-the-minute information on the status and availability of all AWS services in tabular form for all Regions that AWS is present in. You can check on this page <a href=\"https://status.aws.amazon.com/\">https://status.aws.amazon.com/</a> to get current status information.</p>\n<p>AWS Service Health Dashboard Overview:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q49-i2.jpg\"/>\nvia - <a href=\"https://status.aws.amazon.com/\">https://status.aws.amazon.com/</a></p>\n<p><strong>In AWS Organizations</strong> - AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS. Organizations help you to centrally manage billing; control access, compliance, and security; and share resources across your AWS accounts. You cannot use AWS Organizations to know about issues impacting all the resources in your AWS account.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/technology/personal-health-dashboard/\">https://aws.amazon.com/premiumsupport/technology/personal-health-dashboard/</a></p>\n<p><a href=\"https://status.aws.amazon.com/\">https://status.aws.amazon.com/</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>Your company has decided to elect AWS champions that will train and drive the AWS cloud adoption internally. You would like to perform an analysis to see your most active AWS users.</p>\n<p>How can you do that?</p>\n", "answers": ["Use CloudTrail and Athena", "Use IAM usage report and Athena", "Use VPC Flow Logs and Athena", "Use GuardDuty and Athena"], "correct_answer": "Use CloudTrail and Athena", "explanation": "<p>Correct option:</p>\n<p><strong>Use CloudTrail and Athena</strong></p>\n<p>AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services.</p>\n<p>For an ongoing record of events in your AWS account, including events for IAM and AWS STS, create a trail. A trail enables CloudTrail to deliver log files to an Amazon S3 bucket. By default, when you create a trail in the console, the trail applies to all Regions. The trail logs events from all Regions in the AWS partition and delivers the log files to the Amazon S3 bucket that you specify.</p>\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n<p>You can use Athena to analyze the CloudTrail log data in S3 specific to IAM events.</p>\n<p>Incorrect options:</p>\n<p><strong>Use IAM usage report and Athena</strong> - This is a made-up option as there is no such thing as an IAM usage report.</p>\n<p><strong>Use VPC Flow Logs and Athena</strong> - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data is used to analyze network traces and helps with network security. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. You cannot use VPC Flow Logs to get information about the most active AWS users.</p>\n<p><strong>Use GuardDuty and Athena</strong> - GuardDuty is a threat detection service that monitors malicious activity and unauthorized behavior to protect your AWS account. GuardDuty analyzes billions of events across your AWS accounts from AWS CloudTrail (AWS user and API activity in your accounts), Amazon VPC Flow Logs (network traffic data), and DNS Logs (name query patterns). GuardDuty cannot be used to get information about the most active AWS users.</p>\n<p>How GuardDuty Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Amazon-GuardDuty_how-it-works.4370200b49eddc34d3a55c52c584484ceb2d532b.png\"/>\nvia - <a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html</a></p>\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>The security team at your travel company has detected a series of malicious attacks on port 846. As such, it needs to ensure that all your security groups are compliant with having this port closed, at all times. In the event such port is being opened, you need to receive a notification as soon as possible.</p>\n<p>Which service can help you with achieving such task?</p>\n", "answers": ["AWS WAF", "AWS GuardDuty", "AWS Config", "AWS Shield"], "correct_answer": "AWS Config", "explanation": "<p>Correct option:</p>\n<p><strong>AWS Config</strong></p>\n<p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - \"What did my AWS resource look like at xyz point in time?\".</p>\n<p>You can use an EventBridge rule with a custom event pattern and an input transformer to match an AWS Config evaluation rule output as NON_COMPLIANT. Then, route the response to an Amazon Simple Notification Service (Amazon SNS) topic.</p>\n<p>Incorrect options:</p>\n<p><strong>AWS WAF</strong> - AWS WAF is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection, and cross-site scripting. WAF cannot be used to detect and get notified about any security gaps when port 846 is opened.</p>\n<p>How WAF Works:\n<img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\"/>\nvia - <a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p>\n<p><strong>AWS GuardDuty</strong> - - GuardDuty is a threat detection service that monitors malicious activity and unauthorized behavior to protect your AWS account. GuardDuty analyzes billions of events across your AWS accounts from AWS CloudTrail (AWS user and API activity in your accounts), Amazon VPC Flow Logs (network traffic data), and DNS Logs (name query patterns). GuardDuty cannot be used to detect and get notified about any security gaps when port 846 is opened.</p>\n<p>How GuardDuty Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Amazon-GuardDuty_how-it-works.4370200b49eddc34d3a55c52c584484ceb2d532b.png\"/>\nvia - <a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n<p><strong>AWS Shield</strong> - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection. Shield cannot be used to detect and get notified about any security gaps when port 846 is opened.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p>\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n<p><a href=\"https://aws.amazon.com/shield/\">https://aws.amazon.com/shield/</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>As part of your yearly compliance report, it has been noted that many of your EC2 instances have been lagging with their OS patches updates. You have decided to use SSM to patch these instances regularly. To meet regulatory guidelines, you need to provide a report showing that no outstanding known vulnerabilities are left unpatched. This report must be generated on a weekly basis.</p>\n<p>Which service should you use?</p>\n", "answers": ["AWS Shield", "AWS Inspector", "AWS GuardDuty", "AWS SSM"], "correct_answer": "AWS Inspector", "explanation": "<p>Correct option:</p>\n<p><strong>AWS Inspector</strong></p>\n<p>Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances.</p>\n<p>An Amazon Inspector assessment report can be generated for an assessment run once it has been successfully completed. An assessment report is a document that details what is tested in the assessment run, and the results of the assessment. The results of your assessment are formatted into a standard report, which can be generated to share results within your team for remediation actions, to enrich compliance audit data, or to store for future reference.</p>\n<p>You can select from two types of report for your assessment, a findings report or a full report. The findings report contains an executive summary of the assessment, the instances targeted, the rules packages tested, the rules that generated findings, and detailed information about each of these rules along with the list of instances that failed the check. The full report contains all the information in the findings report, and additionally provides the list of rules that were checked and passed on all instances in the assessment target.</p>\n<p>Incorrect options:</p>\n<p><strong>AWS GuardDuty</strong> - GuardDuty is a threat detection service that monitors malicious activity and unauthorized behavior to protect your AWS account. GuardDuty analyzes billions of events across your AWS accounts from AWS CloudTrail (AWS user and API activity in your accounts), Amazon VPC Flow Logs (network traffic data), and DNS Logs (name query patterns). GuardDuty cannot provide a report showing that no outstanding known vulnerabilities are left unpatched.</p>\n<p>How GuardDuty Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Amazon-GuardDuty_how-it-works.4370200b49eddc34d3a55c52c584484ceb2d532b.png\"/>\nvia - <a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n<p><strong>AWS Shield</strong> - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection. Shield cannot provide a report showing that no outstanding known vulnerabilities are left unpatched.</p>\n<p>\"AWS SSM\" - AWS Systems Manager (SSM) gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks such as running commands, managing patches, and configuring servers across AWS Cloud as well as on-premises infrastructure.</p>\n<p>With Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources.</p>\n<p>Systems Manager cannot provide a report showing that no outstanding known vulnerabilities are left unpatched.</p>\n<p>How Systems Manager Works:\n<img src=\"https://d1.awsstatic.com/AWS%20Systems%20Manager/product-page-diagram-AWS-Systems-Manager_how-it-works.2e7c5d550e833eed0f49fb8dc1872de23b09d183.png\"/>\nvia - <a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p>\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n<p><a href=\"https://aws.amazon.com/systems-manager/\">https://aws.amazon.com/systems-manager/</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>When you launched as a short term rental company, you had 5 employees all working on the same AWS cloud account. These employees deployed their applications for various purposes, including billing, operations, finance, etc. Each of these employees have been operating in their own VPC. Now that you have grown to over 200 employees, some employees belonging to different teams have created VPC peering connections and interfered with each other's work. You would like to properly separate the environment your employees work in based on the department they belong to.</p>\n<p>What's the best way of achieving that?</p>\n", "answers": ["AWS IAM Groups with restrictive policies", "AWS Organizations with OU", "AWS IAM Roles with restrictive policies", "AWS GuardDuty"], "correct_answer": "AWS Organizations with OU", "explanation": "<p>Correct option:</p>\n<p><strong>AWS Organizations with OU</strong></p>\n<p>AWS Organizations helps you to centrally manage billing; control access, compliance, and security; and share resources across your AWS accounts. Using AWS Organizations, you can automate account creation, create groups of accounts to reflect your business needs, and apply policies for these groups for governance. You can also simplify billing by setting up a single payment method for all of your AWS accounts. AWS Organizations is available to all AWS customers at no additional charge.</p>\n<p>Key Features of AWS Organizations:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q53-i1.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/organizations/\">https://aws.amazon.com/organizations/</a></p>\n<p>An Organization Unit(OU) is a container for accounts within a root. An OU also can contain other OUs, enabling you to create a hierarchy that resembles an upside-down tree, with a root at the top and branches of OUs that reach down, ending in accounts that are the leaves of the tree. When you attach a policy to one of the nodes in the hierarchy, it flows down and affects all the branches (OUs) and leaves (accounts) beneath it. An OU can have exactly one parent, and currently each account can be a member of exactly one OU.</p>\n<p><img src=\"https://docs.aws.amazon.com/organizations/latest/userguide/images/BasicOrganization.png\"/>\nvia - <a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html</a></p>\n<p>You can use AWS Organizations with OU to segregate the environment your employees work in based on the department they belong to.</p>\n<p>Incorrect options:</p>\n<p><strong>AWS IAM Groups with restrictive policies</strong></p>\n<p><strong>AWS IAM Roles with restrictive policies</strong></p>\n<p>IAM groups or IAM roles (even with restrictive policies) cannot be used to create segregated environment on AWS, so both these options are incorrect.</p>\n<p><strong>AWS GuardDuty</strong> - GuardDuty is a threat detection service that monitors malicious activity and unauthorized behavior to protect your AWS account. GuardDuty analyzes billions of events across your AWS accounts from AWS CloudTrail (AWS user and API activity in your accounts), Amazon VPC Flow Logs (network traffic data), and DNS Logs (name query patterns). GuardDuty cannot be used to segregate the environment your employees work in based on the department they belong to.</p>\n<p>How GuardDuty Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Amazon-GuardDuty_how-it-works.4370200b49eddc34d3a55c52c584484ceb2d532b.png\"/>\nvia - <a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html</a></p>\n<p><a href=\"https://aws.amazon.com/organizations/\">https://aws.amazon.com/organizations/</a></p>\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>You are looking to be better at assigning the accounting for your AWS bills to the different departments resources belong to. You currently have all your resources under one AWS account.</p>\n<p>What is the best way to properly get billing reports for the different company departments, with the least possible administrative overhead?</p>\n", "answers": ["Use Tags", "Use Cost Allocation Tags", "Use AWS Organizations", "Use EC2 Billing Report"], "correct_answer": "Use Cost Allocation Tags", "explanation": "<p>Correct option:</p>\n<p><strong>Use Cost Allocation Tags</strong></p>\n<p>A tag is a label that you or AWS assigns to an AWS resource. Each tag consists of a key and a value. For each resource, each tag key must be unique, and each tag key can have only one value. You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level. After you activate cost allocation tags, AWS uses the cost allocation tags to organize your resource costs on your cost allocation report, to make it easier for you to categorize and track your AWS costs.</p>\n<p><img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/Tag_Example.png\"/>\nvia - <a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html</a></p>\n<p>After you or AWS applies tags to your AWS resources (such as Amazon EC2 instances or Amazon S3 buckets) and you activate the tags in the Billing and Cost Management console, AWS generates a cost allocation report as a comma-separated value (CSV file) with your usage and costs grouped by your active tags. You can apply tags that represent business categories (such as cost centers, application names, or owners) to organize your costs across multiple services.</p>\n<p>At the end of the billing cycle, the total charges (tagged and untagged) on the billing report with cost allocation tags reconciles with the total charges on your Bills page total and other billing reports for the same period.</p>\n<p>For the given use-case, you can use cost allocation tags to get billing reports for the different company departments.</p>\n<p>Incorrect options:</p>\n<p><strong>Use Tags</strong> - By default, new tag keys that you add using the API or the AWS Management Console are automatically excluded from the cost allocation report. When you select tag keys to include in your cost allocation report, each key becomes an additional column that lists the value for each corresponding line item. Because you might use tags for more than just your cost allocation report (for example, tags for security or operational reasons), you can include or exclude individual tag keys for the report. By default, you cannot use tags to get billing reports for the different company departments.</p>\n<p><strong>Use AWS Organizations</strong> - AWS Organizations helps you to centrally manage billing; control access, compliance, and security; and share resources across your AWS accounts. Using AWS Organizations, you can automate account creation, create groups of accounts to reflect your business needs, and apply policies for these groups for governance. You can also simplify billing by setting up a single payment method for all of your AWS accounts. AWS Organizations is available to all AWS customers at no additional charge. You cannot use AWS Organizations to get billing reports for the different company departments.</p>\n<p><strong>Use EC2 Billing Report</strong> - This is a made-up option as there is no such thing as an EC2 Billing Report.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html</a></p>\n", "section": "Domain 1: Monitoring and Reporting", "type": "radio"}, {"question": "<p>Among the following actions, what action is the IAM policy allowing you to do?</p>\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Id\": \"Secret Policy\",\n    \"Statement\": [\n        {\n            \"Sid\": \"EC2\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"ec2:*\",\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"Passrole\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:PassRole\"\n            ],\n            \"Resource\": \"arn:aws:iam:::role/RDS-*\"\n        }\n    ]\n}\n</code></pre>\n", "answers": ["Allowing you to give any role to RDS instances", "Allowing you to give any role to EC2 instances", "Allowing you to give RDS full access to EC2 instances", "Allowing you to assign IAM Roles to EC2 if they start with \"RDS-"], "correct_answer": "Allowing you to assign IAM Roles to EC2 if they start with \"RDS-", "explanation": "<p>Correct option:</p>\n<p><strong>Allowing you to assign IAM Roles to EC2 if they start with \"RDS-\"</strong></p>\n<p>To configure many AWS services, you must pass an IAM role to the service. This allows the service to later assume the role and perform actions on your behalf. You only have to pass the role to the service once during set-up, and not every time that the service assumes the role.</p>\n<p>To pass a role (and its permissions) to an AWS service, a user must have permissions to pass the role to the service. This helps administrators ensure that only approved users can configure a service with a role that grants permissions. To allow a user to pass a role to an AWS service, you must grant the PassRole permission to the user's IAM user, role, or group.</p>\n<p>For the given policy, the first statement block applies only to EC2 specific actions and the second statement block applies to roles only starting with <code>RDS-</code>, so the overall policy allows you to assign IAM Roles to EC2 if they start with \"RDS-\".</p>\n<p>Incorrect options:</p>\n<p><strong>Allowing you to give any role to RDS instances</strong></p>\n<p><strong>Allowing you to give any role to EC2 instances</strong></p>\n<p><strong>Allowing you to give RDS full access to EC2 instances</strong></p>\n<p>These three options contradict the explanation above, so all three options are incorrect.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>Your consumer-facing website is a high risk target for a DDoS attack and you would like to get 24/7 support in case they happen, as well as AWS bill reimbursement for the incurred costs during the attack.</p>\n<p>What service should you use?</p>\n", "answers": ["AWS Shield Advanced", "AWS WAF", "AWS Shield", "AWS DDoS OpsTeam"], "correct_answer": "AWS Shield Advanced", "explanation": "<p>Correct option:</p>\n<p><strong>AWS Shield Advanced</strong></p>\n<p>AWS Shield Standard is activated for all AWS customers, by default. For higher levels of protection against attacks, you can subscribe to AWS Shield Advanced. With Shield Advanced, you also have exclusive access to advanced, real-time metrics and reports for extensive visibility into attacks on your AWS resources. With the assistance of the DRT (DDoS response team), AWS Shield Advanced includes intelligent DDoS attack detection and mitigation for not only for network layer (layer 3) and transport layer (layer 4) attacks but also for application layer (layer 7) attacks.</p>\n<p>AWS Shield Advanced includes intelligent DDoS attack detection and mitigation not only for network layer (layer 3) and transport layer (layer 4) attacks but also for application layer (layer 7) attacks. AWS Shield Advanced is a paid service that provides additional protections for internet-facing applications. Additionally,  you get <strong>DDoS cost protection for scaling</strong>, a feature that protects your AWS bill from usage spikes on your AWS Shield Advanced protected EC2, Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 resources as a result of a DDoS attack.</p>\n<p>Incorrect options:</p>\n<p><strong>AWS WAF</strong> - AWS WAF is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection, and cross-site scripting. WAF cannot be used to detect and get notified about any security gaps when port 846 is opened. WAF cannot protect your AWS bill from usage spikes due to DDoS attacks.</p>\n<p>How WAF Works:\n<img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\"/>\nvia - <a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p>\n<p><strong>AWS Shield</strong> - AWS Shield Standard cannot protect your AWS bill from usage spikes due to DDoS attacks.</p>\n<p><strong>AWS DDoS OpsTeam</strong> - This is a made-up option and has been added as a distractor.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/shield/\">https://aws.amazon.com/shield/</a></p>\n<p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>As part of an internal IT audit, you must provide proof that AWS has the necessary ISO certifications.</p>\n<p>How can you gain access to these documents?</p>\n", "answers": ["Contact the AWS Support", "Use AWS Artifact", "Use AWS GuardDuty", "Fill out an ISO Penetration Testing form"], "correct_answer": "Use AWS Artifact", "explanation": "<p>Correct option:</p>\n<p><strong>Use AWS Artifact</strong></p>\n<p>AWS Artifact is your go-to, central resource for compliance-related information that matters to your organization. It provides on-demand access to AWS’ security and compliance reports and select online agreements. Different types of agreements are available in AWS Artifact Agreements to address the needs of customers subject to specific regulations. For example, the Business Associate Addendum (BAA) is available for customers that need to comply with the Health Insurance Portability and Accountability Act (HIPAA). It is not a service, it's a no-cost, self-service portal for on-demand access to AWS’ compliance reports.</p>\n<p>You can use AWS Artifact Reports to download AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports.</p>\n<p>Incorrect options:</p>\n<p><strong>Contact the AWS Support</strong> - You do not need to contact the AWS Support to download AWS security and compliance documents, such as AWS ISO certifications.</p>\n<p><strong>Use AWS GuardDuty</strong> - GuardDuty is a threat detection service that monitors malicious activity and unauthorized behavior to protect your AWS account. GuardDuty analyzes billions of events across your AWS accounts from AWS CloudTrail (AWS user and API activity in your accounts), Amazon VPC Flow Logs (network traffic data), and DNS Logs (name query patterns). GuardDuty cannot be used to segregate the environment your employees work in based on the department they belong to.</p>\n<p>How GuardDuty Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Amazon-GuardDuty_how-it-works.4370200b49eddc34d3a55c52c584484ceb2d532b.png\"/>\nvia - <a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n<p><strong>Fill out an ISO Penetration Testing form</strong> - This is a made-up option and has been added as a distractor.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/artifact/faq/\">https://aws.amazon.com/artifact/faq/</a></p>\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n", "section": "Domain 3: Deployment and Provisioning", "type": "radio"}, {"question": "<p>Under the shared responsibility model, what are you NOT responsible for in Amazon S3?</p>\n", "answers": ["S3 Server Side encryption", "S3 bucket policies", "S3 versioning", "S3 ACLs"], "correct_answer": "S3 Server Side encryption", "explanation": "<p>Correct option:</p>\n<p><strong>S3 Server Side encryption</strong></p>\n<p>Security and Compliance is a shared responsibility between AWS and the customer. This shared model can help relieve the customer’s operational burden as AWS operates, manages and controls the components from the host operating system and virtualization layer down to the physical security of the facilities in which the service operates.</p>\n<p>Controls that apply to both the infrastructure layer and customer layers, but in completely separate contexts or perspectives are called shared controls. In a shared control, AWS provides the requirements for the infrastructure and the customer must provide their own control implementation within their use of AWS services. Configuration Management forms a part of shared controls - AWS maintains the configuration of its infrastructure devices, but a customer is responsible for configuring their own guest operating systems, databases, and applications.</p>\n<p>For the given use-case, AWS is responsible for managing the S3 Server Side encryption.</p>\n<p>Shared Responsibility Model Overview:\n<img src=\"https://d1.awsstatic.com/security-center/Shared_Responsibility_Model_V2.59d1eccec334b366627e9295b304202faf7b899b.jpg\"/>\nvia - <a href=\"https://aws.amazon.com/compliance/shared-responsibility-model/\">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p>\n<p>Incorrect options:</p>\n<p><strong>S3 bucket policies</strong></p>\n<p><strong>S3 versioning</strong></p>\n<p><strong>S3 ACLs</strong></p>\n<p>These three options are the responsibilities of the customer.</p>\n<p>Reference:</p>\n<p><a href=\"https://aws.amazon.com/compliance/shared-responsibility-model/\">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>Your bank has an on-premise key store and wants to migrate it to the cloud. The bank needs to ensure that the keys were isolated on a self managed encryption module for compliance reasons.</p>\n<p>What service do you recommend?</p>\n", "answers": ["CloudHSM", "KMS", "S3 SSE", "GuardDuty"], "correct_answer": "CloudHSM", "explanation": "<p>Correct option:</p>\n<p><strong>CloudHSM</strong></p>\n<p>AWS CloudHSM is a service for creating and managing cloud-based hardware security modules. A hardware security module (HSM) is a specialized security device that generates and stores cryptographic keys.</p>\n<p>You should use AWS CloudHSM when you need to manage the HSMs that generate and store your encryption keys. In AWS CloudHSM, you create and manage HSMs, including creating users and setting their permissions. You also create the symmetric keys and asymmetric key pairs that the HSM stores.</p>\n<p>Incorrect options:</p>\n<p><strong>KMS</strong> - If you need to secure your encryption keys in a service backed by FIPS-validated HSMs, but you do not need to manage the HSM, you should use AWS Key Management Service (KMS).</p>\n<p>When you encrypt data, you need to protect your encryption key. If you encrypt your key, you need to protect its encryption key. Eventually, you must protect the highest level encryption key (known as a master key) in the hierarchy that protects your data. That's where AWS KMS comes in.</p>\n<p>KMS lets you create, store, and manage customer master keys (CMKs) securely. Your CMKs never leave AWS KMS unencrypted. To use a CMK in a cryptographic operation, you call KMS.</p>\n<p>KMS does not offer a self managed encryption module.</p>\n<p><strong>S3 SSE</strong> - Server-side encryption protects data at rest. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available to encrypt your data, 256-bit Advanced Encryption Standard (AES-256).</p>\n<p>S3 SSE does not offer a self managed encryption module.</p>\n<p><strong>GuardDuty</strong> - GuardDuty is a threat detection service that monitors malicious activity and unauthorized behavior to protect your AWS account. GuardDuty analyzes billions of events across your AWS accounts from AWS CloudTrail (AWS user and API activity in your accounts), Amazon VPC Flow Logs (network traffic data), and DNS Logs (name query patterns). GuardDuty cannot be used to segregate the environment your employees work in based on the department they belong to.</p>\n<p>How GuardDuty Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram-Amazon-GuardDuty_how-it-works.4370200b49eddc34d3a55c52c584484ceb2d532b.png\"/>\nvia - <a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/cloudhsm/latest/userguide/introduction.html\">https://docs.aws.amazon.com/cloudhsm/latest/userguide/introduction.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/crypto/latest/userguide/awscryp-choose-hsm.html\">https://docs.aws.amazon.com/crypto/latest/userguide/awscryp-choose-hsm.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/crypto/latest/userguide/awscryp-choose-kms.html\">https://docs.aws.amazon.com/crypto/latest/userguide/awscryp-choose-kms.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html</a></p>\n", "section": "Domain 5: Security and Compliance", "type": "radio"}, {"question": "<p>Your company's main website is deployed on Amazon S3, and distributed by CloudFront. You have set up bucket policies on S3 to ensure only CloudFront can access your bucket. You would like your website URL to be https://johntrucks.com/ .</p>\n<p>In Route 53, which type of record should you use?</p>\n", "answers": ["Alias", "CNAME", "A", "AAAA"], "correct_answer": "Alias", "explanation": "<p>Correct option:</p>\n<p><strong>Alias</strong></p>\n<p>Amazon Route 53 alias records provide a Route 53–specific extension to DNS functionality. Alias records let you route traffic to selected AWS resources, such as CloudFront distributions and Amazon S3 buckets. They also let you route traffic from one record in a hosted zone to another record.</p>\n<p>Unlike a CNAME record, you can create an alias record at the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You can't create a CNAME record for example.com, but you can create an alias record for example.com that routes traffic to www.example.com.</p>\n<p>Incorrect options:</p>\n<p><strong>CNAME</strong> - A CNAME record maps DNS queries for the name of the current record, such as acme.example.com, to another domain (example.com or example.net) or subdomain (acme.example.com or zenith.example.org). You cannot use a CNAME record for root domains, so this otion is ruled out.</p>\n<p><strong>A</strong> - You use an A record to route traffic to a resource, such as a web server, using an IPv4 address in dotted decimal notation.</p>\n<p><strong>AAAA</strong> - You use an AAAA record to route traffic to a resource, such as a web server, using an IPv6 address in colon-separated hexadecimal format.</p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>You plan on creating a subnet and want it to have at least capacity for 28 EC2 instances.</p>\n<p>What's the minimum size you need to have for your subnet?</p>\n", "answers": ["/28", "/27", "/26", "/25"], "correct_answer": "/26", "explanation": "<p>Correct option:</p>\n<p><strong>/26</strong></p>\n<p>You should note that the first four IP addresses and the last IP address in each subnet CIDR block aren't available for your use. These 5 IP addresses can't be assigned to an instance.</p>\n<p>The formula for a \"/x\" subnet = (2 * power(32-x)) - 5</p>\n<p>Therefore, for a \"/26\" subnet, you have:</p>\n<p>= (2 * power(32-26)) - 5</p>\n<p>= (2 * power(6)) - 5</p>\n<p>= 64 - 5</p>\n<p>= 59</p>\n<p>So you have 59 IP addresses, which meets the given requirement of having capacity for at least 28 EC2 instances.</p>\n<p>Incorrect options:</p>\n<p><strong>/28</strong></p>\n<p><strong>/27</strong></p>\n<p><strong>/25</strong></p>\n<p>These three options contradict the explanation above, so these are incorrect.</p>\n<p>References:</p>\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/change-subnet-mask/\">https://aws.amazon.com/premiumsupport/knowledge-center/change-subnet-mask/</a></p>\n<p><a href=\"https://s3.amazonaws.com/tr-learncanvas/docs/IP_Filtering_in_Canvas.pdf\">https://s3.amazonaws.com/tr-learncanvas/docs/IP_Filtering_in_Canvas.pdf</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>VPC Peering has been enabled between VPC A and VPC B, and the route tables have been updated for VPC A. Still, your instances cannot communicate.</p>\n<p>What is the most likely issue?</p>\n", "answers": ["Check the NACL", "Check the route tables in VPC B", "Check the instance security groups", "Check if DNS Resolution is enabled"], "correct_answer": "Check the route tables in VPC B", "explanation": "<p>Correct option:</p>\n<p><strong>Check the route tables in VPC B</strong></p>\n<p>A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection).</p>\n<p><img src=\"https://docs.aws.amazon.com/vpc/latest/peering/images/peering-intro-diagram.png\"/>\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\">https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html</a></p>\n<p>You should note that to send private IPv4 traffic from your instance to an instance in a peer VPC, you must add a route to the route table that's associated with your subnet in which your instance resides. The owner of the peer VPC (VPC B in this case) must also complete these steps to add a route to direct traffic back to your VPC through the VPC peering connection, as usually this is the most likely cause for a failed communication between peered VPCs.</p>\n<p>Incorrect options:</p>\n<p><strong>Check the NACL</strong> You should verify that an ALLOW rule exists in the network access control (network ACL) table for the required traffic.</p>\n<p><strong>Check the instance security groups</strong> - You should verify that the security group rules allow network traffic between the peered VPCs.</p>\n<p><strong>Check if DNS Resolution is enabled</strong> - DNS Resolution is used to enable resolution of public DNS hostnames to private IP addresses when queried from the peered VPC. This functionality also supports cross-account VPC peering so the two VPCs can be in different accounts.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\">https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>You have deployed a public and a private subnet. As such, you have also deployed an Internet Gateway, a NAT Gateway, an Egress Only Internet Gateway, and a Virtual Private Gateway. You would like your private subnet instances to get IPv4 access to the internet.</p>\n<p>How should you edit your route tables?</p>\n", "answers": ["Add a route with a target of 0.0.0.0/0 to the NAT Gateway", "Add a route with a target of 0.0.0.0/0 to the Internet Gateway", "Add a route with a target of 0.0.0.0/0 to the Egress Only Internet Gateway", "Add a route with a target of 10.0.0.0/12 to the Virtual Private Gateway"], "correct_answer": "Add a route with a target of 0.0.0.0/0 to the NAT Gateway", "explanation": "<p>Correct option:</p>\n<p><strong>Add a route with a target of 0.0.0.0/0 to the NAT Gateway</strong></p>\n<p>You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. A NAT gateway has the following characteristics and limitations:</p>\n<ol>\n<li><p>A NAT gateway supports 5 Gbps of bandwidth and automatically scales up to 45 Gbps.</p></li>\n<li><p>You can associate exactly one Elastic IP address with a NAT gateway.</p></li>\n<li><p>A NAT gateway supports the following protocols: TCP, UDP, and ICMP.</p></li>\n<li><p>You cannot associate a security group with a NAT gateway.</p></li>\n<li><p>You can use a network ACL to control the traffic to and from the subnet in which the NAT gateway is located.</p></li>\n<li><p>A NAT gateway can support up to 55,000 simultaneous connections to each unique destination.</p></li>\n</ol>\n<p>Therefore you must use a NAT Gateway in your public subnet in order to provide internet access to your instances in your private subnets. You also need to set up the appropriate entries in the route table of the private subnets, so for the given use-case, you need to add a route with a target of 0.0.0.0/0 to the NAT Gateway. You are charged for creating and using a NAT gateway in your account. NAT gateway hourly usage and data processing rates apply.</p>\n<p>Incorrect options:</p>\n<p><strong>Add a route with a target of 0.0.0.0/0 to the Internet Gateway</strong> - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. Internet Gateways must be deployed in a public subnet and the corresponding entry should be added in the route table of the public subnet. You should set up the correct entry in the route table of the private subnet, so you need to add a route with a target of 0.0.0.0/0 to the NAT Gateway and NOT to the Internet Gateway.</p>\n<p><strong>Add a route with a target of 0.0.0.0/0 to the Egress Only Internet Gateway</strong> - An egress-only internet gateway is for use with IPv6 traffic only, so this option is incorrect.</p>\n<p><strong>Add a route with a target of 10.0.0.0/12 to the Virtual Private Gateway</strong> - A Virtual Private Gateway is the Amazon VPC side of a VPN connection, so it's not relevant to the given scenario.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>You would like to establish a software only based private connection between your corporate data center and your AWS VPC.</p>\n<p>Which of the following component should you NOT use?</p>\n", "answers": ["Direct Connect", "Virtual Private Gateway", "Customer Gateway", "Site-to-Site VPN"], "correct_answer": "Direct Connect", "explanation": "<p>Correct option:</p>\n<p>\"Direct Connect\"</p>\n<p>AWS Direct Connect creates a dedicated private connection from a remote network to your VPC. This is a private connection and does not use the public internet. Takes at least a month to establish this connection. Direct Connect is a connectivity service and you cannot use it to provide AWS Cloud based storage access to on-premises applications. Direct Connect is a physical connection, hence it should NOT be used for the given use-case.</p>\n<p>Incorrect options:</p>\n<p>\"Site-to-Site VPN\" - By default, instances that you launch into an Amazon VPC can't communicate with your own (remote) network. You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q64-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html</a></p>\n<p>\"Virtual Private Gateway\" - A Virtual Private Gateway is the Amazon VPC side of a VPN connection.</p>\n<p>\"Customer Gateway\" - An AWS resource which provides information to AWS about your customer gateway device.</p>\n<p>Reference:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}, {"question": "<p>You would like to ensure that your instances in your private subnet for us-west-1b can talk to your public instances in us-west-1c using their private IP addresses.</p>\n<p>How can you establish network connectivity between the two subnets in the simplest possible way?</p>\n", "answers": ["Create two VPCs with one subnet each, and peer them", "Create one VPC with two subnets", "Use a NAT Gateway", "Use a NAT instance"], "correct_answer": "Create one VPC with two subnets", "explanation": "<p>Correct option:</p>\n<p><strong>Create one VPC with two subnets</strong> - When you create a new VPC, there is a main route table that automatically comes with your VPC. It controls the routing for all subnets that are not explicitly associated with any other route table.  Each subnet in your VPC must be associated with a route table, which controls the routing for the subnet (subnet route table). You can explicitly associate a subnet with a particular route table. Otherwise, the subnet is implicitly associated with the main route table. A subnet can only be associated with one route table at a time, but you can associate multiple subnets with the same subnet route table.</p>\n<p>So for the given use-case, as the VPC has two subnets, the main route table will take care of the routing between the two subnets.</p>\n<p><img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q65-i1.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html</a></p>\n<p>Incorrect options:</p>\n<p><strong>Create two VPCs with one subnet each, and peer them</strong> - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection). VPC Peering is a complex solution for the given use-case.</p>\n<p><strong>Use a NAT Gateway</strong> - You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances.</p>\n<p><strong>Use a NAT instance</strong> - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the internet.</p>\n<p>Comparison of NAT instances and NAT gateways:\n<img src=\"https://media.datacumulus.com/aws-soa-pt/assets/pt2-q65-i2.jpg\"/>\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a></p>\n<p>References:</p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html</a></p>\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a></p>\n", "section": "Domain 6: Networking", "type": "radio"}]; 

    let correctCount = 0;
    let incorrectCount = 0;
    let totalQuestions = allQuizData.length;

    function shuffleArray(array) {
        for (let i = array.length - 1; i > 0; i--) {
            const j = Math.floor(Math.random() * (i + 1));
            [array[i], array[j]] = [array[j], array[i]];
        }
        return array;
    }

    function updateScoreDisplay() {
        document.getElementById('total-questions-display').textContent = totalQuestions;
        document.getElementById('correct-answers-display').textContent = correctCount;
        document.getElementById('wrong-answers-display').textContent = incorrectCount;
    }

    function renderQuiz() {
        const quizContainer = document.getElementById('quiz-container');
        quizContainer.innerHTML = '';
        correctCount = 0;
        incorrectCount = 0;

        const shuffledQuestions = shuffleArray([...allQuizData]);

        let questionCounter = 1;
        const sectionsMap = new Map();

        shuffledQuestions.forEach(qData => {
            const sectionName = qData.section || 'Uncategorized';
            if (!sectionsMap.has(sectionName)) {
                sectionsMap.set(sectionName, []);
            }
            sectionsMap.get(sectionName).push(qData);
        });

        sectionsMap.forEach((questionsInSection, sectionName) => {
            const sectionHeaderHtml = `<h2 class="section-header">${sectionName}</h2>`;
            quizContainer.insertAdjacentHTML('beforeend', sectionHeaderHtml);

            questionsInSection.forEach(qData => {
                const questionId = `q_${qData.id || questionCounter}`;
                const questionHtml = qData.question; // Giữ nguyên HTML
                const correctAnswers = qData.correct_answer;
                const explanation = qData.explanation;
                const questionType = qData.type;

                const shuffledAnswers = shuffleArray([...qData.answers]);

                let optionsListHtml = "";
                shuffledAnswers.forEach((ansText, i) => {
                    const inputId = `${questionId}_option_${i}`;
                    optionsListHtml += `
                    <label for="${inputId}" class="option-label">
                        <input type="${questionType}" id="${inputId}" name="${questionId}" value="${ansText.replace(/"/g, '&quot;')}" />
                        <span>${ansText}</span>
                    </label>
                    `;
                });
                
                const encodedCorrectAnswers = encodeURIComponent(JSON.stringify(correctAnswers));
                const formHtml = `
                <form id="form_${questionId}" class="question-container" data-question-id="${questionId}" data-correct-answer="${encodedCorrectAnswers}" data-question-type="${questionType}">
                    <p class="question-prompt">Câu hỏi ${questionCounter}:</p>
                    <div class="question-content">${questionHtml}</div> <div class="options-container">
                        ${optionsListHtml}
                    </div>
                    <div class="buttons-container">
                        <button type="button" class="submit-btn" onclick="checkAnswer('${questionId}')">Kiểm tra câu trả lời</button>
                        <button type="button" class="explanation-btn" onclick="showExplanation('${questionId}')">Xem giải thích</button>
                    </div>
                    <div id="feedback_${questionId}" class="feedback-message"></div>
                    <div id="explanation_content_${questionId}" style="display: none;">${explanation}</div>
                </form>
                `;
                quizContainer.insertAdjacentHTML('beforeend', formHtml);
                questionCounter++;
            });
        });
        updateScoreDisplay();
    }

    function checkAnswer(questionId) {
        const form = document.getElementById(`form_${questionId}`);
        const questionType = form.dataset.questionType;
        const correctAnswers = JSON.parse(decodeURIComponent(form.dataset.correctAnswer)); 
        const feedbackDiv = document.getElementById(`feedback_${questionId}`);
        const submitBtn = form.querySelector('.submit-btn');

        let selectedOptions = [];
        if (questionType === 'radio') {
            const selectedRadio = form.querySelector(`input[name="${questionId}"]:checked`);
            if (selectedRadio) {
                selectedOptions.push(selectedRadio.value);
            }
        } else { /* checkbox */
            form.querySelectorAll(`input[name="${questionId}"]:checked`).forEach(checkbox => {
                selectedOptions.push(checkbox.value);
            });
        }

        if (selectedOptions.length === 0) {
            feedbackDiv.className = 'incorrect-feedback';
            feedbackDiv.innerHTML = 'Vui lòng chọn ít nhất một câu trả lời!';
            return; 
        }

        if (form.dataset.answered === 'true') { 
             return;
        }

        form.querySelectorAll(`input[name="${questionId}"]`).forEach(input => {
            input.disabled = true;
        });
        submitBtn.disabled = true;
        submitBtn.style.opacity = '0.6';
        submitBtn.style.cursor = 'not-allowed';

        let isCorrect = false;
        if (questionType === 'radio') {
            isCorrect = (selectedOptions[0] === correctAnswers);
        } else { /* checkbox */
            const sortedSelected = selectedOptions.sort();
            const sortedCorrect = correctAnswers.sort();
            isCorrect = (sortedSelected.length === sortedCorrect.length &&
                         sortedSelected.every((val, index) => val === sortedCorrect[index]));
        }

        form.querySelectorAll('.option-label').forEach(label => {
            label.classList.remove('correct', 'incorrect');
        });

        if (isCorrect) {
            feedbackDiv.className = 'correct-feedback';
            feedbackDiv.innerHTML = 'Chính xác!';
            correctCount++;
        } else {
            feedbackDiv.className = 'incorrect-feedback';
            feedbackDiv.innerHTML = 'Sai rồi. Câu trả lời đúng là: ' + 
                                    (Array.isArray(correctAnswers) ? correctAnswers.join('; ') : correctAnswers);
            incorrectCount++;
        }
        
        form.dataset.answered = 'true';

        form.querySelectorAll(`input[name="${questionId}"]`).forEach(input => {
            const label = input.closest('.option-label');
            if (label) {
                if (questionType === 'radio') {
                    if (input.value === correctAnswers) {
                        label.classList.add('correct');
                    } else if (input.checked) { 
                        label.classList.add('incorrect');
                    }
                } else { // checkbox
                    if (correctAnswers.includes(input.value)) {
                        label.classList.add('correct');
                    } else if (input.checked) { 
                        label.classList.add('incorrect');
                    }
                }
            }
        });
        updateScoreDisplay();
    }

    const modal = document.getElementById('explanationModal');
    const modalContentBody = document.getElementById('modalExplanationContent');
    const closeBtn = document.querySelector('.close-button');

    function showExplanation(questionId) {
        const explanationText = document.getElementById(`explanation_content_${questionId}`).innerHTML;
        modalContentBody.innerHTML = explanationText; // Hiển thị HTML giải thích
        modal.style.display = 'block';
    }

    closeBtn.onclick = function() {
        modal.style.display = 'none';
    }

    window.onclick = function(event) {
        if (event.target == modal) {
            modal.style.display = 'none';
        }
    }
    // Chạy hàm renderQuiz khi trang được tải hoặc làm mới
    document.addEventListener('DOMContentLoaded', renderQuiz);
    
        </script>
    </body>
    </html>
    
